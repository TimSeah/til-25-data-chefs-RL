{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851706a-7856-4b4b-a174-f180fd1449fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dac43a-973d-4451-b7ad-31326185f954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./til_marl_tensorboard_rs_explore_v7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018f5a3-ac05-4ee4-bba8-52bf88a54d39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "import imageio\n",
    "import numpy as np # For frame manipulation\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from til_environment import gridworld\n",
    "from til_environment.gridworld import NUM_ITERS\n",
    "from til_environment.flatten_dict import FlattenDictWrapper\n",
    "from supersuit import frame_stack_v2\n",
    "from til_environment.types import RewardNames, Action\n",
    "\n",
    "from pettingzoo.utils.env import ActionType, AECEnv, AgentID, ObsType\n",
    "from pettingzoo.utils.wrappers.base import BaseWrapper\n",
    "\n",
    "# --- Custom Wrapper to Clip Step Observation (from training script) ---\n",
    "class ClipStepObservationWrapper(BaseWrapper[AgentID, ObsType, ActionType]):\n",
    "    def __init__(self, env: AECEnv[AgentID, ObsType, ActionType]):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observe(self, agent: AgentID) -> ObsType | None:\n",
    "        obs = super().observe(agent)\n",
    "        if obs is not None and isinstance(obs, dict) and \"step\" in obs:\n",
    "            obs[\"step\"] = min(obs[\"step\"], NUM_ITERS - 1)\n",
    "        return obs\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent: AgentID):\n",
    "        return super().observation_space(agent)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent: AgentID):\n",
    "        return super().action_space(agent)\n",
    "\n",
    "# --- Define constants and configurations relevant to v6 model ---\n",
    "REWARDS_DICT_V6 = {\n",
    "    RewardNames.GUARD_CAPTURES: 50,\n",
    "    RewardNames.SCOUT_CAPTURED: -50,\n",
    "    RewardNames.SCOUT_RECON: 5,\n",
    "    RewardNames.SCOUT_MISSION: 20,\n",
    "    RewardNames.WALL_COLLISION: -0.5,\n",
    "    RewardNames.STATIONARY_PENALTY: -0.1,\n",
    "    RewardNames.SCOUT_STEP: -0.01,\n",
    "    RewardNames.GUARD_STEP: -0.01,\n",
    "    RewardNames.GUARD_TRUNCATION: -10,\n",
    "    RewardNames.SCOUT_TRUNCATION: 0,\n",
    "}\n",
    "print(f\"Using v6 rewards_dict for evaluation: {REWARDS_DICT_V6}\")\n",
    "\n",
    "CUSTOM_ENV_WRAPPERS_LIST_V6 = [\n",
    "    ClipStepObservationWrapper,\n",
    "    FlattenDictWrapper,\n",
    "    partial(frame_stack_v2, stack_size=4, stack_dim=-1),\n",
    "]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "NOVICE_MODE = False\n",
    "MODEL_NAME_FOR_VIDEOS = \"til_ai_marl_ppo_rs_explore_v6\"\n",
    "\n",
    "def create_placeholder_frame(frame_shape, color=(0,0,0)):\n",
    "    \"\"\"Creates a blank frame of a given shape and color.\"\"\"\n",
    "    ph_frame = np.full(frame_shape, color, dtype=np.uint8)\n",
    "    return ph_frame\n",
    "\n",
    "def create_composite_frame(frames_list, grid_layout=(2,2)):\n",
    "    \"\"\"\n",
    "    Creates a composite frame by arranging frames in a grid.\n",
    "    Assumes all frames in frames_list have the same shape.\n",
    "    Handles cases where fewer frames are provided than grid slots by using placeholders.\n",
    "    \"\"\"\n",
    "    if not frames_list or not any(f is not None for f in frames_list):\n",
    "        return None # Or a default blank image of expected composite size\n",
    "\n",
    "    # Determine frame shape from the first valid frame\n",
    "    first_valid_frame = next((f for f in frames_list if f is not None), None)\n",
    "    if first_valid_frame is None: # Should not happen if a game ran\n",
    "        return None\n",
    "    frame_h, frame_w, frame_c = first_valid_frame.shape\n",
    "    placeholder = create_placeholder_frame((frame_h, frame_w, frame_c))\n",
    "\n",
    "    rows, cols = grid_layout\n",
    "    \n",
    "    # Ensure frames_list has enough elements for the grid, padding with placeholders\n",
    "    padded_frames = list(frames_list) # Make a mutable copy\n",
    "    while len(padded_frames) < rows * cols:\n",
    "        padded_frames.append(placeholder)\n",
    "    \n",
    "    # Replace None frames with placeholders\n",
    "    for i in range(len(padded_frames)):\n",
    "        if padded_frames[i] is None:\n",
    "            padded_frames[i] = placeholder\n",
    "\n",
    "    grid_rows = []\n",
    "    for i in range(rows):\n",
    "        row_frames = padded_frames[i*cols : (i+1)*cols]\n",
    "        if not row_frames: continue # Should not happen with padding\n",
    "        # If a row has fewer frames than cols (e.g. 3 games for 2x2), pad it\n",
    "        while len(row_frames) < cols:\n",
    "            row_frames.append(placeholder)\n",
    "        grid_rows.append(np.hstack(row_frames))\n",
    "    \n",
    "    if not grid_rows:\n",
    "        return placeholder # Fallback, should not happen\n",
    "    \n",
    "    composite = np.vstack(grid_rows)\n",
    "    return composite\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_til_ai_marl(\n",
    "    num_games_to_display_concurrently: int = 4, # Number of games for the grid display\n",
    "    render_mode: str | None = None, # MUST be \"rgb_array\" for concurrent video\n",
    "    novice_eval: bool = True,\n",
    "    model_to_load_path: str | None = None,\n",
    "    eval_device: str = \"cpu\",\n",
    "    print_actions: bool = False\n",
    "):\n",
    "    if model_to_load_path is None:\n",
    "        print(\"Error: model_to_load_path must be provided.\")\n",
    "        return\n",
    "    \n",
    "    if render_mode != \"rgb_array\" and num_games_to_display_concurrently > 0:\n",
    "        print(\"Warning: For concurrent video, render_mode must be 'rgb_array'.\")\n",
    "        print(\"Video generation for concurrent display will be skipped.\")\n",
    "        # Fallback to sequential non-video evaluation if not rgb_array\n",
    "        # Or, you could force render_mode to rgb_array here if a concurrent display is explicitly requested.\n",
    "\n",
    "    print(f\"\\nStarting evaluation for model: {model_to_load_path}\")\n",
    "    print(f\"Preparing to display {num_games_to_display_concurrently} games concurrently in video.\")\n",
    "    print(f\"(Novice: {novice_eval}, Render: {render_mode}, Device: {eval_device})...\")\n",
    "\n",
    "    eval_rewards_dict = REWARDS_DICT_V6\n",
    "    eval_env_wrappers = CUSTOM_ENV_WRAPPERS_LIST_V6\n",
    "\n",
    "    eval_env_kwargs = {\n",
    "        \"render_mode\": \"rgb_array\", # Force rgb_array for frame collection\n",
    "        \"novice\": novice_eval,\n",
    "        \"env_wrappers\": eval_env_wrappers,\n",
    "        \"rewards_dict\": eval_rewards_dict,\n",
    "    }\n",
    "\n",
    "    eval_env = gridworld.env(**eval_env_kwargs) # One env instance, used sequentially\n",
    "            \n",
    "    try:\n",
    "        loaded_model = PPO.load(model_to_load_path, device=eval_device)\n",
    "        print(f\"Successfully loaded model: {model_to_load_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_to_load_path}: {e}\")\n",
    "        eval_env.close()\n",
    "        return\n",
    "\n",
    "    total_rewards_all_games_stats = {agent: 0.0 for agent in eval_env.possible_agents} # For stats\n",
    "    \n",
    "    video_folder = f\"./logs/videos_{MODEL_NAME_FOR_VIDEOS}_eval_concurrent/\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "    all_runs_all_frames = [] # List to store lists of frames for each game run\n",
    "\n",
    "    for game_idx in range(num_games_to_display_concurrently):\n",
    "        print(f\"\\n--- Simulating Game {game_idx + 1} of {num_games_to_display_concurrently} ---\")\n",
    "        eval_env.reset(seed=RANDOM_SEED + game_idx + 3000) # Different seed for each game\n",
    "        \n",
    "        current_game_collected_frames = []\n",
    "        game_rewards_this_round = {agent: 0.0 for agent in eval_env.possible_agents}\n",
    "        # action_counts_this_game = {act: 0 for act in Action} # If needed per game\n",
    "\n",
    "        for agent_id in eval_env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = eval_env.last()\n",
    "            game_rewards_this_round[agent_id] += reward\n",
    "\n",
    "            if termination or truncation: action_val = None\n",
    "            else: action_val, _ = loaded_model.predict(observation, deterministic=False)\n",
    "            \n",
    "            eval_env.step(action_val)\n",
    "\n",
    "            if print_actions and action_val is not None:\n",
    "                print(f\"Game {game_idx+1}, Agent {agent_id}, Action: {Action(action_val).name}\")\n",
    "\n",
    "            # Always render if we intend to make a video\n",
    "            try:\n",
    "                frame = eval_env.render() # mode is already rgb_array from env creation\n",
    "                if frame is not None:\n",
    "                    current_game_collected_frames.append(frame)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not render frame for game {game_idx+1}, agent {agent_id}: {e}\")\n",
    "        \n",
    "        for agent_id_sum in eval_env.possible_agents:\n",
    "            total_rewards_all_games_stats[agent_id_sum] += game_rewards_this_round[agent_id_sum]\n",
    "        \n",
    "        all_runs_all_frames.append(current_game_collected_frames)\n",
    "        print(f\"Game {game_idx + 1} finished. Rewards: {game_rewards_this_round}. Collected {len(current_game_collected_frames)} frames.\")\n",
    "\n",
    "    eval_env.close() # Close the single environment instance\n",
    "\n",
    "    # --- Post-simulation: Create Composite Video ---\n",
    "    if render_mode == \"rgb_array\" and all_runs_all_frames and any(all_runs_all_frames):\n",
    "        print(\"\\n--- Creating Composite Video ---\")\n",
    "        # Determine grid layout (e.g., 2x2 for 4 games, 1xN or Nx1 for others)\n",
    "        if num_games_to_display_concurrently == 0:\n",
    "             print(\"No games specified for concurrent display.\")\n",
    "        elif num_games_to_display_concurrently == 1:\n",
    "            grid_layout = (1,1)\n",
    "        elif num_games_to_display_concurrently == 2:\n",
    "            grid_layout = (1,2) # Side-by-side\n",
    "        elif num_games_to_display_concurrently == 3:\n",
    "            grid_layout = (1,3) # Side-by-side\n",
    "        elif num_games_to_display_concurrently == 4:\n",
    "            grid_layout = (2,2)\n",
    "        else: # Default for > 4, could be smarter\n",
    "            cols = int(np.ceil(np.sqrt(num_games_to_display_concurrently)))\n",
    "            rows = int(np.ceil(num_games_to_display_concurrently / cols))\n",
    "            grid_layout = (rows, cols)\n",
    "            print(f\"Using grid layout: {grid_layout} for {num_games_to_display_concurrently} games.\")\n",
    "\n",
    "        max_frames_in_any_run = 0\n",
    "        if all_runs_all_frames:\n",
    "             max_frames_in_any_run = max(len(frames) for frames in all_runs_all_frames if frames) if any(all_runs_all_frames) else 0\n",
    "\n",
    "\n",
    "        composite_video_frames = []\n",
    "        example_frame_shape = None\n",
    "        if max_frames_in_any_run > 0:\n",
    "            for game_frames in all_runs_all_frames:\n",
    "                if game_frames:\n",
    "                    example_frame_shape = game_frames[0].shape\n",
    "                    break\n",
    "        \n",
    "        if example_frame_shape is None and num_games_to_display_concurrently > 0 :\n",
    "             print(\"Could not determine frame shape. Skipping composite video.\")\n",
    "        elif num_games_to_display_concurrently > 0 :\n",
    "            placeholder_frame = create_placeholder_frame(example_frame_shape)\n",
    "\n",
    "            for t in range(max_frames_in_any_run):\n",
    "                frames_at_this_timestep = []\n",
    "                for game_idx in range(num_games_to_display_concurrently):\n",
    "                    if game_idx < len(all_runs_all_frames) and t < len(all_runs_all_frames[game_idx]):\n",
    "                        frames_at_this_timestep.append(all_runs_all_frames[game_idx][t])\n",
    "                    else: # Game ended or no frame\n",
    "                        # Use last available frame for that game, or placeholder\n",
    "                        if game_idx < len(all_runs_all_frames) and all_runs_all_frames[game_idx]:\n",
    "                             frames_at_this_timestep.append(all_runs_all_frames[game_idx][-1])\n",
    "                        else:\n",
    "                             frames_at_this_timestep.append(placeholder_frame)\n",
    "                \n",
    "                composite = create_composite_frame(frames_at_this_timestep, grid_layout)\n",
    "                if composite is not None:\n",
    "                    composite_video_frames.append(composite)\n",
    "            \n",
    "            if composite_video_frames:\n",
    "                video_file_suffix = f\"concurrent_{grid_layout[0]}x{grid_layout[1]}_{num_games_to_display_concurrently}games\"\n",
    "                composite_video_filename = f\"{os.path.basename(model_to_load_path).replace('.zip', '')}_{video_file_suffix}.mp4\"\n",
    "                composite_video_path = os.path.join(video_folder, composite_video_filename)\n",
    "                try:\n",
    "                    imageio.mimsave(composite_video_path, composite_video_frames, fps=eval_env.metadata.get(\"render_fps\", 10))\n",
    "                    print(f\"Saved composite video to {composite_video_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving composite video: {e}\")\n",
    "            else:\n",
    "                print(\"No composite frames generated for video.\")\n",
    "\n",
    "    # --- Final Statistics ---\n",
    "    print(\"\\n--- Overall Evaluation Summary ---\")\n",
    "    avg_rewards_per_agent = {\n",
    "        agent: total_rewards_all_games_stats[agent] / num_games_to_display_concurrently for agent in eval_env.possible_agents\n",
    "    }\n",
    "    print(f\"Average rewards per agent over {num_games_to_display_concurrently} simulated games (using v6 training rewards): {avg_rewards_per_agent}\")\n",
    "    \n",
    "    if eval_env.possible_agents: # Check if possible_agents is not empty\n",
    "        team_avg_reward = sum(avg_rewards_per_agent.values())\n",
    "        print(f\"Sum of average rewards for all agents (team perspective): {team_avg_reward:.4f}\")\n",
    "        print(f\"Note: The avg_rewards_per_agent is based on the v6 training rewards_dict values.\")\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "if __name__ == \"__main__\":\n",
    "    PATH_TO_V6_MODEL = \"./til_ai_marl_ppo_rs_explore_v6_20250520-071438.zip\" # YOUR MODEL PATH\n",
    "\n",
    "    if not os.path.exists(PATH_TO_V6_MODEL):\n",
    "        print(f\"ERROR: Model file not found at {PATH_TO_V6_MODEL}\")\n",
    "        print(\"Please update PATH_TO_V6_MODEL with the correct path to your .zip file.\")\n",
    "    else:\n",
    "        # Example: Render 4 games into a single 2x2 concurrent video\n",
    "        evaluate_til_ai_marl(\n",
    "            num_games_to_display_concurrently=4, # Number of games for the grid\n",
    "            render_mode=\"rgb_array\",      # Must be \"rgb_array\" for this to work\n",
    "            novice_eval=NOVICE_MODE,      # Should match training\n",
    "            model_to_load_path=PATH_TO_V6_MODEL,\n",
    "            eval_device=\"cpu\",            # Can be \"cuda\" if preferred and available\n",
    "            print_actions=False           # Typically false when focusing on video\n",
    "        )\n",
    "        \n",
    "        # Example: Render 2 games side-by-side\n",
    "        # evaluate_til_ai_marl(\n",
    "        #     num_games_to_display_concurrently=2,\n",
    "        #     render_mode=\"rgb_array\",\n",
    "        #     novice_eval=NOVICE_MODE,\n",
    "        #     model_to_load_path=PATH_TO_V6_MODEL,\n",
    "        #     eval_device=\"cpu\",\n",
    "        #     print_actions=False\n",
    "        # )\n",
    "\n",
    "        video_output_dir = os.path.abspath(f\"./logs/videos_{MODEL_NAME_FOR_VIDEOS}_eval_concurrent/\")\n",
    "        print(f\"\\nTo view concurrent videos, check the '{video_output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08412cc0-2c05-4ea0-a954-9e41b85174f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLManager self-test initiated.\n",
      "RLManager: Observation space for flattening defined with NUM_ITERS=100.\n",
      "RLManager Warning: __file__ not defined. Assuming model path 'til_ai_marl_ppo_rs_explore_v7_20250520-104436.zip' is relative to current working directory.\n",
      "RLManager: Successfully loaded model from /home/jupyter/Tim Testing/til_ai_marl_ppo_rs_explore_v7_20250520-104436.zip\n",
      "RLManager: Shape of a single flattened observation: (44,)\n",
      "RLManager: Resetting agent state (frame stack).\n",
      "RLManager: Initialization complete.\n",
      "\n",
      "Sample raw observation: {'viewcone': [[102, 220, 225, 95, 179], [61, 234, 203, 92, 3], [98, 243, 14, 149, 245], [46, 106, 244, 99, 187], [71, 212, 153, 199, 188], [174, 65, 153, 20, 44], [203, 152, 102, 214, 240]], 'direction': 1, 'scout': 0, 'location': [6, 4], 'step': 74}\n",
      "An error occurred during RLManager self-test: Error: Unexpected observation shape (176,) for Box environment, please use (572,) or (n_env, 572) for the observation shape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/tmp/ipykernel_15068/2936968207.py\", line 131, in <module>\n",
      "    action = manager.rl(sample_raw_observation)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/tmp/ipykernel_15068/2936968207.py\", line 106, in rl\n",
      "    action, _ = self.model.predict(model_input, deterministic=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/env/lib/python3.12/site-packages/stable_baselines3/common/base_class.py\", line 557, in predict\n",
      "    return self.policy.predict(observation, state, episode_start, deterministic)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/env/lib/python3.12/site-packages/stable_baselines3/common/policies.py\", line 365, in predict\n",
      "    obs_tensor, vectorized_env = self.obs_to_tensor(observation)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/env/lib/python3.12/site-packages/stable_baselines3/common/policies.py\", line 272, in obs_to_tensor\n",
      "    vectorized_env = is_vectorized_observation(observation, self.observation_space)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/env/lib/python3.12/site-packages/stable_baselines3/common/utils.py\", line 404, in is_vectorized_observation\n",
      "    return is_vec_obs_func(observation, observation_space)  # type: ignore[operator]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/env/lib/python3.12/site-packages/stable_baselines3/common/utils.py\", line 271, in is_vectorized_box_observation\n",
      "    raise ValueError(\n",
      "ValueError: Error: Unexpected observation shape (176,) for Box environment, please use (572,) or (n_env, 572) for the observation shape.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Manages the RL model.\"\"\"\n",
    "\n",
    "import os\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from gymnasium.spaces import Box, Discrete, Dict, flatten\n",
    "\n",
    "# Assuming til_environment is in the python path or PYTHONPATH is set.\n",
    "try:\n",
    "    from til_environment.gridworld import NUM_ITERS\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not import NUM_ITERS from til_environment.gridworld.\")\n",
    "    print(\"Ensure til_environment is installed and accessible.\")\n",
    "    print(\"Using a default NUM_ITERS = 100 for clipping. This might be incorrect.\")\n",
    "    NUM_ITERS = 100\n",
    "\n",
    "class RLManager:\n",
    "    \"\"\"\n",
    "    Manages the RL model for inference, including observation preprocessing\n",
    "    to match the training setup (clipping, flattening, frame stacking).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str = \"til_ai_marl_ppo_rs_explore_v7_20250520-104436.zip\"):\n",
    "        \"\"\"\n",
    "        Initializes the RLManager, loads the model, and sets up\n",
    "        observation processing components.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained PPO model .zip file.\n",
    "        \"\"\"\n",
    "        self.observation_space_unflattened = Dict({\n",
    "            \"viewcone\": Box(0, 255, (7, 5), np.uint8),\n",
    "            \"direction\": Discrete(4),\n",
    "            \"scout\": Discrete(2),\n",
    "            \"location\": Box(np.array([0, 0]), np.array([15, 15]), (2,), np.int8),\n",
    "            \"step\": Box(0, NUM_ITERS - 1, (1,), np.int8)\n",
    "        })\n",
    "        print(f\"RLManager: Observation space for flattening defined with NUM_ITERS={NUM_ITERS}.\")\n",
    "\n",
    "        absolute_model_path = model_path\n",
    "        if not os.path.isabs(model_path):\n",
    "            try:\n",
    "                # Preferred method: model path relative to this script file\n",
    "                script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "                absolute_model_path = os.path.join(script_dir, model_path)\n",
    "            except NameError:\n",
    "                # Fallback for environments where __file__ is not defined (e.g., Jupyter)\n",
    "                print(f\"RLManager Warning: __file__ not defined. Assuming model path '{model_path}' is relative to current working directory.\")\n",
    "                absolute_model_path = os.path.join(os.getcwd(), model_path)\n",
    "        \n",
    "        if not os.path.exists(absolute_model_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"RLManager Error: Model file not found at {absolute_model_path}. \"\n",
    "                \"Please ensure the model .zip file is correctly located.\"\n",
    "            )\n",
    "        \n",
    "        self.model = PPO.load(absolute_model_path, device=\"cpu\")\n",
    "        print(f\"RLManager: Successfully loaded model from {absolute_model_path}\")\n",
    "\n",
    "        self.frame_stack_size = 4\n",
    "        self._frame_stack_buffer = deque(maxlen=self.frame_stack_size)\n",
    "\n",
    "        dummy_unflattened_obs = self.observation_space_unflattened.sample()\n",
    "        dummy_unflattened_obs[\"step\"] = np.clip(dummy_unflattened_obs[\"step\"], 0, NUM_ITERS - 1).astype(np.int8)\n",
    "        self._single_flat_obs_shape = flatten(\n",
    "            self.observation_space_unflattened, dummy_unflattened_obs\n",
    "        ).shape\n",
    "        print(f\"RLManager: Shape of a single flattened observation: {self._single_flat_obs_shape}\")\n",
    "\n",
    "        self.reset_agent_state()\n",
    "        print(\"RLManager: Initialization complete.\")\n",
    "\n",
    "    def _preprocess_observation(self, observation: dict) -> np.ndarray:\n",
    "        processed_observation = observation.copy()\n",
    "        if \"step\" in processed_observation:\n",
    "            step_val = np.array(processed_observation[\"step\"])\n",
    "            processed_observation[\"step\"] = np.clip(step_val, 0, NUM_ITERS - 1).astype(np.int8)\n",
    "        else:\n",
    "            print(\"Warning: 'step' not in observation. Adding default 0.\")\n",
    "            processed_observation[\"step\"] = np.array([0], dtype=np.int8)\n",
    "\n",
    "        if \"location\" in processed_observation and isinstance(processed_observation[\"location\"], list):\n",
    "            processed_observation[\"location\"] = np.array(processed_observation[\"location\"], dtype=np.int8)\n",
    "        if \"viewcone\" in processed_observation and isinstance(processed_observation[\"viewcone\"], list):\n",
    "            processed_observation[\"viewcone\"] = np.array(processed_observation[\"viewcone\"], dtype=np.uint8)\n",
    "        if \"step\" in processed_observation and isinstance(processed_observation[\"step\"], int):\n",
    "             processed_observation[\"step\"] = np.array([processed_observation[\"step\"]], dtype=np.int8)\n",
    "\n",
    "        flat_obs = flatten(self.observation_space_unflattened, processed_observation)\n",
    "        return flat_obs\n",
    "\n",
    "    def reset_agent_state(self):\n",
    "        print(\"RLManager: Resetting agent state (frame stack).\")\n",
    "        zero_flat_obs = np.zeros(self._single_flat_obs_shape, dtype=np.float32)\n",
    "        self._frame_stack_buffer.clear()\n",
    "        for _ in range(self.frame_stack_size):\n",
    "            self._frame_stack_buffer.append(zero_flat_obs)\n",
    "\n",
    "    def rl(self, observation: dict[str, int | list[int] | list[list[int]]]) -> int:\n",
    "        current_flat_obs = self._preprocess_observation(observation)\n",
    "        current_flat_obs = current_flat_obs.astype(np.float32)\n",
    "        self._frame_stack_buffer.append(current_flat_obs)\n",
    "        stacked_observation_list = list(self._frame_stack_buffer)\n",
    "        model_input = np.concatenate(stacked_observation_list, axis=0)\n",
    "        action, _ = self.model.predict(model_input, deterministic=True)\n",
    "        return int(action)\n",
    "\n",
    "# Example usage (for testing locally, not part of the deployed RLManager typically)\n",
    "if __name__ == '__main__':\n",
    "    print(\"RLManager self-test initiated.\")\n",
    "    # Ensure the model zip file is in the current working directory when running this self-test\n",
    "    # in an environment where __file__ is not defined (like Jupyter).\n",
    "    # Or, provide an absolute path to RLManager.\n",
    "    # model_file_for_test = \"til_ai_marl_ppo_rs_explore_v7_20250520-104436.zip\"\n",
    "    \n",
    "    try:\n",
    "        # If __file__ is not defined, this will assume the model_path is relative to os.getcwd()\n",
    "        manager = RLManager() # Uses the default model path from the constructor\n",
    "\n",
    "        sample_raw_observation = {\n",
    "            \"viewcone\": np.random.randint(0, 256, (7, 5), dtype=np.uint8).tolist(),\n",
    "            \"direction\": np.random.randint(0, 4),\n",
    "            \"scout\": np.random.randint(0, 2),\n",
    "            \"location\": np.random.randint(0, 16, (2,), dtype=np.int8).tolist(),\n",
    "            \"step\": np.random.randint(0, NUM_ITERS)\n",
    "        }\n",
    "        print(f\"\\nSample raw observation: {sample_raw_observation}\")\n",
    "\n",
    "        for i in range(5):\n",
    "            action = manager.rl(sample_raw_observation)\n",
    "            print(f\"Step {i+1}: Received observation, predicted action: {action}\")\n",
    "            sample_raw_observation[\"step\"] = min(NUM_ITERS -1, sample_raw_observation[\"step\"] + 1)\n",
    "            sample_raw_observation[\"location\"][0] = (sample_raw_observation[\"location\"][0] + 1) % 16\n",
    "\n",
    "        print(\"\\nSimulating reset:\")\n",
    "        manager.reset_agent_state()\n",
    "        action_after_reset = manager.rl(sample_raw_observation)\n",
    "        print(f\"Action after reset: {action_after_reset}\")\n",
    "        \n",
    "        print(\"\\nRLManager self-test completed.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"RLManager Self-Test FileNotFoundError: {e}\")\n",
    "        print(\"Ensure the model file is in the correct location (e.g., current working directory for Jupyter test, or update path).\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during RLManager self-test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918b311-966f-4bfa-a8cf-36950cf5249a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
