{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd2987f-7f6f-471d-aca5-7a0b5f2d3241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training environment (Novice mode: True)...\n",
      "Observation space type: <class 'gymnasium.spaces.box.Box'>\n",
      "Observation space shape: (572,)\n",
      "Observation space dtype: int64\n",
      "Action space: Discrete(5)\n",
      "Number of environments (agents passed to SB3): 4\n",
      "Starting training with PPO and MlpPolicy for 100000 timesteps.\n",
      "Using cpu device\n",
      "Logging to ./til_marl_tensorboard/PPO_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/conda/envs/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter \n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/conda/envs/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter \n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 647  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 636           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 25            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037209538 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -0.155        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.91          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    value_loss           | 4.22          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02af610b9594a70823b13f4efc7644d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002950217 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -0.065       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.98         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 4.2          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 615           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038065546 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -0.0191       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.68          |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    value_loss           | 3.57          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004590539 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 0.0307       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.29         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 8.78         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 601           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 81            |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059594505 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.6          |\n",
      "|    explained_variance   | 0.105         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.06          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.0017       |\n",
      "|    value_loss           | 4.28          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008234307 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | 0.0403       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.23         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    value_loss           | 14.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 596          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009668194 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | 0.203        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.61         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 5.42         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 595           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 123           |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065588654 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.58         |\n",
      "|    explained_variance   | 0.0809        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.33          |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00192      |\n",
      "|    value_loss           | 14.9          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 594          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007262474 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.96         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0028      |\n",
      "|    value_loss           | 10.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 590          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 152          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016217978 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | 0.287        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    value_loss           | 3.35         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 590          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013788105 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.93         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 16.1         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 588           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 180           |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072810624 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.196         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.5           |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.00229      |\n",
      "|    value_loss           | 9.16          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./til_ai_marl_ppo_20250519-154146.zip\n",
      "Training finished.\n",
      "\n",
      "Starting evaluation (Novice: True, Games: 5, Render: None)...\n",
      "Game 1 finished. Rewards this game: {'player_0': 7.0, 'player_1': 0.0, 'player_2': 0.0, 'player_3': 0.0}\n",
      "Game 2 finished. Rewards this game: {'player_0': 0.0, 'player_1': 7.0, 'player_2': 0.0, 'player_3': 0.0}\n",
      "Game 3 finished. Rewards this game: {'player_0': 0.0, 'player_1': 0.0, 'player_2': 7.0, 'player_3': 0.0}\n",
      "Game 4 finished. Rewards this game: {'player_0': 0.0, 'player_1': 0.0, 'player_2': 0.0, 'player_3': 7.0}\n",
      "Game 5 finished. Rewards this game: {'player_0': 7.0, 'player_1': 0.0, 'player_2': 0.0, 'player_3': 0.0}\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "Average rewards per agent over 5 games: {'player_0': 2.8, 'player_1': 1.4, 'player_2': 1.4, 'player_3': 1.4}\n",
      "Sum of average rewards for all agents (team perspective): 7.0000\n",
      "Example TIL-AI style score for 'player_0' (avg per game / 100): 0.0280\n",
      "\n",
      "Starting evaluation (Novice: True, Games: 1, Render: rgb_array)...\n",
      "Game 1 finished. Rewards this game: {'player_0': 7.0, 'player_1': 0.0, 'player_2': 0.0, 'player_3': 0.0}\n",
      "Saved video of game 0 to ./logs/videos_marl/til_ai_marl_ppo_game_0.mp4\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "Average rewards per agent over 1 games: {'player_0': 7.0, 'player_1': 0.0, 'player_2': 0.0, 'player_3': 0.0}\n",
      "Sum of average rewards for all agents (team perspective): 7.0000\n",
      "Example TIL-AI style score for 'player_0' (avg per game / 100): 0.0700\n",
      "\n",
      "To view videos, check the '/home/jupyter/Tim Testing/logs/videos_marl' directory.\n",
      "To view TensorBoard logs (if enabled and tensorboard installed), run: tensorboard --logdir ./til_marl_tensorboard/\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import functools # Make sure functools is imported\n",
    "from functools import partial\n",
    "\n",
    "import imageio\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "# Import the TIL-AI environment\n",
    "from til_environment import gridworld\n",
    "from til_environment.gridworld import NUM_ITERS # Import NUM_ITERS\n",
    "from til_environment.flatten_dict import FlattenDictWrapper # For explicit wrapper list\n",
    "from supersuit import frame_stack_v2 # For explicit wrapper list\n",
    "\n",
    "# Import the SB3 wrapper from SuperSuit and types for monkey-patching\n",
    "from supersuit.vector.sb3_vector_wrapper import SB3VecEnvWrapper\n",
    "import types\n",
    "\n",
    "# For custom wrapper type hints\n",
    "from pettingzoo.utils.env import ActionType, AECEnv, AgentID, ObsType\n",
    "from pettingzoo.utils.wrappers.base import BaseWrapper\n",
    "\n",
    "\n",
    "# --- Custom Wrapper to Clip Step Observation ---\n",
    "class ClipStepObservationWrapper(BaseWrapper[AgentID, ObsType, ActionType]):\n",
    "    def __init__(self, env: AECEnv[AgentID, ObsType, ActionType]):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observe(self, agent: AgentID) -> ObsType | None:\n",
    "        obs = super().observe(agent)\n",
    "        if obs is not None and isinstance(obs, dict) and \"step\" in obs:\n",
    "            obs[\"step\"] = min(obs[\"step\"], NUM_ITERS - 1)\n",
    "        return obs\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent: AgentID):\n",
    "        return super().observation_space(agent)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent: AgentID):\n",
    "        return super().action_space(agent)\n",
    "\n",
    "\n",
    "# --- Training Parameters ---\n",
    "TOTAL_TIMESTEPS = 100_000\n",
    "RANDOM_SEED = 42\n",
    "MODEL_SAVE_NAME = \"til_ai_marl_ppo\"\n",
    "NOVICE_MODE = True\n",
    "LEARNING_RATE = 0.0003\n",
    "\n",
    "\n",
    "# --- Environment Setup ---\n",
    "custom_env_wrappers_list = [\n",
    "    ClipStepObservationWrapper,\n",
    "    FlattenDictWrapper,\n",
    "    partial(frame_stack_v2, stack_size=4, stack_dim=-1),\n",
    "]\n",
    "\n",
    "train_env_kwargs = {\n",
    "    \"render_mode\": None,\n",
    "    \"novice\": NOVICE_MODE,\n",
    "    \"env_wrappers\": custom_env_wrappers_list,\n",
    "}\n",
    "\n",
    "def create_training_env():\n",
    "    env = gridworld.parallel_env(**train_env_kwargs)\n",
    "    env = ss.black_death_v3(env)\n",
    "    return env\n",
    "\n",
    "print(f\"Initializing training environment (Novice mode: {NOVICE_MODE})...\")\n",
    "\n",
    "pz_env = create_training_env()\n",
    "ss_markov_vec_env = ss.pettingzoo_env_to_vec_env_v1(pz_env)\n",
    "\n",
    "def no_op_seed_for_markov_vec_env(self, seed=None):\n",
    "    pass\n",
    "ss_markov_vec_env.seed = types.MethodType(no_op_seed_for_markov_vec_env, ss_markov_vec_env)\n",
    "\n",
    "vec_env = SB3VecEnvWrapper(ss_markov_vec_env)\n",
    "\n",
    "N_STEPS_PPO = 2048\n",
    "BATCH_SIZE_PPO = N_STEPS_PPO * vec_env.num_envs\n",
    "\n",
    "# MODIFICATION: Avoid printing the full observation space directly if it causes RecursionError\n",
    "# print(f\"Observation space: {vec_env.observation_space}\") # Problematic line\n",
    "print(f\"Observation space type: {type(vec_env.observation_space)}\")\n",
    "print(f\"Observation space shape: {vec_env.observation_space.shape}\")\n",
    "print(f\"Observation space dtype: {vec_env.observation_space.dtype}\")\n",
    "\n",
    "print(f\"Action space: {vec_env.action_space}\") # Action space is usually simpler to print\n",
    "print(f\"Number of environments (agents passed to SB3): {vec_env.num_envs}\")\n",
    "\n",
    "\n",
    "# --- Model Training ---\n",
    "print(f\"Starting training with PPO and MlpPolicy for {TOTAL_TIMESTEPS} timesteps.\")\n",
    "model = PPO(\n",
    "    MlpPolicy,\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    seed=RANDOM_SEED,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_steps=N_STEPS_PPO,\n",
    "    batch_size=BATCH_SIZE_PPO,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    tensorboard_log=\"./til_marl_tensorboard/\",\n",
    "    device=\"cpu\" \n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True)\n",
    "\n",
    "# --- Save Model ---\n",
    "model_filename = f\"{MODEL_SAVE_NAME}_{time.strftime('%Y%m%d-%H%M%S')}.zip\"\n",
    "model_path = os.path.join(\".\", model_filename)\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "vec_env.close()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_til_ai_marl(\n",
    "    num_games: int = 10,\n",
    "    render_mode: str | None = None,\n",
    "    novice_eval: bool = True,\n",
    "    model_to_load_path: str | None = None\n",
    "):\n",
    "    print(f\"\\nStarting evaluation (Novice: {novice_eval}, Games: {num_games}, Render: {render_mode})...\")\n",
    "\n",
    "    eval_env_kwargs = {\n",
    "        \"render_mode\": render_mode,\n",
    "        \"novice\": novice_eval,\n",
    "        \"env_wrappers\": custom_env_wrappers_list,\n",
    "    }\n",
    "    \n",
    "    eval_env = gridworld.env(**eval_env_kwargs)\n",
    "\n",
    "    if model_to_load_path is None:\n",
    "        try:\n",
    "            list_of_models = glob.glob(f\"./{MODEL_SAVE_NAME}*.zip\")\n",
    "            if not list_of_models:\n",
    "                print(\"No trained model found for evaluation. Exiting evaluation.\")\n",
    "                eval_env.close()\n",
    "                return\n",
    "            model_to_load_path = max(list_of_models, key=os.path.getctime)\n",
    "            print(f\"Loading latest model for evaluation: {model_to_load_path}\")\n",
    "        except ValueError:\n",
    "            print(\"Could not find a model to load. Exiting evaluation.\")\n",
    "            eval_env.close()\n",
    "            return\n",
    "            \n",
    "    loaded_model = PPO.load(model_to_load_path, device=\"cpu\")\n",
    "\n",
    "    total_rewards_all_games = {agent: 0.0 for agent in eval_env.possible_agents}\n",
    "    \n",
    "    video_folder = \"./logs/videos_marl\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "    for game_num in range(num_games):\n",
    "        eval_env.reset(seed=RANDOM_SEED + game_num + 1000)\n",
    "        \n",
    "        current_game_frames = []\n",
    "        game_rewards_this_round = {agent: 0.0 for agent in eval_env.possible_agents}\n",
    "\n",
    "        for agent_id in eval_env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = eval_env.last()\n",
    "            \n",
    "            game_rewards_this_round[agent_id] += reward\n",
    "\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "            else:\n",
    "                action, _ = loaded_model.predict(observation, deterministic=True)\n",
    "            \n",
    "            eval_env.step(action)\n",
    "\n",
    "            if render_mode == \"rgb_array\" and action is not None:\n",
    "                try:\n",
    "                    frame = eval_env.render()\n",
    "                    if frame is not None:\n",
    "                        current_game_frames.append(frame)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not render frame for game {game_num}, agent {agent_id}: {e}\")\n",
    "        \n",
    "        for agent_id_sum in eval_env.possible_agents:\n",
    "            total_rewards_all_games[agent_id_sum] += game_rewards_this_round[agent_id_sum]\n",
    "        \n",
    "        print(f\"Game {game_num + 1} finished. Rewards this game: {game_rewards_this_round}\")\n",
    "\n",
    "        if render_mode == \"rgb_array\" and current_game_frames:\n",
    "            video_path = os.path.join(video_folder, f\"{MODEL_SAVE_NAME}_game_{game_num}.mp4\")\n",
    "            try:\n",
    "                imageio.mimsave(video_path, current_game_frames, fps=eval_env.metadata.get(\"render_fps\", 10))\n",
    "                print(f\"Saved video of game {game_num} to {video_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving video for game {game_num}: {e}\")\n",
    "    \n",
    "    eval_env.close()\n",
    "\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    avg_rewards_per_agent = {\n",
    "        agent: total_rewards_all_games[agent] / num_games for agent in eval_env.possible_agents\n",
    "    }\n",
    "    print(f\"Average rewards per agent over {num_games} games: {avg_rewards_per_agent}\")\n",
    "    \n",
    "    if eval_env.possible_agents:\n",
    "        team_avg_reward = sum(avg_rewards_per_agent.values())\n",
    "        print(f\"Sum of average rewards for all agents (team perspective): {team_avg_reward:.4f}\")\n",
    "        \n",
    "        main_agent_example = eval_env.possible_agents[0]\n",
    "        til_score_example = avg_rewards_per_agent[main_agent_example] / 100\n",
    "        print(f\"Example TIL-AI style score for '{main_agent_example}' (avg per game / 100): {til_score_example:.4f}\")\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_til_ai_marl(\n",
    "        num_games=5,\n",
    "        render_mode=None,\n",
    "        novice_eval=NOVICE_MODE,\n",
    "        model_to_load_path=model_path\n",
    "    )\n",
    "    \n",
    "    evaluate_til_ai_marl(\n",
    "        num_games=1,\n",
    "        render_mode=\"rgb_array\",\n",
    "        novice_eval=NOVICE_MODE,\n",
    "        model_to_load_path=model_path\n",
    "    )\n",
    "    print(f\"\\nTo view videos, check the '{os.path.abspath('./logs/videos_marl/')}' directory.\")\n",
    "    print(\"To view TensorBoard logs (if enabled and tensorboard installed), run: tensorboard --logdir ./til_marl_tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9ccb7f-0f1e-434e-ac97-e66f2855a98c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 33177), started 0:06:35 ago. (Use '!kill 33177' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c7107f22a9e81184\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c7107f22a9e81184\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./til_marl_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d488479-44a9-4d30-afa7-28fc9a6d290e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting numpy>=1.12.0 (from tensorboard)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting packaging (from tensorboard)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Using cached protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard)\n",
      "  Using cached setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>1.9 (from tensorboard)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m143.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m145.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)\n",
      "Using cached setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Installing collected packages: tensorboard-data-server, six, setuptools, protobuf, packaging, numpy, MarkupSafe, markdown, grpcio, absl-py, werkzeug, tensorboard\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.8.2\n",
      "    Uninstalling setuptools-75.8.2:\n",
      "      Successfully uninstalled setuptools-75.8.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/envs/jupyterlab4-preview/lib/python3.10/site-packages/google/~rotobuf'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.71.0\n",
      "    Uninstalling grpcio-1.71.0:\n",
      "      Successfully uninstalled grpcio-1.71.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/envs/jupyterlab4-preview/lib/python3.10/site-packages/~rpc'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.2.2 grpcio-1.71.0 markdown-3.8 numpy-2.2.6 packaging-25.0 protobuf-6.31.0 setuptools-80.7.1 six-1.17.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95688c33-8ef5-45ea-925f-9b3c5567844e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
