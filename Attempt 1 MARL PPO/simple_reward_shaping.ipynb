{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2987f-7f6f-471d-aca5-7a0b5f2d3241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom rewards_dict (v7): {<RewardNames.GUARD_CAPTURES: 'guard_captures'>: 50, <RewardNames.SCOUT_CAPTURED: 'scout_captured'>: -50, <RewardNames.SCOUT_RECON: 'scout_recon'>: 5, <RewardNames.SCOUT_MISSION: 'scout_mission'>: 20, <RewardNames.WALL_COLLISION: 'wall_collision'>: -0.5, <RewardNames.STATIONARY_PENALTY: 'stationary_penalty'>: -0.2, <RewardNames.SCOUT_STEP: 'scout_step'>: -0.01, <RewardNames.GUARD_STEP: 'guard_step'>: -0.02, <RewardNames.GUARD_TRUNCATION: 'guard_truncation'>: -10, <RewardNames.SCOUT_TRUNCATION: 'scout_truncation'>: 0}\n",
      "Initializing training environment (Novice mode: True)...\n",
      "Observation space type: <class 'gymnasium.spaces.box.Box'>\n",
      "Observation space shape: (572,)\n",
      "Observation space dtype: int64\n",
      "Action space: Discrete(5)\n",
      "Number of environments (agents passed to SB3): 4\n",
      "Starting training with PPO and MlpPolicy for 5000000 timesteps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c253c99c88f24be2a2e702d54b182ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "import imageio\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "# Import the TIL-AI environment\n",
    "from til_environment import gridworld\n",
    "from til_environment.gridworld import NUM_ITERS\n",
    "from til_environment.flatten_dict import FlattenDictWrapper\n",
    "from supersuit import frame_stack_v2\n",
    "from til_environment.types import RewardNames, Action\n",
    "\n",
    "from supersuit.vector.sb3_vector_wrapper import SB3VecEnvWrapper\n",
    "import types\n",
    "\n",
    "from pettingzoo.utils.env import ActionType, AECEnv, AgentID, ObsType\n",
    "from pettingzoo.utils.wrappers.base import BaseWrapper\n",
    "\n",
    "# --- Custom Wrapper to Clip Step Observation ---\n",
    "class ClipStepObservationWrapper(BaseWrapper[AgentID, ObsType, ActionType]):\n",
    "    def __init__(self, env: AECEnv[AgentID, ObsType, ActionType]):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observe(self, agent: AgentID) -> ObsType | None:\n",
    "        obs = super().observe(agent)\n",
    "        if obs is not None and isinstance(obs, dict) and \"step\" in obs:\n",
    "            obs[\"step\"] = min(obs[\"step\"], NUM_ITERS - 1)\n",
    "        return obs\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent: AgentID):\n",
    "        return super().observation_space(agent)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent: AgentID):\n",
    "        return super().action_space(agent)\n",
    "\n",
    "# --- Reward Shaping Definition (v7) ---\n",
    "rewards_dict = {\n",
    "    RewardNames.GUARD_CAPTURES: 50,\n",
    "    RewardNames.SCOUT_CAPTURED: -50,\n",
    "    RewardNames.SCOUT_RECON: 5,\n",
    "    RewardNames.SCOUT_MISSION: 20,\n",
    "    RewardNames.WALL_COLLISION: -0.5,\n",
    "    RewardNames.STATIONARY_PENALTY: -0.2,  # Stronger penalty for staying still or spinning\n",
    "    RewardNames.SCOUT_STEP: -0.01,\n",
    "    RewardNames.GUARD_STEP: -0.02,         # Stronger penalty for guards to encourage movement\n",
    "    RewardNames.GUARD_TRUNCATION: -10,\n",
    "    RewardNames.SCOUT_TRUNCATION: 0,\n",
    "}\n",
    "print(f\"Using custom rewards_dict (v7): {rewards_dict}\")\n",
    "\n",
    "# --- Training Parameters ---\n",
    "TOTAL_TIMESTEPS = 5_000_000  # For improved performance\n",
    "RANDOM_SEED = 42\n",
    "MODEL_SAVE_NAME = \"til_ai_marl_ppo_rs_explore_v7\"\n",
    "NOVICE_MODE = True\n",
    "LEARNING_RATE = 0.0003\n",
    "\n",
    "custom_env_wrappers_list = [\n",
    "    ClipStepObservationWrapper,\n",
    "    FlattenDictWrapper,\n",
    "    partial(frame_stack_v2, stack_size=4, stack_dim=-1),\n",
    "]\n",
    "\n",
    "train_env_kwargs = {\n",
    "    \"render_mode\": None,\n",
    "    \"novice\": NOVICE_MODE,\n",
    "    \"env_wrappers\": custom_env_wrappers_list,\n",
    "    \"rewards_dict\": rewards_dict,\n",
    "}\n",
    "\n",
    "def create_training_env():\n",
    "    env = gridworld.parallel_env(**train_env_kwargs)\n",
    "    env = ss.black_death_v3(env)\n",
    "    return env\n",
    "\n",
    "print(f\"Initializing training environment (Novice mode: {NOVICE_MODE})...\")\n",
    "\n",
    "pz_env = create_training_env()\n",
    "ss_markov_vec_env = ss.pettingzoo_env_to_vec_env_v1(pz_env)\n",
    "\n",
    "def no_op_seed_for_markov_vec_env(self, seed=None):\n",
    "    pass\n",
    "ss_markov_vec_env.seed = types.MethodType(no_op_seed_for_markov_vec_env, ss_markov_vec_env)\n",
    "\n",
    "vec_env = SB3VecEnvWrapper(ss_markov_vec_env)\n",
    "\n",
    "N_STEPS_PPO = 2048\n",
    "BATCH_SIZE_PPO = N_STEPS_PPO * vec_env.num_envs\n",
    "\n",
    "print(f\"Observation space type: {type(vec_env.observation_space)}\")\n",
    "print(f\"Observation space shape: {vec_env.observation_space.shape}\")\n",
    "print(f\"Observation space dtype: {vec_env.observation_space.dtype}\")\n",
    "\n",
    "print(f\"Action space: {vec_env.action_space}\")\n",
    "print(f\"Number of environments (agents passed to SB3): {vec_env.num_envs}\")\n",
    "\n",
    "# --- Model Training ---\n",
    "print(f\"Starting training with PPO and MlpPolicy for {TOTAL_TIMESTEPS} timesteps.\")\n",
    "model = PPO(\n",
    "    MlpPolicy,\n",
    "    vec_env,\n",
    "    verbose=0,  # Set to 0 to suppress logging output\n",
    "    seed=RANDOM_SEED,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_steps=N_STEPS_PPO,\n",
    "    batch_size=BATCH_SIZE_PPO,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.001,\n",
    "    tensorboard_log=\"./til_marl_tensorboard_rs_explore_v7/\",\n",
    "    device=\"cpu\"  # Change to \"cuda\" if GPU is available and configured\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True)\n",
    "\n",
    "# --- Save Model ---\n",
    "model_filename = f\"{MODEL_SAVE_NAME}_{time.strftime('%Y%m%d-%H%M%S')}.zip\"\n",
    "model_path = os.path.join(\".\", model_filename)\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "vec_env.close()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_til_ai_marl(\n",
    "    num_games: int = 10,\n",
    "    render_mode: str | None = None,\n",
    "    novice_eval: bool = True,\n",
    "    model_to_load_path: str | None = None,\n",
    "    eval_device: str = \"cpu\",\n",
    "    print_actions: bool = False\n",
    "):\n",
    "    print(f\"\\nStarting evaluation (Novice: {novice_eval}, Games: {num_games}, Render: {render_mode}, Device: {eval_device})...\")\n",
    "\n",
    "    eval_rewards_dict = rewards_dict\n",
    "\n",
    "    eval_env_kwargs = {\n",
    "        \"render_mode\": render_mode,\n",
    "        \"novice\": novice_eval,\n",
    "        \"env_wrappers\": custom_env_wrappers_list, \n",
    "        \"rewards_dict\": eval_rewards_dict, \n",
    "    }\n",
    "    \n",
    "    eval_env = gridworld.env(**eval_env_kwargs)\n",
    "\n",
    "    if model_to_load_path is None:\n",
    "        try:\n",
    "            list_of_models = glob.glob(f\"./{MODEL_SAVE_NAME}*.zip\") \n",
    "            if not list_of_models:\n",
    "                print(f\"No trained model found matching pattern ./{MODEL_SAVE_NAME}*.zip. Exiting evaluation.\")\n",
    "                eval_env.close()\n",
    "                return\n",
    "            model_to_load_path = max(list_of_models, key=os.path.getctime)\n",
    "            print(f\"Loading latest model for evaluation: {model_to_load_path}\")\n",
    "        except ValueError:\n",
    "            print(f\"Could not find a model to load with pattern ./{MODEL_SAVE_NAME}*.zip or list was empty. Exiting evaluation.\")\n",
    "            eval_env.close()\n",
    "            return\n",
    "            \n",
    "    loaded_model = PPO.load(model_to_load_path, device=eval_device)\n",
    "\n",
    "    total_rewards_all_games = {agent: 0.0 for agent in eval_env.possible_agents}\n",
    "    \n",
    "    video_folder = \"./logs/videos_marl_rs_explore_v7\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "    for game_num in range(num_games):\n",
    "        eval_env.reset(seed=RANDOM_SEED + game_num + 1000)\n",
    "        \n",
    "        current_game_frames = []\n",
    "        game_rewards_this_round = {agent: 0.0 for agent in eval_env.possible_agents}\n",
    "        action_counts_this_game = {act: 0 for act in Action}\n",
    "\n",
    "        for agent_id in eval_env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = eval_env.last()\n",
    "            \n",
    "            game_rewards_this_round[agent_id] += reward\n",
    "\n",
    "            if termination or truncation:\n",
    "                action_val = None\n",
    "            else:\n",
    "                action_val, _ = loaded_model.predict(observation, deterministic=True)\n",
    "            \n",
    "            eval_env.step(action_val)\n",
    "\n",
    "            if action_val is not None:\n",
    "                action_enum_member = Action(action_val)\n",
    "                action_counts_this_game[action_enum_member] +=1\n",
    "                if print_actions:\n",
    "                    print(f\"Game {game_num+1}, Agent {agent_id}, Action: {action_enum_member.name} ({action_val})\")\n",
    "\n",
    "            if render_mode == \"rgb_array\" and action_val is not None:\n",
    "                try:\n",
    "                    frame = eval_env.render()\n",
    "                    if frame is not None:\n",
    "                        current_game_frames.append(frame)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not render frame for game {game_num+1}, agent {agent_id}: {e}\")\n",
    "        \n",
    "        for agent_id_sum in eval_env.possible_agents:\n",
    "            total_rewards_all_games[agent_id_sum] += game_rewards_this_round[agent_id_sum]\n",
    "        \n",
    "        print(f\"Game {game_num + 1} finished. Rewards this game: {game_rewards_this_round}\")\n",
    "        print(f\"Action distribution for Game {game_num + 1}: { {act.name: count for act, count in action_counts_this_game.items()} }\")\n",
    "\n",
    "        if render_mode == \"rgb_array\" and current_game_frames:\n",
    "            video_path = os.path.join(video_folder, f\"{MODEL_SAVE_NAME}_game_{game_num}.mp4\")\n",
    "            try:\n",
    "                imageio.mimsave(video_path, current_game_frames, fps=eval_env.metadata.get(\"render_fps\", 10))\n",
    "                print(f\"Saved video of game {game_num+1} to {video_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving video for game {game_num+1}: {e}\")\n",
    "    \n",
    "    eval_env.close()\n",
    "\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    avg_rewards_per_agent = {\n",
    "        agent: total_rewards_all_games[agent] / num_games for agent in eval_env.possible_agents\n",
    "    }\n",
    "    print(f\"Average rewards per agent over {num_games} games: {avg_rewards_per_agent}\")\n",
    "    \n",
    "    if eval_env.possible_agents:\n",
    "        team_avg_reward = sum(avg_rewards_per_agent.values())\n",
    "        print(f\"Sum of average rewards for all agents (team perspective): {team_avg_reward:.4f}\")\n",
    "        \n",
    "        scout_agent_example = None\n",
    "        for agent_name in eval_env.possible_agents:\n",
    "            if \"scout\" in agent_name.lower():\n",
    "                scout_agent_example = agent_name\n",
    "                break\n",
    "        if not scout_agent_example and eval_env.possible_agents:\n",
    "            scout_agent_example = eval_env.possible_agents[0]\n",
    "\n",
    "        official_scout_recon_reward = 1 \n",
    "        official_scout_mission_reward = 5\n",
    "        print(f\"Note: The avg_rewards_per_agent is based on the training rewards_dict values.\")\n",
    "        print(f\"A true qualifier score would need evaluation with official reward values.\")\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_til_ai_marl(\n",
    "        num_games=2,\n",
    "        render_mode=None,\n",
    "        novice_eval=NOVICE_MODE,\n",
    "        model_to_load_path=model_path, \n",
    "        eval_device=\"cpu\",\n",
    "        print_actions=True\n",
    "    )\n",
    "    \n",
    "    evaluate_til_ai_marl(\n",
    "        num_games=1,\n",
    "        render_mode=\"rgb_array\",\n",
    "        novice_eval=NOVICE_MODE,\n",
    "        model_to_load_path=model_path, \n",
    "        eval_device=\"cpu\",\n",
    "        print_actions=False\n",
    "    )\n",
    "    print(f\"\\nTo view videos, check the '{os.path.abspath('./logs/videos_marl_rs_explore_v7/')}' directory.\")\n",
    "    print(\"To view TensorBoard logs (if enabled and tensorboard installed), run: tensorboard --logdir ./til_marl_tensorboard_rs_explore_v7/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ccb7f-0f1e-434e-ac97-e66f2855a98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af516fb-b377-4657-bd7e-74c1d8a5711f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./til_marl_tensorboard_rs_explore_v6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b2e0f-6f4d-497f-a2f0-4409c91d295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
