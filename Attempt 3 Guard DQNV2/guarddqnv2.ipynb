{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67beef4-8323-4454-8cc0-17bc050a0fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported til_environment.gridworld\n",
      "Possible agents: ['player_0', 'player_1', 'player_2', 'player_3']\n",
      "Training agent: player_1\n",
      "Action space for player_1: Discrete(5)\n",
      "Observation space for player_1: Dict('direction': Discrete(4), 'location': Box(0, 16, (2,), uint8), 'scout': Discrete(2), 'step': Discrete(100), 'viewcone': Box(0, 255, (7, 5), uint8))\n",
      "Using device: cuda\n",
      "Loaded pre-trained policy_net from guard_6200_eps_no_scout.pth\n",
      "Using device: cuda\n",
      "Error loading model from agent_reward_12000_eps.pth: Error(s) in loading state_dict for DQN2:\n",
      "\tUnexpected key(s) in state_dict: \"fc4.weight\", \"fc4.bias\". \n",
      "\tsize mismatch for fc3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([5, 256]).\n",
      "\tsize mismatch for fc3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([5]).. Initializing with random weights.\n",
      "Updated target network.\n",
      "Episode 2\tAverage Score (last 100): -167.45\tEpsilon: 0.9980\tTotal Steps: 100Updated target network.\n",
      "Episode 3\tAverage Score (last 100): -157.60\tEpsilon: 0.9970\tTotal Steps: 200Updated target network.\n",
      "Episode 4\tAverage Score (last 100): -167.17\tEpsilon: 0.9960\tTotal Steps: 300Updated target network.\n",
      "Episode 6\tAverage Score (last 100): -165.93\tEpsilon: 0.9940\tTotal Steps: 400Updated target network.\n",
      "Episode 7\tAverage Score (last 100): -169.36\tEpsilon: 0.9930\tTotal Steps: 500Updated target network.\n",
      "Episode 8\tAverage Score (last 100): -172.17\tEpsilon: 0.9920\tTotal Steps: 600Updated target network.\n",
      "Episode 10\tAverage Score (last 100): -161.53\tEpsilon: 0.9900\tTotal Steps: 700Updated target network.\n",
      "Episode 11\tAverage Score (last 100): -165.20\tEpsilon: 0.9891\tTotal Steps: 800Updated target network.\n",
      "Episode 12\tAverage Score (last 100): -166.51\tEpsilon: 0.9881\tTotal Steps: 900Updated target network.\n",
      "Episode 14\tAverage Score (last 100): -160.79\tEpsilon: 0.9861\tTotal Steps: 1000Updated target network.\n",
      "Episode 15\tAverage Score (last 100): -163.53\tEpsilon: 0.9851\tTotal Steps: 1100Updated target network.\n",
      "Episode 16\tAverage Score (last 100): -162.17\tEpsilon: 0.9841\tTotal Steps: 1200Updated target network.\n",
      "Episode 18\tAverage Score (last 100): -152.09\tEpsilon: 0.9822\tTotal Steps: 1300Updated target network.\n",
      "Episode 19\tAverage Score (last 100): -152.56\tEpsilon: 0.9812\tTotal Steps: 1400Updated target network.\n",
      "Episode 20\tAverage Score (last 100): -150.47\tEpsilon: 0.9802\tTotal Steps: 1500Updated target network.\n",
      "Episode 22\tAverage Score (last 100): -152.20\tEpsilon: 0.9782\tTotal Steps: 1600Updated target network.\n",
      "Episode 23\tAverage Score (last 100): -151.14\tEpsilon: 0.9773\tTotal Steps: 1700Updated target network.\n",
      "Episode 24\tAverage Score (last 100): -150.51\tEpsilon: 0.9763\tTotal Steps: 1800Updated target network.\n",
      "Episode 26\tAverage Score (last 100): -150.04\tEpsilon: 0.9743\tTotal Steps: 1900Updated target network.\n",
      "Episode 27\tAverage Score (last 100): -151.59\tEpsilon: 0.9733\tTotal Steps: 2000Updated target network.\n",
      "Episode 28\tAverage Score (last 100): -152.92\tEpsilon: 0.9724\tTotal Steps: 2100Updated target network.\n",
      "Episode 30\tAverage Score (last 100): -155.46\tEpsilon: 0.9704\tTotal Steps: 2200Updated target network.\n",
      "Episode 31\tAverage Score (last 100): -154.31\tEpsilon: 0.9695\tTotal Steps: 2300Updated target network.\n",
      "Episode 32\tAverage Score (last 100): -151.21\tEpsilon: 0.9685\tTotal Steps: 2400Updated target network.\n",
      "Episode 34\tAverage Score (last 100): -150.43\tEpsilon: 0.9666\tTotal Steps: 2500Updated target network.\n",
      "Episode 35\tAverage Score (last 100): -151.44\tEpsilon: 0.9656\tTotal Steps: 2600Updated target network.\n",
      "Episode 36\tAverage Score (last 100): -152.54\tEpsilon: 0.9646\tTotal Steps: 2700Updated target network.\n",
      "Episode 38\tAverage Score (last 100): -152.45\tEpsilon: 0.9627\tTotal Steps: 2800Updated target network.\n",
      "Episode 39\tAverage Score (last 100): -150.67\tEpsilon: 0.9617\tTotal Steps: 2900Updated target network.\n",
      "Episode 40\tAverage Score (last 100): -148.72\tEpsilon: 0.9608\tTotal Steps: 3000Updated target network.\n",
      "Episode 42\tAverage Score (last 100): -148.02\tEpsilon: 0.9588\tTotal Steps: 3100Updated target network.\n",
      "Episode 43\tAverage Score (last 100): -146.51\tEpsilon: 0.9579\tTotal Steps: 3200Updated target network.\n",
      "Episode 44\tAverage Score (last 100): -145.45\tEpsilon: 0.9569\tTotal Steps: 3300Updated target network.\n",
      "Episode 46\tAverage Score (last 100): -149.77\tEpsilon: 0.9550\tTotal Steps: 3400Updated target network.\n",
      "Episode 47\tAverage Score (last 100): -148.16\tEpsilon: 0.9541\tTotal Steps: 3500Updated target network.\n",
      "Episode 48\tAverage Score (last 100): -146.42\tEpsilon: 0.9531\tTotal Steps: 3600Updated target network.\n",
      "Episode 50\tAverage Score (last 100): -147.21\tEpsilon: 0.9512\tTotal Steps: 3700Updated target network.\n",
      "Episode 51\tAverage Score (last 100): -147.02\tEpsilon: 0.9503\tTotal Steps: 3800Updated target network.\n",
      "Episode 52\tAverage Score (last 100): -148.16\tEpsilon: 0.9493\tTotal Steps: 3900Updated target network.\n",
      "Episode 54\tAverage Score (last 100): -149.24\tEpsilon: 0.9474\tTotal Steps: 4000Updated target network.\n",
      "Episode 55\tAverage Score (last 100): -149.23\tEpsilon: 0.9465\tTotal Steps: 4100Updated target network.\n",
      "Episode 56\tAverage Score (last 100): -148.73\tEpsilon: 0.9455\tTotal Steps: 4200Updated target network.\n",
      "Episode 58\tAverage Score (last 100): -148.98\tEpsilon: 0.9436\tTotal Steps: 4300Updated target network.\n",
      "Episode 59\tAverage Score (last 100): -150.42\tEpsilon: 0.9427\tTotal Steps: 4400Updated target network.\n",
      "Episode 60\tAverage Score (last 100): -153.44\tEpsilon: 0.9417\tTotal Steps: 4500Updated target network.\n",
      "Episode 62\tAverage Score (last 100): -150.97\tEpsilon: 0.9399\tTotal Steps: 4600Updated target network.\n",
      "Episode 63\tAverage Score (last 100): -151.31\tEpsilon: 0.9389\tTotal Steps: 4700Updated target network.\n",
      "Episode 64\tAverage Score (last 100): -150.88\tEpsilon: 0.9380\tTotal Steps: 4800Updated target network.\n",
      "Episode 66\tAverage Score (last 100): -151.77\tEpsilon: 0.9361\tTotal Steps: 4900Updated target network.\n",
      "Episode 67\tAverage Score (last 100): -150.51\tEpsilon: 0.9352\tTotal Steps: 5000Updated target network.\n",
      "Episode 68\tAverage Score (last 100): -149.38\tEpsilon: 0.9342\tTotal Steps: 5100Updated target network.\n",
      "Episode 70\tAverage Score (last 100): -148.34\tEpsilon: 0.9324\tTotal Steps: 5200Updated target network.\n",
      "Episode 71\tAverage Score (last 100): -147.60\tEpsilon: 0.9314\tTotal Steps: 5300Updated target network.\n",
      "Episode 72\tAverage Score (last 100): -146.33\tEpsilon: 0.9305\tTotal Steps: 5400Updated target network.\n",
      "Episode 74\tAverage Score (last 100): -146.32\tEpsilon: 0.9286\tTotal Steps: 5500Updated target network.\n",
      "Episode 75\tAverage Score (last 100): -145.93\tEpsilon: 0.9277\tTotal Steps: 5600Updated target network.\n",
      "Episode 76\tAverage Score (last 100): -145.60\tEpsilon: 0.9268\tTotal Steps: 5700"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 775\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSuccessfully imported til_environment.gridworld\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    772\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m    773\u001b[39m \u001b[38;5;66;03m# Set load_model_from to a .pth file to continue training or fine-tune\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Set save_model_to to where you want the final model to be saved\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m trained_scores = train_agent(\n\u001b[32m    776\u001b[39m     gridworld, \n\u001b[32m    777\u001b[39m     num_episodes=\u001b[32m20000\u001b[39m, \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[32m    778\u001b[39m     novice_track=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Or True for the Novice track map\u001b[39;00m\n\u001b[32m    779\u001b[39m     load_scout_model_from=\u001b[33m\"\u001b[39m\u001b[33magent_reward_12000_eps.pth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# \"trained_dqn_agent.pth\" to resume\u001b[39;00m\n\u001b[32m    780\u001b[39m     load_guard_model_from=\u001b[33m\"\u001b[39m\u001b[33mguard_6200_eps_no_scout.pth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# \"trained_dqn_agent.pth\" to resume\u001b[39;00m\n\u001b[32m    781\u001b[39m     save_model_to=\u001b[33m\"\u001b[39m\u001b[33mguard_20k_eps_w_scout.pth\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    782\u001b[39m     render_mode=\u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    783\u001b[39m     video_folder=\u001b[33m\"\u001b[39m\u001b[33m./rl_renders_guard\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    784\u001b[39m )\n\u001b[32m    785\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    787\u001b[39m \u001b[38;5;66;03m# You can add plotting for scores if you like:\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 715\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(env_module, num_episodes, novice_track, load_scout_model_from, load_guard_model_from, save_model_to, render_mode, video_folder)\u001b[39m\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    713\u001b[39m         action = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Should not happen if agent is not done\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m env.step(action) \u001b[38;5;66;03m# Step the environment with the chosen action (or None)\u001b[39;00m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m pet_agent_id == my_agent_id \u001b[38;5;129;01mand\u001b[39;00m last_observation_for_my_agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    718\u001b[39m     \u001b[38;5;66;03m# If our agent is done, we need to record the final transition\u001b[39;00m\n\u001b[32m    719\u001b[39m     prev_state_np, prev_action, _ = last_observation_for_my_agent \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:70\u001b[39m, in \u001b[36mOrderEnforcingWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m._has_updated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28msuper\u001b[39m().step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[39m, in \u001b[36mBaseWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py:26\u001b[39m, in \u001b[36mAssertOutOfBoundsWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     18\u001b[39m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m         action\n\u001b[32m     25\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33maction is not in action space\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28msuper\u001b[39m().step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[39m, in \u001b[36mBaseWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/til-25-data-chefs/til-25-environment/til_environment/gridworld.py:820\u001b[39m, in \u001b[36mraw_env.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    818\u001b[39m     \u001b[38;5;66;03m# render\u001b[39;00m\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata[\u001b[33m\"\u001b[39m\u001b[33mrender_modes\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28mself\u001b[39m.render()\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    822\u001b[39m     \u001b[38;5;66;03m# no rewards are allocated until all players give an action\u001b[39;00m\n\u001b[32m    823\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_rewards()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/til-25-data-chefs/til-25-environment/til_environment/gridworld.py:200\u001b[39m, in \u001b[36mraw_env.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata[\u001b[33m\"\u001b[39m\u001b[33mrender_modes\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._render_frame()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/til-25-data-chefs/til-25-environment/til_environment/gridworld.py:366\u001b[39m, in \u001b[36mraw_env._render_frame\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    363\u001b[39m     \u001b[38;5;28mself\u001b[39m.clock.tick(\u001b[38;5;28mself\u001b[39m.metadata[\u001b[33m\"\u001b[39m\u001b[33mrender_fps\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.transpose(\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         np.array(pygame.surfarray.pixels3d(\u001b[38;5;28mself\u001b[39m.window)), axes=(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import imageio\n",
    "from collections import deque, namedtuple\n",
    "from til_environment.gridworld import RewardNames\n",
    "\n",
    "# --- Configuration (Adjust these as needed) ---\n",
    "# Environment specific (match your game)\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100 # Max steps in one round of the game\n",
    "\n",
    "# Neural Network Hyperparameters (from rl_agent_python_v1)\n",
    "INPUT_FEATURES = 288  # 7*5*8 (viewcone) + 4 (direction) + 2 (location) + 1 (scout) + 1 (step)\n",
    "HIDDEN_LAYER_1_SIZE = 512\n",
    "HIDDEN_LAYER_2_SIZE = 256\n",
    "HIDDEN_LAYER_3_SIZE = 256\n",
    "OUTPUT_ACTIONS = 5  # 0:Forward, 1:Backward, 2:TurnL, 3:TurnR, 4:Stay\n",
    "\n",
    "# Training Hyperparameters\n",
    "BUFFER_SIZE = int(1e5)  # Replay buffer size\n",
    "BATCH_SIZE = 32         # Minibatch size for training\n",
    "GAMMA = 0.99            # Discount factor\n",
    "LEARNING_RATE = 1e-4    # Learning rate for the optimizer\n",
    "TARGET_UPDATE_EVERY = 100 # How often to update the target network (in steps)\n",
    "UPDATE_EVERY = 4        # How often to run a learning step (in steps)\n",
    "\n",
    "# Epsilon-greedy exploration parameters (for training)\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.999 # Multiplicative decay factor per episode/fixed number of steps\n",
    "\n",
    "# PER Parameters\n",
    "PER_ALPHA = 0.6  # Prioritization exponent (0 for uniform, 1 for full prioritization)\n",
    "PER_BETA_START = 0.4 # Initial importance sampling exponent\n",
    "PER_BETA_FRAMES = int(1e5) # Number of frames over which beta is annealed to 1.0\n",
    "PER_EPSILON = 1e-6 # Small constant to ensure non-zero priority\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reward shaping\n",
    "EXPLORATION_BONUS_REWARD = 1000\n",
    "CUSTOM_REWARDS_DICT = {\n",
    "    # --- Positive Rewards (Scout achieves goals) ---\n",
    "    RewardNames.SCOUT_MISSION: 0,      # Big positive reward for completing the main objective\n",
    "    RewardNames.SCOUT_RECON: 0,         # Positive reward for collecting recon (intermediate goal)\n",
    "    # RewardNames.SCOUT_TRUNCATION: 0,   # Positive reward for surviving the entire episode without capture\n",
    "\n",
    "    # --- Negative Rewards (Scout fails or undesirable behaviour) ---\n",
    "    # RewardNames.SCOUT_CAPTURED: -100.0,     # Huge negative reward for being captured (terminal state)\n",
    "    # Note: GUARD_WINS and GUARD_CAPTURES also imply the Scout was captured,\n",
    "    # but SCOUT_CAPTURED is explicitly for the Scout agent itself.\n",
    "    # You could potentially add small penalties here if you want, but SCOUT_CAPTURED is the primary signal.\n",
    "    # RewardNames.GUARD_WINS: -10.0, # Optional: small additional penalty if Guards collectively win\n",
    "    # RewardNames.GUARD_CAPTURES: -10.0, # Optional: small additional penalty if a Guard captures\n",
    "\n",
    "    # RewardNames.SCOUT_STEP: -0.5,          # Small negative reward per step (time penalty, encourages efficiency)\n",
    "    RewardNames.WALL_COLLISION: -10.0,       # Penalty for colliding with a wall (discourages invalid moves)\n",
    "    RewardNames.AGENT_COLLIDER: -1.0,       # Penalty for colliding *into* another agent (discourages bumping)\n",
    "    RewardNames.AGENT_COLLIDEE: -1.0,       # Small penalty for being collided *into* (suggests being in a bad position)\n",
    "    RewardNames.STATIONARY_PENALTY: -1.0,   # Small penalty for staying still (encourages movement unless strategically beneficial)\n",
    "\n",
    "    # --- Rewards for other agents (generally keep at 0 or default for Scout's dict) ---\n",
    "    # These would be relevant if training Guards, but not the Scout directly via this dict.\n",
    "    RewardNames.GUARD_WINS: 100.0,\n",
    "    RewardNames.GUARD_CAPTURES: 100.0,\n",
    "    RewardNames.GUARD_TRUNCATION: -10, # This corresponds to SCOUT_TRUNCATION for the Scout\n",
    "    RewardNames.GUARD_STEP: -0.1,\n",
    "}\n",
    "\n",
    "\n",
    "# --- SumTree for Prioritized Replay Buffer ---\n",
    "class SumTree:\n",
    "    \"\"\"\n",
    "    A SumTree is a binary tree data structure where the value of a parent node\n",
    "    is the sum of its children. It is used for efficient sampling from a\n",
    "    distribution. Leaf nodes store priorities, and internal nodes store sums.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1) # Tree storage\n",
    "        self.data = np.zeros(capacity, dtype=object) # Data storage (transitions)\n",
    "        self.data_pointer = 0 # Current position to write new data\n",
    "        self.n_entries = 0 # Current number of entries in the buffer\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        \"\"\"Add priority score and data to the tree.\"\"\"\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0 # Cycle back to the beginning\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        \"\"\"Update priority of a node and propagate changes upwards.\"\"\"\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        # Propagate the change up the tree\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        \"\"\"Find sample on leaf node based on a cumulative sum value.\"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree): # Reached leaf\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        \n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "# --- Prioritized Replay Buffer ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha # Controls how much prioritization is used (0=uniform, 1=full)\n",
    "        self.max_priority = 1.0 # Max priority for new experiences\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adds a new experience to the buffer with max priority.\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences from the buffer.\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "            beta (float): Importance-sampling exponent.\n",
    "        Returns:\n",
    "            tuple: (experiences, indices, weights)\n",
    "        \"\"\"\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "\n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = priority_segment * i\n",
    "            b = priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities, -beta) # (N * P(i))^-beta\n",
    "            batch_idx[i] = index\n",
    "            batch_data[i] = data\n",
    "        \n",
    "        weights /= weights.max() # Normalize for stability\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*[e for e in batch_data])\n",
    "\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        \"\"\"Updates the priorities of sampled experiences.\"\"\"\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON # Add epsilon to ensure non-zero priority\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        \n",
    "        for idx, priority in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority) # Update max_priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- Deep Q-Network (DQN) Model (same as in rl_agent_python_v1) ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class DQN2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DQN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.policy_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, HIDDEN_LAYER_3_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        self.target_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, HIDDEN_LAYER_3_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "                print(f\"Loaded pre-trained policy_net from {model_load_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model from {model_load_path}: {e}. Initializing with random weights.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            print(f\"No model path provided or path {model_load_path} does not exist. Initializing policy_net with random weights.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict()) # Initialize target_net with policy_net weights\n",
    "        self.target_net.eval() # Target network is only for inference\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step = 0 # Counter for triggering learning and target network updates\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES\n",
    "        self.global_step = 0\n",
    "\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value): # Same as in rl_agent_python_v1\n",
    "        tile_features = []\n",
    "        tile_features.append(float(tile_value & 0b01)) \n",
    "        tile_features.append(float((tile_value & 0b10) >> 1))\n",
    "        for i in range(2, 8):\n",
    "            tile_features.append(float((tile_value >> i) & 1))\n",
    "        return tile_features\n",
    "\n",
    "    def process_observation(self, observation_dict): # Same as in rl_agent_python_v1\n",
    "        processed_features = []\n",
    "        viewcone = observation_dict.get(\"viewcone\", [])\n",
    "        for r in range(7):\n",
    "            for c in range(5):\n",
    "                tile_value = viewcone[r][c] if r < len(viewcone) and c < len(viewcone[r]) else 0\n",
    "                processed_features.extend(self._unpack_viewcone_tile(tile_value))\n",
    "        \n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4\n",
    "        if 0 <= direction < 4: direction_one_hot[direction] = 1.0\n",
    "        processed_features.extend(direction_one_hot)\n",
    "\n",
    "        location = observation_dict.get(\"location\", [0, 0])\n",
    "        norm_x = location[0] / MAP_SIZE_X if MAP_SIZE_X > 0 else 0.0\n",
    "        norm_y = location[1] / MAP_SIZE_Y if MAP_SIZE_Y > 0 else 0.0\n",
    "        processed_features.extend([norm_x, norm_y])\n",
    "\n",
    "        scout_role = float(observation_dict.get(\"scout\", 0))\n",
    "        processed_features.append(scout_role)\n",
    "\n",
    "        step = observation_dict.get(\"step\", 0)\n",
    "        norm_step = step / MAX_STEPS_PER_EPISODE if MAX_STEPS_PER_EPISODE > 0 else 0.0\n",
    "        processed_features.append(norm_step)\n",
    "        \n",
    "        # Ensure correct feature length (should be INPUT_FEATURES)\n",
    "        if len(processed_features) != INPUT_FEATURES:\n",
    "            # This indicates an issue with feature processing or constants\n",
    "            raise ValueError(f\"Feature length mismatch. Expected {INPUT_FEATURES}, got {len(processed_features)}\")\n",
    "\n",
    "        return np.array(processed_features, dtype=np.float32) # Return as numpy array for buffer\n",
    "\n",
    "    def select_action(self, state_np, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        Args:\n",
    "            state_np (np.ndarray): Processed state as a numpy array.\n",
    "            epsilon (float): Exploration rate.\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state_tensor = torch.from_numpy(state_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval() # Set to evaluation mode for action selection\n",
    "            with torch.no_grad():\n",
    "                action_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train() # Set back to training mode\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Increment the global step counter every time step is called for the agent\n",
    "        self.global_step += 1\n",
    "\n",
    "        # This part correctly triggers the LEARNING step every UPDATE_EVERY steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                # Perform the learning step\n",
    "                experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "                self.learn(experiences, indices, weights, GAMMA)\n",
    "                # Anneal beta, typically done per learning step or total step\n",
    "                self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "\n",
    "\n",
    "        # This part correctly triggers the TARGET NETWORK UPDATE every TARGET_UPDATE_EVERY global steps\n",
    "        # Use the global_step counter for this check\n",
    "        if self.global_step % TARGET_UPDATE_EVERY == 0:\n",
    "             self.update_target_net() # This method performs the hard copy\n",
    "\n",
    "\n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * Q_target(s', argmax_a Q_policy(s', a))\n",
    "        Args:\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            indices (np.ndarray): indices of these experiences in the SumTree\n",
    "            importance_sampling_weights (torch.Tensor): weights for IS\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from policy network\n",
    "        # This is for Double DQN: action selection from policy_net, evaluation from target_net\n",
    "        q_next_policy = self.policy_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        # Get Q values for next_states from target_net using actions selected by policy_net\n",
    "        q_targets_next = self.target_net(next_states).detach().gather(1, q_next_policy)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from policy_net\n",
    "        q_expected = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        # Compute TD errors for PER\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        # Compute loss (element-wise multiplication with IS weights)\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0) # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def update_target_net(self):\n",
    "        \"\"\"Soft update model parameters: θ_target = τ*θ_local + (1 - τ)*θ_target\"\"\"\n",
    "        # For hard update:\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        print(\"Updated target network.\")\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "        print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_state(self): # For compatibility with potential stateful components, not used in this DQN\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "#######################HARD CODED AGENT W 2 HIDDEN LAYERS #########################################################################################\n",
    "# --- Trainable RL Agent 2---\n",
    "class TrainableRLAgent2:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        # Neural Network Hyperparameters (from rl_agent_python_v1)\n",
    "        INPUT_FEATURES = 288\n",
    "        HIDDEN_LAYER_1_SIZE = 256\n",
    "        HIDDEN_LAYER_2_SIZE = 256\n",
    "        OUTPUT_ACTIONS = 5  # 0:Forward, 1:Backward, 2:TurnL, 3:TurnR, 4:Stay\n",
    "        self.policy_net = DQN2(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        self.target_net = DQN2(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "                print(f\"Loaded pre-trained policy_net from {model_load_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model from {model_load_path}: {e}. Initializing with random weights.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            print(f\"No model path provided or path {model_load_path} does not exist. Initializing policy_net with random weights.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict()) # Initialize target_net with policy_net weights\n",
    "        self.target_net.eval() # Target network is only for inference\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step = 0 # Counter for triggering learning and target network updates\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES\n",
    "        self.global_step = 0\n",
    "\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value): # Same as in rl_agent_python_v1\n",
    "        tile_features = []\n",
    "        tile_features.append(float(tile_value & 0b01)) \n",
    "        tile_features.append(float((tile_value & 0b10) >> 1))\n",
    "        for i in range(2, 8):\n",
    "            tile_features.append(float((tile_value >> i) & 1))\n",
    "        return tile_features\n",
    "\n",
    "    def process_observation(self, observation_dict): # Same as in rl_agent_python_v1\n",
    "        processed_features = []\n",
    "        viewcone = observation_dict.get(\"viewcone\", [])\n",
    "        for r in range(7):\n",
    "            for c in range(5):\n",
    "                tile_value = viewcone[r][c] if r < len(viewcone) and c < len(viewcone[r]) else 0\n",
    "                processed_features.extend(self._unpack_viewcone_tile(tile_value))\n",
    "        \n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4\n",
    "        if 0 <= direction < 4: direction_one_hot[direction] = 1.0\n",
    "        processed_features.extend(direction_one_hot)\n",
    "\n",
    "        location = observation_dict.get(\"location\", [0, 0])\n",
    "        norm_x = location[0] / MAP_SIZE_X if MAP_SIZE_X > 0 else 0.0\n",
    "        norm_y = location[1] / MAP_SIZE_Y if MAP_SIZE_Y > 0 else 0.0\n",
    "        processed_features.extend([norm_x, norm_y])\n",
    "\n",
    "        scout_role = float(observation_dict.get(\"scout\", 0))\n",
    "        processed_features.append(scout_role)\n",
    "\n",
    "        step = observation_dict.get(\"step\", 0)\n",
    "        norm_step = step / MAX_STEPS_PER_EPISODE if MAX_STEPS_PER_EPISODE > 0 else 0.0\n",
    "        processed_features.append(norm_step)\n",
    "        \n",
    "        # Ensure correct feature length (should be INPUT_FEATURES)\n",
    "        if len(processed_features) != INPUT_FEATURES:\n",
    "            # This indicates an issue with feature processing or constants\n",
    "            raise ValueError(f\"Feature length mismatch. Expected {INPUT_FEATURES}, got {len(processed_features)}\")\n",
    "\n",
    "        return np.array(processed_features, dtype=np.float32) # Return as numpy array for buffer\n",
    "\n",
    "    def select_action(self, state_np, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        Args:\n",
    "            state_np (np.ndarray): Processed state as a numpy array.\n",
    "            epsilon (float): Exploration rate.\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state_tensor = torch.from_numpy(state_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval() # Set to evaluation mode for action selection\n",
    "            with torch.no_grad():\n",
    "                action_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train() # Set back to training mode\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Increment the global step counter every time step is called for the agent\n",
    "        self.global_step += 1\n",
    "\n",
    "        # This part correctly triggers the LEARNING step every UPDATE_EVERY steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                # Perform the learning step\n",
    "                experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "                self.learn(experiences, indices, weights, GAMMA)\n",
    "                # Anneal beta, typically done per learning step or total step\n",
    "                self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "\n",
    "\n",
    "        # This part correctly triggers the TARGET NETWORK UPDATE every TARGET_UPDATE_EVERY global steps\n",
    "        # Use the global_step counter for this check\n",
    "        if self.global_step % TARGET_UPDATE_EVERY == 0:\n",
    "             self.update_target_net() # This method performs the hard copy\n",
    "\n",
    "\n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * Q_target(s', argmax_a Q_policy(s', a))\n",
    "        Args:\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            indices (np.ndarray): indices of these experiences in the SumTree\n",
    "            importance_sampling_weights (torch.Tensor): weights for IS\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from policy network\n",
    "        # This is for Double DQN: action selection from policy_net, evaluation from target_net\n",
    "        q_next_policy = self.policy_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        # Get Q values for next_states from target_net using actions selected by policy_net\n",
    "        q_targets_next = self.target_net(next_states).detach().gather(1, q_next_policy)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from policy_net\n",
    "        q_expected = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        # Compute TD errors for PER\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        # Compute loss (element-wise multiplication with IS weights)\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0) # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def update_target_net(self):\n",
    "        \"\"\"Soft update model parameters: θ_target = τ*θ_local + (1 - τ)*θ_target\"\"\"\n",
    "        # For hard update:\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        print(\"Updated target network.\")\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "        print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_state(self): # For compatibility with potential stateful components, not used in this DQN\n",
    "        pass\n",
    "#######################HARD CODED AGENT W 2 HIDDEN LAYERS #########################################################################################\n",
    "\n",
    "# --- Main Training Loop (Example) ---\n",
    "def train_agent(env_module, num_episodes=2000, novice_track=False, load_scout_model_from=None, load_guard_model_from=None, save_model_to=\"trained_guard_dqn_agent.pth\", render_mode=None, video_folder=None):\n",
    "    \"\"\"\n",
    "    Main training loop for the RL agent.\n",
    "    Args:\n",
    "        env_module: The imported environment module (e.g., til_environment.gridworld).\n",
    "        num_episodes (int): Number of episodes to train for.\n",
    "        novice_track (bool): If true, uses the novice environment settings.\n",
    "        load_scout_model_from (str, optional): Path to load a pre-trained scout model.\n",
    "        load_guard_model_from (str, optional): Path to load a pre-trained guard model.\n",
    "        save_model_to (str): Path to save the trained model.\n",
    "    \"\"\"    \n",
    "    # Initialise your game environment\n",
    "    # This assumes your 'til_environment.gridworld' has an 'env' function\n",
    "    # that returns a PettingZoo-like environment.\n",
    "    env = env_module.env(env_wrappers=[], render_mode=render_mode, novice=novice_track, rewards_dict=CUSTOM_REWARDS_DICT)\n",
    "    # env = env_module.env(env_wrappers=[], render_mode=render_mode, novice=novice_track)\n",
    "    \n",
    "    # Create video folder if needed\n",
    "    if render_mode == \"rgb_array\" and video_folder:\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "    \n",
    "    # Assuming your agent is always the first one in possible_agents\n",
    "    # Adjust if your setup is different or if you want to train a specific agent\n",
    "    my_agent_id = env.possible_agents[1] \n",
    "    print(f\"Possible agents: {env.possible_agents}\")\n",
    "    print(f\"Training agent: {my_agent_id}\")\n",
    "    print(f\"Action space for {my_agent_id}: {env.action_space(my_agent_id)}\")\n",
    "    print(f\"Observation space for {my_agent_id}: {env.observation_space(my_agent_id)}\")\n",
    "    \n",
    "    # Train guard agent\n",
    "    guard_agent = TrainableRLAgent(model_load_path=load_guard_model_from, model_save_path=save_model_to)\n",
    "    scout_agent = TrainableRLAgent2(model_load_path=load_scout_model_from, model_save_path=None)\n",
    "    \n",
    "    scores_deque = deque(maxlen=100) # For tracking recent scores\n",
    "    scores = [] # List of scores from all episodes\n",
    "    epsilon = EPSILON_START\n",
    "    total_steps_taken = 0\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset() # Reset environment at the start of each episode\n",
    "        guard_agent.reset_state() # Reset agent's internal state if any (not for this DQN)\n",
    "        \n",
    "        # The environment interaction loop from your test script\n",
    "        current_rewards_this_episode = {agent_id: 0 for agent_id in env.possible_agents}\n",
    "        \n",
    "        # Initialise a set to track visited states\n",
    "        visited_states = set()\n",
    "        \n",
    "        # Get initial observation for our agent\n",
    "        # This part needs careful handling with PettingZoo's agent_iter\n",
    "        # We need to get the first observation for our agent\n",
    "        \n",
    "        # The loop below processes all agents. We only train `my_agent_id`.\n",
    "        # We need to store the state for `my_agent_id` to pass to `agent.step`\n",
    "        \n",
    "        last_observation_for_my_agent = None\n",
    "        \n",
    "        # Initialize frame list ONLY if this episode is a recording episode\n",
    "        episode_frames = []\n",
    "        should_record_video = (render_mode == \"rgb_array\" and i_episode % 100 == 0) # Flag to control recording\n",
    "        \n",
    "        for pet_agent_id in env.agent_iter(): # PettingZoo's iterator\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            # Capture frame ONLY if recording is enabled for this episode\n",
    "            if should_record_video: # Use the flag\n",
    "                try:\n",
    "                    frame = env.render()\n",
    "                    if frame is not None:\n",
    "                        episode_frames.append(frame)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nWarning: Could not render frame for episode {i_episode}, step {guard_agent.global_step}: {e}\")\n",
    "                    traceback.print_exc()\n",
    " \n",
    "            # Accumulate rewards for all agents for this step\n",
    "            for ag_id in env.agents: # env.agents are live agents in current step\n",
    "                 current_rewards_this_episode[ag_id] += env.rewards.get(ag_id, 0)\n",
    "\n",
    "            done = termination or truncation\n",
    "\n",
    "            if done: # If an agent is done, it might not take an action\n",
    "                action = None # PettingZoo expects None if agent is done\n",
    "            # Scout's action\n",
    "            elif observation[\"scout\"] == 1:\n",
    "                # It's scout's turn\n",
    "                # 1. Process observation\n",
    "                obs_dict = {k: v if isinstance(v, (int, float)) else v.tolist() for k, v in observation.items()}\n",
    "                current_state_np = scout_agent.process_observation(obs_dict)\n",
    "\n",
    "                # 2. Select action based on pre-trained scout\n",
    "                action = scout_agent.select_action(current_state_np, epsilon)\n",
    "\n",
    "            # Our Guard's action\n",
    "            elif pet_agent_id == my_agent_id and observation[\"scout\"] == 0:\n",
    "                # It's our agent's turn\n",
    "                # 1. Process observation\n",
    "                obs_dict = {k: v if isinstance(v, (int, float)) else v.tolist() for k, v in observation.items()}\n",
    "                current_state_np = guard_agent.process_observation(obs_dict)\n",
    "                current_location = tuple(obs_dict.get(\"location\", [None, None]))\n",
    "                current_exploration_bonus = 0.0\n",
    "                if current_location != (None,None) and current_location not in visited_states:\n",
    "                    visited_states.add(current_location)\n",
    "                    current_exploration_bonus = EXPLORATION_BONUS_REWARD\n",
    "                \n",
    "                # Add in reward for visiting new state\n",
    "                reward += current_exploration_bonus\n",
    "                \n",
    "                # 2. Store previous transition if available\n",
    "                if last_observation_for_my_agent is not None:\n",
    "                    # last_observation_for_my_agent = (prev_state, prev_action, prev_reward_for_my_agent)\n",
    "                    prev_state_np, prev_action, prev_reward = last_observation_for_my_agent\n",
    "                    # The reward for the (s,a) pair is what we received *after* taking action 'a' in state 's'\n",
    "                    # which is the 'reward' variable from env.last() *now*\n",
    "                    guard_agent.step(prev_state_np, prev_action, reward, current_state_np, done)\n",
    "                    total_steps_taken +=1\n",
    "\n",
    "                # 3. Select action\n",
    "                action = guard_agent.select_action(current_state_np, epsilon)\n",
    "\n",
    "                # 4. Store current state, action, and this step's reward for the *next* transition\n",
    "                last_observation_for_my_agent = (current_state_np, action, reward) # reward here is for the current (s,a)\n",
    "\n",
    "            else:\n",
    "                # Other agents take random actions (or use their own policies if implemented)\n",
    "                if env.action_space(pet_agent_id) is not None:\n",
    "                     action = env.action_space(pet_agent_id).sample()\n",
    "                else:\n",
    "                    action = None # Should not happen if agent is not done\n",
    "\n",
    "            env.step(action) # Step the environment with the chosen action (or None)\n",
    "            \n",
    "            if done and pet_agent_id == my_agent_id and last_observation_for_my_agent is not None:\n",
    "                # If our agent is done, we need to record the final transition\n",
    "                prev_state_np, prev_action, _ = last_observation_for_my_agent \n",
    "                # The final reward is `reward` from env.last() when done is true\n",
    "                # The next_state is not critical as it's a terminal state, can be zeros or current_state_np\n",
    "                final_next_state_np = np.zeros_like(prev_state_np) # Or current_state_np\n",
    "                guard_agent.step(prev_state_np, prev_action, reward, final_next_state_np, True)\n",
    "                total_steps_taken +=1\n",
    "                last_observation_for_my_agent = None # Reset for next episode start\n",
    "\n",
    "        # End of episode\n",
    "        episode_score = current_rewards_this_episode[my_agent_id]\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        \n",
    "        epsilon = max(EPSILON_END, EPSILON_DECAY * epsilon) # Decay epsilon\n",
    "        \n",
    "        # Save video at the end of the episode ONLY if frames were collected\n",
    "        if should_record_video and video_folder and len(episode_frames) > 0: # Check the flag\n",
    "            video_path = os.path.join(video_folder, f\"episode_{i_episode:06d}.mp4\") # Use 6 digits for episode number\n",
    "            try:\n",
    "                # imageio needs the frames to be in (T, H, W, C) format, which env.render() provides\n",
    "                imageio.mimsave(video_path, episode_frames, fps=30) # Adjust fps as needed\n",
    "                print(f\"\\nSaved video for episode {i_episode} to {video_path}\") # Print on a new line after progress\n",
    "            except Exception as e:\n",
    "                print(f\"\\nWarning: Could not save video for episode {i_episode} to {video_path}: {e}\")\n",
    "                traceback.print_exc()\n",
    "       \n",
    "        print(f'\\rEpisode {i_episode}\\tAverage Score (last 100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tTotal Steps: {total_steps_taken}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tAverage Score (last 100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tTotal Steps: {total_steps_taken}')\n",
    "            guard_agent.save_model()\n",
    "        \n",
    "        if np.mean(scores_deque) >= 2000.0: # Example condition to stop training\n",
    "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes!\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "            guard_agent.save_model()\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    return scores\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- HOW TO USE ---\n",
    "    # 1. Make sure you have 'til_environment' and its 'gridworld' module accessible.\n",
    "    #    (e.g., it's in your PYTHONPATH or the same directory)\n",
    "    # 2. Install PyTorch: pip install torch\n",
    "    # 3. Run this script: python your_training_script_name.py\n",
    "    import time\n",
    "    start = time.time()\n",
    "    # Example:\n",
    "    try:\n",
    "        from til_environment import gridworld # Assuming this is your environment module\n",
    "        print(\"Successfully imported til_environment.gridworld\")\n",
    "        \n",
    "        # Start training\n",
    "        # Set load_model_from to a .pth file to continue training or fine-tune\n",
    "        # Set save_model_to to where you want the final model to be saved\n",
    "        trained_scores = train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=20000, # Adjust as needed\n",
    "            novice_track=False, # Or True for the Novice track map\n",
    "            load_scout_model_from=\"agent_reward_12000_eps.pth\", # \"trained_dqn_agent.pth\" to resume\n",
    "            load_guard_model_from=\"guard_6200_eps_no_scout.pth\", # \"trained_dqn_agent.pth\" to resume\n",
    "            save_model_to=\"guard_20k_eps_w_scout.pth\",\n",
    "            render_mode=\"rgb_array\",\n",
    "            video_folder=\"./rl_renders_guard\"\n",
    "        )\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # You can add plotting for scores if you like:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(np.arange(len(trained_scores)), trained_scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'.\")\n",
    "        print(\"Please ensure the environment module is correctly set up and accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time taken = \", end-start)\n",
    "    print(f\"Time taken = {end-start:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
