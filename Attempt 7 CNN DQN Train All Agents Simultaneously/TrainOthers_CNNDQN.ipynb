{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b551715-6751-4eeb-bd5c-4ddca1ef31fa",
   "metadata": {},
   "source": [
    "# Train Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad351c-e290-4511-9a2a-4c94603cb470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating CNN DQN training at 2025-05-23 08:31:20 UTC\n",
      "Starting CNN DQN training: 100000 episodes, Novice: False\n",
      "Attempting to load model from: my_wargame_cnn_agent_35500.pth\n",
      "Models will be saved to: all_cnn_agent_135500.pth\n",
      "Using device: cuda\n",
      "Epsilon for learning agent will start at 0.8000 and decay towards 0.1000\n",
      "Loading model from my_wargame_cnn_agent_35500.pth\n",
      "Primary learning agent ID: player_0\n",
      "Other agents (['player_1', 'player_2', 'player_3']) will use player_0's policy greedily (epsilon=0.0).\n",
      "Ep 100\tAvgScore(player_0, last 100): 0.56\tEps: 0.8000\tGlobalSteps: 7385\tBeta: 0.4109\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 200\tAvgScore(player_0, last 100): 4.38\tEps: 0.4618\tGlobalSteps: 15494\tBeta: 0.4229\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 300\tAvgScore(player_0, last 100): 7.39\tEps: 0.2311\tGlobalSteps: 22418\tBeta: 0.4332\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 400\tAvgScore(player_0, last 100): 10.51\tEps: 0.1141\tGlobalSteps: 29471\tBeta: 0.4436\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 500\tAvgScore(player_0, last 100): 18.13\tEps: 0.1000\tGlobalSteps: 36711\tBeta: 0.4543\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 600\tAvgScore(player_0, last 100): 12.21\tEps: 0.1000\tGlobalSteps: 44224\tBeta: 0.4655\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 700\tAvgScore(player_0, last 100): 13.41\tEps: 0.1000\tGlobalSteps: 50992\tBeta: 0.4755\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 800\tAvgScore(player_0, last 100): 7.37\tEps: 0.1000\tGlobalSteps: 57733\tBeta: 0.4855\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 900\tAvgScore(player_0, last 100): 17.01\tEps: 0.1000\tGlobalSteps: 65287\tBeta: 0.4968\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1000\tAvgScore(player_0, last 100): 17.21\tEps: 0.1000\tGlobalSteps: 73004\tBeta: 0.5082\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1100\tAvgScore(player_0, last 100): 12.71\tEps: 0.1000\tGlobalSteps: 78636\tBeta: 0.5165\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1200\tAvgScore(player_0, last 100): 14.65\tEps: 0.1000\tGlobalSteps: 85306\tBeta: 0.5264\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1300\tAvgScore(player_0, last 100): 12.76\tEps: 0.1000\tGlobalSteps: 92667\tBeta: 0.5373\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1400\tAvgScore(player_0, last 100): 14.86\tEps: 0.1000\tGlobalSteps: 99385\tBeta: 0.5473\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1500\tAvgScore(player_0, last 100): 5.10\tEps: 0.1000\tGlobalSteps: 106490\tBeta: 0.5578\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1600\tAvgScore(player_0, last 100): 17.99\tEps: 0.1000\tGlobalSteps: 112796\tBeta: 0.5671\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1700\tAvgScore(player_0, last 100): 15.75\tEps: 0.1000\tGlobalSteps: 119544\tBeta: 0.5771\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1800\tAvgScore(player_0, last 100): 7.77\tEps: 0.1000\tGlobalSteps: 127268\tBeta: 0.5886\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 1900\tAvgScore(player_0, last 100): 13.93\tEps: 0.1000\tGlobalSteps: 133707\tBeta: 0.5981\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2000\tAvgScore(player_0, last 100): 15.56\tEps: 0.1000\tGlobalSteps: 140908\tBeta: 0.6088\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2100\tAvgScore(player_0, last 100): 15.18\tEps: 0.1000\tGlobalSteps: 147718\tBeta: 0.6189\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2200\tAvgScore(player_0, last 100): 14.31\tEps: 0.1000\tGlobalSteps: 154353\tBeta: 0.6288\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2300\tAvgScore(player_0, last 100): 11.86\tEps: 0.1000\tGlobalSteps: 160183\tBeta: 0.6373\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2400\tAvgScore(player_0, last 100): 10.34\tEps: 0.1000\tGlobalSteps: 166773\tBeta: 0.6471\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2500\tAvgScore(player_0, last 100): 15.74\tEps: 0.1000\tGlobalSteps: 173136\tBeta: 0.6565\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2600\tAvgScore(player_0, last 100): 13.11\tEps: 0.1000\tGlobalSteps: 180659\tBeta: 0.6677\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2700\tAvgScore(player_0, last 100): 12.29\tEps: 0.1000\tGlobalSteps: 187571\tBeta: 0.6779\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2800\tAvgScore(player_0, last 100): 11.09\tEps: 0.1000\tGlobalSteps: 194048\tBeta: 0.6875\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 2900\tAvgScore(player_0, last 100): 15.66\tEps: 0.1000\tGlobalSteps: 201137\tBeta: 0.6981\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3000\tAvgScore(player_0, last 100): 13.47\tEps: 0.1000\tGlobalSteps: 208205\tBeta: 0.7086\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3100\tAvgScore(player_0, last 100): 18.95\tEps: 0.1000\tGlobalSteps: 214258\tBeta: 0.7175\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3200\tAvgScore(player_0, last 100): 14.08\tEps: 0.1000\tGlobalSteps: 220762\tBeta: 0.7271\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3300\tAvgScore(player_0, last 100): 14.12\tEps: 0.1000\tGlobalSteps: 227484\tBeta: 0.7371\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3400\tAvgScore(player_0, last 100): 15.53\tEps: 0.1000\tGlobalSteps: 235186\tBeta: 0.7485\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3500\tAvgScore(player_0, last 100): 16.08\tEps: 0.1000\tGlobalSteps: 241047\tBeta: 0.7572\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3600\tAvgScore(player_0, last 100): 12.34\tEps: 0.1000\tGlobalSteps: 246440\tBeta: 0.7651\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3700\tAvgScore(player_0, last 100): 15.56\tEps: 0.1000\tGlobalSteps: 253240\tBeta: 0.7751\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3800\tAvgScore(player_0, last 100): 8.92\tEps: 0.1000\tGlobalSteps: 260179\tBeta: 0.7854\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 3900\tAvgScore(player_0, last 100): 13.03\tEps: 0.1000\tGlobalSteps: 266686\tBeta: 0.7950\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4000\tAvgScore(player_0, last 100): 13.88\tEps: 0.1000\tGlobalSteps: 272764\tBeta: 0.8040\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4100\tAvgScore(player_0, last 100): 20.66\tEps: 0.1000\tGlobalSteps: 278952\tBeta: 0.8131\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4200\tAvgScore(player_0, last 100): 13.92\tEps: 0.1000\tGlobalSteps: 285550\tBeta: 0.8228\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4300\tAvgScore(player_0, last 100): 15.41\tEps: 0.1000\tGlobalSteps: 292830\tBeta: 0.8337\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4400\tAvgScore(player_0, last 100): 17.77\tEps: 0.1000\tGlobalSteps: 299260\tBeta: 0.8432\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4500\tAvgScore(player_0, last 100): 17.79\tEps: 0.1000\tGlobalSteps: 306313\tBeta: 0.8536\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4600\tAvgScore(player_0, last 100): 13.61\tEps: 0.1000\tGlobalSteps: 313490\tBeta: 0.8643\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4700\tAvgScore(player_0, last 100): 13.80\tEps: 0.1000\tGlobalSteps: 320064\tBeta: 0.8740\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4800\tAvgScore(player_0, last 100): 20.59\tEps: 0.1000\tGlobalSteps: 326861\tBeta: 0.8840\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 4900\tAvgScore(player_0, last 100): 14.03\tEps: 0.1000\tGlobalSteps: 333244\tBeta: 0.8934\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5000\tAvgScore(player_0, last 100): 10.91\tEps: 0.1000\tGlobalSteps: 340485\tBeta: 0.9042\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5100\tAvgScore(player_0, last 100): 14.81\tEps: 0.1000\tGlobalSteps: 347418\tBeta: 0.9145\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5200\tAvgScore(player_0, last 100): 16.42\tEps: 0.1000\tGlobalSteps: 354531\tBeta: 0.9250\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5300\tAvgScore(player_0, last 100): 15.16\tEps: 0.1000\tGlobalSteps: 361647\tBeta: 0.9356\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5400\tAvgScore(player_0, last 100): 13.26\tEps: 0.1000\tGlobalSteps: 368716\tBeta: 0.9461\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5500\tAvgScore(player_0, last 100): 15.81\tEps: 0.1000\tGlobalSteps: 375153\tBeta: 0.9556\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5600\tAvgScore(player_0, last 100): 15.19\tEps: 0.1000\tGlobalSteps: 382243\tBeta: 0.9661\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5700\tAvgScore(player_0, last 100): 15.84\tEps: 0.1000\tGlobalSteps: 389397\tBeta: 0.9767\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5800\tAvgScore(player_0, last 100): 17.38\tEps: 0.1000\tGlobalSteps: 397590\tBeta: 0.9889\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 5900\tAvgScore(player_0, last 100): 14.33\tEps: 0.1000\tGlobalSteps: 404855\tBeta: 0.9997\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6000\tAvgScore(player_0, last 100): 14.06\tEps: 0.1000\tGlobalSteps: 411999\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6100\tAvgScore(player_0, last 100): 14.73\tEps: 0.1000\tGlobalSteps: 418998\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6200\tAvgScore(player_0, last 100): 12.77\tEps: 0.1000\tGlobalSteps: 426377\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6300\tAvgScore(player_0, last 100): 19.57\tEps: 0.1000\tGlobalSteps: 433750\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6400\tAvgScore(player_0, last 100): 16.60\tEps: 0.1000\tGlobalSteps: 441235\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6500\tAvgScore(player_0, last 100): 15.04\tEps: 0.1000\tGlobalSteps: 447326\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6600\tAvgScore(player_0, last 100): 18.76\tEps: 0.1000\tGlobalSteps: 453635\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6700\tAvgScore(player_0, last 100): 17.89\tEps: 0.1000\tGlobalSteps: 460354\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6800\tAvgScore(player_0, last 100): 8.45\tEps: 0.1000\tGlobalSteps: 466671\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 6900\tAvgScore(player_0, last 100): 15.67\tEps: 0.1000\tGlobalSteps: 473720\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7000\tAvgScore(player_0, last 100): 16.04\tEps: 0.1000\tGlobalSteps: 481058\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7100\tAvgScore(player_0, last 100): 15.07\tEps: 0.1000\tGlobalSteps: 487953\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7200\tAvgScore(player_0, last 100): 17.90\tEps: 0.1000\tGlobalSteps: 495083\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7300\tAvgScore(player_0, last 100): 12.78\tEps: 0.1000\tGlobalSteps: 501970\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7400\tAvgScore(player_0, last 100): 15.93\tEps: 0.1000\tGlobalSteps: 509776\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7500\tAvgScore(player_0, last 100): 18.51\tEps: 0.1000\tGlobalSteps: 517568\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7600\tAvgScore(player_0, last 100): 18.01\tEps: 0.1000\tGlobalSteps: 524794\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7700\tAvgScore(player_0, last 100): 16.71\tEps: 0.1000\tGlobalSteps: 532227\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7800\tAvgScore(player_0, last 100): 11.26\tEps: 0.1000\tGlobalSteps: 539783\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 7900\tAvgScore(player_0, last 100): 11.98\tEps: 0.1000\tGlobalSteps: 547046\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8000\tAvgScore(player_0, last 100): 20.09\tEps: 0.1000\tGlobalSteps: 554434\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8100\tAvgScore(player_0, last 100): 15.35\tEps: 0.1000\tGlobalSteps: 562562\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8200\tAvgScore(player_0, last 100): 15.85\tEps: 0.1000\tGlobalSteps: 569937\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8300\tAvgScore(player_0, last 100): 17.97\tEps: 0.1000\tGlobalSteps: 576495\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8400\tAvgScore(player_0, last 100): 19.17\tEps: 0.1000\tGlobalSteps: 584363\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8500\tAvgScore(player_0, last 100): 18.20\tEps: 0.1000\tGlobalSteps: 591460\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8600\tAvgScore(player_0, last 100): 15.83\tEps: 0.1000\tGlobalSteps: 598238\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8700\tAvgScore(player_0, last 100): 19.67\tEps: 0.1000\tGlobalSteps: 604644\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 8900\tAvgScore(player_0, last 100): 14.82\tEps: 0.1000\tGlobalSteps: 617942\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9000\tAvgScore(player_0, last 100): 15.06\tEps: 0.1000\tGlobalSteps: 625388\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9100\tAvgScore(player_0, last 100): 14.73\tEps: 0.1000\tGlobalSteps: 633320\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9200\tAvgScore(player_0, last 100): 16.50\tEps: 0.1000\tGlobalSteps: 640554\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9300\tAvgScore(player_0, last 100): 10.67\tEps: 0.1000\tGlobalSteps: 647440\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9400\tAvgScore(player_0, last 100): 17.19\tEps: 0.1000\tGlobalSteps: 654556\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9500\tAvgScore(player_0, last 100): 11.75\tEps: 0.1000\tGlobalSteps: 661336\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9600\tAvgScore(player_0, last 100): 14.26\tEps: 0.1000\tGlobalSteps: 669065\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9700\tAvgScore(player_0, last 100): 17.73\tEps: 0.1000\tGlobalSteps: 676088\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9800\tAvgScore(player_0, last 100): 14.53\tEps: 0.1000\tGlobalSteps: 683179\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 9900\tAvgScore(player_0, last 100): 19.04\tEps: 0.1000\tGlobalSteps: 690301\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10000\tAvgScore(player_0, last 100): 18.45\tEps: 0.1000\tGlobalSteps: 697700\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10100\tAvgScore(player_0, last 100): 16.15\tEps: 0.1000\tGlobalSteps: 705346\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10200\tAvgScore(player_0, last 100): 15.88\tEps: 0.1000\tGlobalSteps: 712964\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10300\tAvgScore(player_0, last 100): 17.85\tEps: 0.1000\tGlobalSteps: 719796\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10400\tAvgScore(player_0, last 100): 14.62\tEps: 0.1000\tGlobalSteps: 726691\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10500\tAvgScore(player_0, last 100): 10.56\tEps: 0.1000\tGlobalSteps: 733781\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10600\tAvgScore(player_0, last 100): 15.42\tEps: 0.1000\tGlobalSteps: 741045\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10700\tAvgScore(player_0, last 100): 15.35\tEps: 0.1000\tGlobalSteps: 748440\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10800\tAvgScore(player_0, last 100): 18.49\tEps: 0.1000\tGlobalSteps: 755908\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 10900\tAvgScore(player_0, last 100): 21.07\tEps: 0.1000\tGlobalSteps: 763127\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11000\tAvgScore(player_0, last 100): 15.64\tEps: 0.1000\tGlobalSteps: 770513\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11100\tAvgScore(player_0, last 100): 18.17\tEps: 0.1000\tGlobalSteps: 777434\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11200\tAvgScore(player_0, last 100): 18.90\tEps: 0.1000\tGlobalSteps: 784596\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11300\tAvgScore(player_0, last 100): 15.59\tEps: 0.1000\tGlobalSteps: 791652\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11400\tAvgScore(player_0, last 100): 14.00\tEps: 0.1000\tGlobalSteps: 798778\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11500\tAvgScore(player_0, last 100): 18.16\tEps: 0.1000\tGlobalSteps: 806586\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11600\tAvgScore(player_0, last 100): 15.08\tEps: 0.1000\tGlobalSteps: 813748\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11700\tAvgScore(player_0, last 100): 12.80\tEps: 0.1000\tGlobalSteps: 820885\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11800\tAvgScore(player_0, last 100): 17.95\tEps: 0.1000\tGlobalSteps: 827991\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 11900\tAvgScore(player_0, last 100): 15.79\tEps: 0.1000\tGlobalSteps: 835410\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12000\tAvgScore(player_0, last 100): 20.76\tEps: 0.1000\tGlobalSteps: 842150\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12100\tAvgScore(player_0, last 100): 13.64\tEps: 0.1000\tGlobalSteps: 849743\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12200\tAvgScore(player_0, last 100): 17.77\tEps: 0.1000\tGlobalSteps: 856764\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12300\tAvgScore(player_0, last 100): 17.82\tEps: 0.1000\tGlobalSteps: 863930\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12400\tAvgScore(player_0, last 100): 17.96\tEps: 0.1000\tGlobalSteps: 870681\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12500\tAvgScore(player_0, last 100): 17.80\tEps: 0.1000\tGlobalSteps: 877839\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12600\tAvgScore(player_0, last 100): 17.09\tEps: 0.1000\tGlobalSteps: 885078\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12700\tAvgScore(player_0, last 100): 19.81\tEps: 0.1000\tGlobalSteps: 892752\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12800\tAvgScore(player_0, last 100): 19.00\tEps: 0.1000\tGlobalSteps: 899780\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 12900\tAvgScore(player_0, last 100): 18.90\tEps: 0.1000\tGlobalSteps: 907706\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13000\tAvgScore(player_0, last 100): 17.09\tEps: 0.1000\tGlobalSteps: 915608\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13100\tAvgScore(player_0, last 100): 16.69\tEps: 0.1000\tGlobalSteps: 923245\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13200\tAvgScore(player_0, last 100): 18.35\tEps: 0.1000\tGlobalSteps: 930319\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13300\tAvgScore(player_0, last 100): 16.72\tEps: 0.1000\tGlobalSteps: 937154\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13400\tAvgScore(player_0, last 100): 18.95\tEps: 0.1000\tGlobalSteps: 944229\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13500\tAvgScore(player_0, last 100): 17.32\tEps: 0.1000\tGlobalSteps: 951776\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13600\tAvgScore(player_0, last 100): 15.87\tEps: 0.1000\tGlobalSteps: 958859\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13700\tAvgScore(player_0, last 100): 24.81\tEps: 0.1000\tGlobalSteps: 965349\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13800\tAvgScore(player_0, last 100): 16.73\tEps: 0.1000\tGlobalSteps: 972758\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 13900\tAvgScore(player_0, last 100): 16.57\tEps: 0.1000\tGlobalSteps: 980208\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14000\tAvgScore(player_0, last 100): 10.69\tEps: 0.1000\tGlobalSteps: 987549\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14100\tAvgScore(player_0, last 100): 16.20\tEps: 0.1000\tGlobalSteps: 994739\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14200\tAvgScore(player_0, last 100): 16.96\tEps: 0.1000\tGlobalSteps: 1001288\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14300\tAvgScore(player_0, last 100): 17.69\tEps: 0.1000\tGlobalSteps: 1008258\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14400\tAvgScore(player_0, last 100): 14.29\tEps: 0.1000\tGlobalSteps: 1015603\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14500\tAvgScore(player_0, last 100): 12.64\tEps: 0.1000\tGlobalSteps: 1022191\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14600\tAvgScore(player_0, last 100): 15.14\tEps: 0.1000\tGlobalSteps: 1029097\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14700\tAvgScore(player_0, last 100): 18.27\tEps: 0.1000\tGlobalSteps: 1035878\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14800\tAvgScore(player_0, last 100): 18.65\tEps: 0.1000\tGlobalSteps: 1043571\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 14900\tAvgScore(player_0, last 100): 17.55\tEps: 0.1000\tGlobalSteps: 1051120\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15000\tAvgScore(player_0, last 100): 19.25\tEps: 0.1000\tGlobalSteps: 1057582\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15100\tAvgScore(player_0, last 100): 19.69\tEps: 0.1000\tGlobalSteps: 1064864\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15200\tAvgScore(player_0, last 100): 26.53\tEps: 0.1000\tGlobalSteps: 1071358\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15300\tAvgScore(player_0, last 100): 16.28\tEps: 0.1000\tGlobalSteps: 1078324\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15400\tAvgScore(player_0, last 100): 18.31\tEps: 0.1000\tGlobalSteps: 1085166\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15500\tAvgScore(player_0, last 100): 22.25\tEps: 0.1000\tGlobalSteps: 1092590\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15600\tAvgScore(player_0, last 100): 22.20\tEps: 0.1000\tGlobalSteps: 1099784\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15700\tAvgScore(player_0, last 100): 18.95\tEps: 0.1000\tGlobalSteps: 1106504\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15800\tAvgScore(player_0, last 100): 19.81\tEps: 0.1000\tGlobalSteps: 1113781\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 15900\tAvgScore(player_0, last 100): 24.32\tEps: 0.1000\tGlobalSteps: 1120686\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n",
      "Ep 16000\tAvgScore(player_0, last 100): 18.93\tEps: 0.1000\tGlobalSteps: 1128292\tBeta: 1.0000\n",
      "Model saved to all_cnn_agent_135500.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "VIEWCONE_CHANNELS = 8\n",
    "VIEWCONE_HEIGHT = 7\n",
    "VIEWCONE_WIDTH = 5\n",
    "OTHER_FEATURES_SIZE = 4 + 2 + 1 + 1\n",
    "\n",
    "CNN_OUTPUT_CHANNELS_1 = 16\n",
    "CNN_OUTPUT_CHANNELS_2 = 32\n",
    "KERNEL_SIZE_1 = (3, 3)\n",
    "STRIDE_1 = 1\n",
    "KERNEL_SIZE_2 = (3, 3)\n",
    "STRIDE_2 = 1\n",
    "MLP_HIDDEN_LAYER_1_SIZE = 128\n",
    "MLP_HIDDEN_LAYER_2_SIZE = 128\n",
    "OUTPUT_ACTIONS = 5\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BUFFER_SIZE = int(1e5) \n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "TARGET_UPDATE_EVERY = 1000 \n",
    "UPDATE_EVERY = 4 \n",
    "\n",
    "EPSILON_START = 0.8      \n",
    "EPSILON_END = 0.1        \n",
    "EPSILON_DECAY_RATE = 0.9999 \n",
    "MIN_EPSILON_FRAMES = int(1e4) \n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4 \n",
    "PER_BETA_FRAMES = int(1e5) \n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state_viewcone\", \"state_other\", \"action\", \"reward\", \"next_state_viewcone\", \"next_state_other\", \"done\"])\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        experience = Experience(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / batch_size if batch_size > 0 and self.tree.total_priority > 0 else 0\n",
    "        \n",
    "        if self.tree.n_entries == 0: # Avoid division by zero if buffer is empty\n",
    "             return (torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)), np.array([]), torch.empty(0)\n",
    "\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta) # Added 1e-8 for stability\n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights /= (weights.max() if weights.max() > 0 else 1.0)\n",
    "\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = zip(*[e for e in batch_data if e is not None]) # Ensure data is not None\n",
    "        \n",
    "        if not states_viewcone: # Handle cases where batch_data might have yielded no valid experiences\n",
    "            return (torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)), np.array([]), torch.empty(0)\n",
    "\n",
    "        states_viewcone = torch.from_numpy(np.array(states_viewcone)).float().to(DEVICE)\n",
    "        states_other = torch.from_numpy(np.array(states_other)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states_viewcone = torch.from_numpy(np.array(next_states_viewcone)).float().to(DEVICE)\n",
    "        next_states_other = torch.from_numpy(np.array(next_states_other)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        if len(batch_indices) == 0: return\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority_val in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority_val)\n",
    "        if priorities.size > 0: # Check if priorities is not empty\n",
    "            self.max_priority = max(self.max_priority, np.max(priorities)) # Use np.max for numpy array\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, viewcone_channels, viewcone_height, viewcone_width, other_features_size, mlp_hidden1, mlp_hidden2, num_actions, dropout_rate):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(viewcone_channels, CNN_OUTPUT_CHANNELS_1, kernel_size=KERNEL_SIZE_1, stride=STRIDE_1, padding=1)\n",
    "        self.relu_conv1 = nn.ReLU()\n",
    "        h_out1 = (viewcone_height + 2 * 1 - KERNEL_SIZE_1[0]) // STRIDE_1 + 1\n",
    "        w_out1 = (viewcone_width + 2 * 1 - KERNEL_SIZE_1[1]) // STRIDE_1 + 1\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(CNN_OUTPUT_CHANNELS_1, CNN_OUTPUT_CHANNELS_2, kernel_size=KERNEL_SIZE_2, stride=STRIDE_2, padding=1)\n",
    "        self.relu_conv2 = nn.ReLU()\n",
    "        h_out2 = (h_out1 + 2 * 1 - KERNEL_SIZE_2[0]) // STRIDE_2 + 1\n",
    "        w_out2 = (w_out1 + 2 * 1 - KERNEL_SIZE_2[1]) // STRIDE_2 + 1\n",
    "\n",
    "        self.cnn_output_flat_size = CNN_OUTPUT_CHANNELS_2 * h_out2 * w_out2\n",
    "        self.fc1_mlp = nn.Linear(self.cnn_output_flat_size + other_features_size, mlp_hidden1)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2_mlp = nn.Linear(mlp_hidden1, mlp_hidden2)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc_output = nn.Linear(mlp_hidden2, num_actions)\n",
    "\n",
    "    def forward(self, viewcone_input, other_features_input):\n",
    "        x_cnn = self.relu_conv1(self.conv1(viewcone_input))\n",
    "        x_cnn = self.relu_conv2(self.conv2(x_cnn))\n",
    "        x_cnn_flat = x_cnn.view(-1, self.cnn_output_flat_size)\n",
    "        combined_features = torch.cat((x_cnn_flat, other_features_input), dim=1)\n",
    "        x = self.relu_fc1(self.fc1_mlp(combined_features))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu_fc2(self.fc2_mlp(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc_output(x)\n",
    "\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_cnn_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        self.target_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                print(f\"Loading model from {model_load_path}\")\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            if model_load_path: print(f\"Warning: Model file not found at {model_load_path}. Initializing new model.\")\n",
    "            else: print(\"No model_load_path specified. Initializing new model.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step_episode = 0 \n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(VIEWCONE_CHANNELS)] \n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        raw_viewcone = observation_dict.get(\"viewcone\", np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8))\n",
    "        if not isinstance(raw_viewcone, np.ndarray): raw_viewcone = np.array(raw_viewcone)\n",
    "        if raw_viewcone.shape != (VIEWCONE_HEIGHT, VIEWCONE_WIDTH):\n",
    "            padded_viewcone = np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8)\n",
    "            h, w = raw_viewcone.shape\n",
    "            h_min, w_min = min(h, VIEWCONE_HEIGHT), min(w, VIEWCONE_WIDTH)\n",
    "            padded_viewcone[:h_min, :w_min] = raw_viewcone[:h_min, :w_min]\n",
    "            raw_viewcone = padded_viewcone\n",
    "\n",
    "        processed_viewcone_channels_data = np.zeros((VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.float32)\n",
    "        for r in range(VIEWCONE_HEIGHT):\n",
    "            for c in range(VIEWCONE_WIDTH):\n",
    "                tile_value = raw_viewcone[r, c]\n",
    "                unpacked_features = self._unpack_viewcone_tile(tile_value)\n",
    "                for channel_idx in range(VIEWCONE_CHANNELS):\n",
    "                    processed_viewcone_channels_data[channel_idx, r, c] = unpacked_features[channel_idx]\n",
    "        \n",
    "        other_features_list = []\n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4; direction_one_hot[direction % 4] = 1.0\n",
    "        other_features_list.extend(direction_one_hot)\n",
    "        location = observation_dict.get(\"location\", [0,0]); norm_x = location[0]/MAP_SIZE_X; norm_y = location[1]/MAP_SIZE_Y\n",
    "        other_features_list.extend([norm_x, norm_y])\n",
    "        other_features_list.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        other_features_list.append(observation_dict.get(\"step\", 0)/MAX_STEPS_PER_EPISODE)\n",
    "        state_other_np = np.array(other_features_list, dtype=np.float32)\n",
    "        \n",
    "        return processed_viewcone_channels_data, state_other_np\n",
    "\n",
    "    def select_action(self, state_viewcone_np, state_other_np, epsilon=0.0):\n",
    "        if random.random() > epsilon:\n",
    "            state_viewcone_tensor = torch.from_numpy(state_viewcone_np).float().unsqueeze(0).to(self.device)\n",
    "            state_other_tensor = torch.from_numpy(state_other_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): action_values = self.policy_net(state_viewcone_tensor, state_other_tensor)\n",
    "            self.policy_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        self.memory.add(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.t_step_episode = (self.t_step_episode + 1) % UPDATE_EVERY\n",
    "        if self.t_step_episode == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            if experiences[0].nelement() > 0: # Check if tensors are not empty\n",
    "                 self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "    \n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = experiences\n",
    "        \n",
    "        # Check for empty batch again, critical if sample() could return empty tensors directly\n",
    "        if states_viewcone.nelement() == 0:\n",
    "            return\n",
    "\n",
    "        q_next_actions_policy = self.policy_net(next_states_viewcone, next_states_other).detach().max(1)[1].unsqueeze(1)\n",
    "        q_targets_next = self.target_net(next_states_viewcone, next_states_other).detach().gather(1, q_next_actions_policy)\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.policy_net(states_viewcone, states_other).gather(1, actions)\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        self.optimizer.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_save_path:\n",
    "            torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "            print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_episode_counters(self): self.t_step_episode = 0\n",
    "\n",
    "\n",
    "def train_agent(env_module, num_episodes=100000, novice_track=False, load_model_from=None, save_model_to=\"trained_cnn_dqn_agent.pth\"):\n",
    "    print(f\"Starting CNN DQN training: {num_episodes} episodes, Novice: {novice_track}\")\n",
    "    if load_model_from: print(f\"Attempting to load model from: {load_model_from}\")\n",
    "    print(f\"Models will be saved to: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Epsilon for learning agent will start at {EPSILON_START:.4f} and decay towards {EPSILON_END:.4f}\")\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100) \n",
    "    epsilon = EPSILON_START \n",
    "    global_total_steps = 0 \n",
    "    \n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "    \n",
    "    # my_agent_id will be the agent that learns. Other agents use its policy greedily.\n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\"\n",
    "    print(f\"Primary learning agent ID: {my_agent_id}\")\n",
    "    other_agent_ids = [ag_id for ag_id in env.possible_agents if ag_id != my_agent_id]\n",
    "    if other_agent_ids:\n",
    "        print(f\"Other agents ({other_agent_ids}) will use {my_agent_id}'s policy greedily (epsilon=0.0).\")\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset() \n",
    "        agent.reset_episode_counters() # Reset for the shared agent instance\n",
    "        current_episode_rewards_accumulator = {id: 0.0 for id in env.possible_agents}\n",
    "        \n",
    "        last_processed_exp_learning_agent = {} # Stores (prev_s_vc, prev_s_other, prev_a) for my_agent_id\n",
    "        \n",
    "        for agent_id_turn in env.agent_iter(): # agent_id_turn is the agent whose turn it is\n",
    "            current_obs_raw, reward_for_current_agent_turn, termination, truncation, info = env.last()\n",
    "            \n",
    "            if agent_id_turn in current_episode_rewards_accumulator:\n",
    "                 current_episode_rewards_accumulator[agent_id_turn] += reward_for_current_agent_turn\n",
    "\n",
    "            done = termination or truncation\n",
    "            action_to_take = None\n",
    "\n",
    "            if done:\n",
    "                if agent_id_turn == my_agent_id and my_agent_id in last_processed_exp_learning_agent:\n",
    "                    prev_s_vc, prev_s_other, prev_a = last_processed_exp_learning_agent.pop(my_agent_id)\n",
    "                    obs_dict_terminal = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in current_obs_raw.items()}\n",
    "                    terminal_s_vc, terminal_s_other = agent.process_observation(obs_dict_terminal)\n",
    "                    agent.step(prev_s_vc, prev_s_other, prev_a, reward_for_current_agent_turn, terminal_s_vc, terminal_s_other, True)\n",
    "                # action_to_take remains None, env.step(None) will be called\n",
    "            else:\n",
    "                obs_dict_current = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in current_obs_raw.items()}\n",
    "                current_s_vc, current_s_other = agent.process_observation(obs_dict_current)\n",
    "\n",
    "                if agent_id_turn == my_agent_id:\n",
    "                    if my_agent_id in last_processed_exp_learning_agent:\n",
    "                        prev_s_vc, prev_s_other, prev_a = last_processed_exp_learning_agent.pop(my_agent_id)\n",
    "                        agent.step(prev_s_vc, prev_s_other, prev_a, reward_for_current_agent_turn, current_s_vc, current_s_other, False)\n",
    "                    \n",
    "                    action_to_take = agent.select_action(current_s_vc, current_s_other, epsilon)\n",
    "                    last_processed_exp_learning_agent[my_agent_id] = (current_s_vc, current_s_other, action_to_take)\n",
    "                    \n",
    "                    global_total_steps += 1\n",
    "                    if global_total_steps > MIN_EPSILON_FRAMES and epsilon > EPSILON_END : \n",
    "                        epsilon *= EPSILON_DECAY_RATE\n",
    "                        epsilon = max(EPSILON_END, epsilon)\n",
    "                    if global_total_steps % TARGET_UPDATE_EVERY == 0 and global_total_steps > 0: \n",
    "                        agent.update_target_net()\n",
    "                else: # Other agents' turns\n",
    "                    action_to_take = agent.select_action(current_s_vc, current_s_other, epsilon=0.2) # Use learned policy greedily\n",
    "            \n",
    "            env.step(action_to_take) \n",
    "        \n",
    "        episode_score_my_agent = current_episode_rewards_accumulator.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score_my_agent)\n",
    "\n",
    "        if i_episode % 100 == 0: \n",
    "            avg_score_str = f\"{np.mean(scores_deque):.2f}\" if scores_deque else \"N/A\"\n",
    "            print(f'\\rEp {i_episode}\\tAvgScore({my_agent_id}, last 100): {avg_score_str}\\tEps: {epsilon:.4f}\\tGlobalSteps: {global_total_steps}\\tBeta: {agent.beta:.4f}')\n",
    "            if save_model_to: agent.save_model() \n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model() \n",
    "    print(f\"\\nCNN DQN Training finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    print(f\"Initiating CNN DQN training at {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(training_start_time))} UTC\")\n",
    "    try:\n",
    "        from til_environment import gridworld # Ensure this is your environment module\n",
    "        \n",
    "        NUM_OVERNIGHT_EPISODES = 100000 \n",
    "        # --- MODIFIED PATHS ---\n",
    "        LOAD_MODEL_PATH = \"my_wargame_cnn_agent_35500.pth\" \n",
    "        SAVE_MODEL_PATH = \"all_cnn_agent_135500.pth\" # New save path\n",
    "        # --- END MODIFIED PATHS ---\n",
    "        NOVICE_MODE = False \n",
    "\n",
    "        train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=NUM_OVERNIGHT_EPISODES,\n",
    "            novice_track=NOVICE_MODE,\n",
    "            load_model_from=LOAD_MODEL_PATH, \n",
    "            save_model_to=SAVE_MODEL_PATH\n",
    "        )\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible and contains your PettingZoo environment.\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"Error: Model file not found. {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during CNN DQN training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    total_time_seconds = time.time() - training_start_time\n",
    "    print(f\"Total CNN DQN training time for this session: {total_time_seconds:.2f} seconds ({total_time_seconds/3600:.2f} hours).\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
