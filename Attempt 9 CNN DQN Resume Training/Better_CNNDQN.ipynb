{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753cf02-1995-450e-bd26-039f8b242a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating CNN DQN training at 2025-05-23 18:57:45 UTC\n",
      "Starting CNN DQN training: 100000 episodes, Novice: False\n",
      "Attempting to load model from: my_wargame_cnn_agent_best.pth\n",
      "Models will be saved to: my_wargame_cnn_agent_best_100000.pth\n",
      "Using device: cuda\n",
      "Epsilon for learning agent will start at 0.8000 and decay towards 0.0100\n",
      "Learning rate scheduler: StepLR, step_size=30000 episodes, gamma=0.5\n",
      "Loading model from my_wargame_cnn_agent_best.pth\n",
      "Primary learning agent ID: player_0\n",
      "Other agents (['player_1', 'player_2', 'player_3']) will use player_0's policy with epsilon=0.2 (some exploration).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "Ep 100\tAvgScore(player_0): -0.60\tEps: 0.8000\tLR: 1.00e-04\tLoss: 14.0778\tBeta: 0.412\tSteps: 8127\n",
      "Model saved to my_wargame_cnn_agent_best_100000.pth\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "Ep 200\tAvgScore(player_0): 5.89\tEps: 0.4060\tLR: 1.00e-04\tLoss: 7.5225\tBeta: 0.425\tSteps: 16782\n",
      "Model saved to my_wargame_cnn_agent_best_100000.pth\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "Ep 300\tAvgScore(player_0): 13.28\tEps: 0.1917\tLR: 1.00e-04\tLoss: 7.0458\tBeta: 0.436\tSteps: 24287\n",
      "Model saved to my_wargame_cnn_agent_best_100000.pth\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "Ep 400\tAvgScore(player_0): 14.02\tEps: 0.0831\tLR: 1.00e-04\tLoss: 10.4981\tBeta: 0.449\tSteps: 32640\n",
      "Model saved to my_wargame_cnn_agent_best_100000.pth\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "Ep 500\tAvgScore(player_0): 10.05\tEps: 0.0393\tLR: 1.00e-04\tLoss: 27.5832\tBeta: 0.460\tSteps: 40123\n",
      "Model saved to my_wargame_cnn_agent_best_100000.pth\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n",
      "\n",
      "Target network updated at global step. Current LR: 1.00e-04\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "VIEWCONE_CHANNELS = 8\n",
    "VIEWCONE_HEIGHT = 7\n",
    "VIEWCONE_WIDTH = 5\n",
    "OTHER_FEATURES_SIZE = 4 + 2 + 1 + 1\n",
    "\n",
    "CNN_OUTPUT_CHANNELS_1 = 16\n",
    "CNN_OUTPUT_CHANNELS_2 = 32\n",
    "KERNEL_SIZE_1 = (3, 3)\n",
    "STRIDE_1 = 1\n",
    "KERNEL_SIZE_2 = (3, 3)\n",
    "STRIDE_2 = 1\n",
    "MLP_HIDDEN_LAYER_1_SIZE = 128\n",
    "MLP_HIDDEN_LAYER_2_SIZE = 128\n",
    "OUTPUT_ACTIONS = 5\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64 # You can experiment with this\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4 # Initial learning rate\n",
    "WEIGHT_DECAY = 1e-5\n",
    "TARGET_UPDATE_EVERY = 1000 # Steps\n",
    "UPDATE_EVERY = 4 # Steps\n",
    "\n",
    "# Learning Rate Scheduler Params\n",
    "LR_SCHEDULER_STEP_SIZE = 30000 # Episodes or steps, adjust based on how you call scheduler.step()\n",
    "LR_SCHEDULER_GAMMA = 0.5    # Multiplicative factor of learning rate decay\n",
    "\n",
    "EPSILON_START = 0.8\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY_RATE = 0.9999\n",
    "MIN_EPSILON_FRAMES = int(1e4) # Global steps before epsilon starts decaying\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_FRAMES = int(1e5) # Global steps\n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state_viewcone\", \"state_other\", \"action\", \"reward\", \"next_state_viewcone\", \"next_state_other\", \"done\"])\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        experience = Experience(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / batch_size if batch_size > 0 and self.tree.total_priority > 0 else 0\n",
    "        \n",
    "        if self.tree.n_entries < batch_size : # Ensure enough samples for a full batch\n",
    "             return (torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)), np.array([]), torch.empty(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta)\n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights /= (weights.max() if weights.max() > 0 else 1.0)\n",
    "\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = zip(*[e for e in batch_data if e is not None])\n",
    "        \n",
    "        if not states_viewcone:\n",
    "            return (torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)), np.array([]), torch.empty(0)\n",
    "\n",
    "        states_viewcone = torch.from_numpy(np.array(states_viewcone)).float().to(DEVICE)\n",
    "        states_other = torch.from_numpy(np.array(states_other)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states_viewcone = torch.from_numpy(np.array(next_states_viewcone)).float().to(DEVICE)\n",
    "        next_states_other = torch.from_numpy(np.array(next_states_other)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        if len(batch_indices) == 0: return\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority_val in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority_val)\n",
    "        if priorities.size > 0:\n",
    "            self.max_priority = max(self.max_priority, np.max(priorities))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, viewcone_channels, viewcone_height, viewcone_width, other_features_size, mlp_hidden1, mlp_hidden2, num_actions, dropout_rate):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(viewcone_channels, CNN_OUTPUT_CHANNELS_1, kernel_size=KERNEL_SIZE_1, stride=STRIDE_1, padding=1)\n",
    "        self.relu_conv1 = nn.ReLU()\n",
    "        h_out1 = (viewcone_height + 2 * 1 - KERNEL_SIZE_1[0]) // STRIDE_1 + 1\n",
    "        w_out1 = (viewcone_width + 2 * 1 - KERNEL_SIZE_1[1]) // STRIDE_1 + 1\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(CNN_OUTPUT_CHANNELS_1, CNN_OUTPUT_CHANNELS_2, kernel_size=KERNEL_SIZE_2, stride=STRIDE_2, padding=1)\n",
    "        self.relu_conv2 = nn.ReLU()\n",
    "        h_out2 = (h_out1 + 2 * 1 - KERNEL_SIZE_2[0]) // STRIDE_2 + 1\n",
    "        w_out2 = (w_out1 + 2 * 1 - KERNEL_SIZE_2[1]) // STRIDE_2 + 1\n",
    "\n",
    "        self.cnn_output_flat_size = CNN_OUTPUT_CHANNELS_2 * h_out2 * w_out2\n",
    "        self.fc1_mlp = nn.Linear(self.cnn_output_flat_size + other_features_size, mlp_hidden1)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        # Consider adding nn.BatchNorm1d(mlp_hidden1) here if you want to experiment with it later\n",
    "        self.fc2_mlp = nn.Linear(mlp_hidden1, mlp_hidden2)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        # Consider adding nn.BatchNorm1d(mlp_hidden2) here\n",
    "        self.fc_output = nn.Linear(mlp_hidden2, num_actions)\n",
    "\n",
    "    def forward(self, viewcone_input, other_features_input):\n",
    "        x_cnn = self.relu_conv1(self.conv1(viewcone_input))\n",
    "        x_cnn = self.relu_conv2(self.conv2(x_cnn))\n",
    "        x_cnn_flat = x_cnn.view(-1, self.cnn_output_flat_size)\n",
    "        combined_features = torch.cat((x_cnn_flat, other_features_input), dim=1)\n",
    "        x = self.relu_fc1(self.fc1_mlp(combined_features))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu_fc2(self.fc2_mlp(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc_output(x)\n",
    "\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_cnn_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        self.target_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                print(f\"Loading model from {model_load_path}\")\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            if model_load_path: print(f\"Warning: Model file not found at {model_load_path}. Initializing new model.\")\n",
    "            else: print(\"No model_load_path specified. Initializing new model.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Added Learning Rate Scheduler\n",
    "        self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, \n",
    "                                                      step_size=LR_SCHEDULER_STEP_SIZE, \n",
    "                                                      gamma=LR_SCHEDULER_GAMMA)\n",
    "        \n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step_episode = 0 \n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "        self.current_loss = 0.0 # For basic loss tracking\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(VIEWCONE_CHANNELS)] \n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        raw_viewcone = observation_dict.get(\"viewcone\", np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8))\n",
    "        if not isinstance(raw_viewcone, np.ndarray): raw_viewcone = np.array(raw_viewcone)\n",
    "        if raw_viewcone.shape != (VIEWCONE_HEIGHT, VIEWCONE_WIDTH):\n",
    "            padded_viewcone = np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8)\n",
    "            h, w = raw_viewcone.shape\n",
    "            h_min, w_min = min(h, VIEWCONE_HEIGHT), min(w, VIEWCONE_WIDTH)\n",
    "            padded_viewcone[:h_min, :w_min] = raw_viewcone[:h_min, :w_min]\n",
    "            raw_viewcone = padded_viewcone\n",
    "\n",
    "        processed_viewcone_channels_data = np.zeros((VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.float32)\n",
    "        for r in range(VIEWCONE_HEIGHT):\n",
    "            for c in range(VIEWCONE_WIDTH):\n",
    "                tile_value = raw_viewcone[r, c]\n",
    "                unpacked_features = self._unpack_viewcone_tile(tile_value)\n",
    "                for channel_idx in range(VIEWCONE_CHANNELS):\n",
    "                    processed_viewcone_channels_data[channel_idx, r, c] = unpacked_features[channel_idx]\n",
    "        \n",
    "        other_features_list = []\n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4; direction_one_hot[direction % 4] = 1.0\n",
    "        other_features_list.extend(direction_one_hot)\n",
    "        location = observation_dict.get(\"location\", [0,0]); norm_x = location[0]/MAP_SIZE_X; norm_y = location[1]/MAP_SIZE_Y\n",
    "        other_features_list.extend([norm_x, norm_y])\n",
    "        other_features_list.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        other_features_list.append(observation_dict.get(\"step\", 0)/MAX_STEPS_PER_EPISODE)\n",
    "        state_other_np = np.array(other_features_list, dtype=np.float32)\n",
    "        \n",
    "        return processed_viewcone_channels_data, state_other_np\n",
    "\n",
    "    def select_action(self, state_viewcone_np, state_other_np, epsilon=0.0):\n",
    "        if random.random() > epsilon:\n",
    "            state_viewcone_tensor = torch.from_numpy(state_viewcone_np).float().unsqueeze(0).to(self.device)\n",
    "            state_other_tensor = torch.from_numpy(state_other_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): action_values = self.policy_net(state_viewcone_tensor, state_other_tensor)\n",
    "            self.policy_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        self.memory.add(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.t_step_episode = (self.t_step_episode + 1) % UPDATE_EVERY\n",
    "        if self.t_step_episode == 0 and len(self.memory) >= BATCH_SIZE: # Ensure enough samples\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            if experiences[0].nelement() > 0:\n",
    "                 self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "    \n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = experiences\n",
    "        \n",
    "        if states_viewcone.nelement() == 0: return\n",
    "\n",
    "        q_next_actions_policy = self.policy_net(next_states_viewcone, next_states_other).detach().max(1)[1].unsqueeze(1)\n",
    "        q_targets_next = self.target_net(next_states_viewcone, next_states_other).detach().gather(1, q_next_actions_policy)\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.policy_net(states_viewcone, states_other).gather(1, actions)\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten() # For PER\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = (importance_sampling_weights * nn.functional.mse_loss(q_expected, q_targets, reduction='none')).mean()\n",
    "        self.current_loss = loss.item() # Store current loss\n",
    "\n",
    "        self.optimizer.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0) # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        print(f\"\\nTarget network updated at global step. Current LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_save_path:\n",
    "            torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "            print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_episode_counters(self): self.t_step_episode = 0\n",
    "\n",
    "\n",
    "def train_agent(env_module, num_episodes=100000, novice_track=False, load_model_from=None, save_model_to=\"trained_cnn_dqn_agent.pth\"):\n",
    "    print(f\"Starting CNN DQN training: {num_episodes} episodes, Novice: {novice_track}\")\n",
    "    if load_model_from: print(f\"Attempting to load model from: {load_model_from}\")\n",
    "    print(f\"Models will be saved to: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Epsilon for learning agent will start at {EPSILON_START:.4f} and decay towards {EPSILON_END:.4f}\")\n",
    "    print(f\"Learning rate scheduler: StepLR, step_size={LR_SCHEDULER_STEP_SIZE} episodes, gamma={LR_SCHEDULER_GAMMA}\")\n",
    "\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100) \n",
    "    epsilon = EPSILON_START \n",
    "    global_total_steps = 0 \n",
    "    \n",
    "    # Ensure til_environment and gridworld are correctly imported and accessible\n",
    "    # from til_environment import gridworld # This should be passed as env_module\n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track) # Use the passed env_module\n",
    "    \n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\"\n",
    "    print(f\"Primary learning agent ID: {my_agent_id}\")\n",
    "    other_agent_ids = [ag_id for ag_id in env.possible_agents if ag_id != my_agent_id]\n",
    "    if other_agent_ids:\n",
    "        print(f\"Other agents ({other_agent_ids}) will use {my_agent_id}'s policy with epsilon=0.2 (some exploration).\")\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset() \n",
    "        agent.reset_episode_counters()\n",
    "        current_episode_rewards_accumulator = {id: 0.0 for id in env.possible_agents}\n",
    "        \n",
    "        last_processed_exp_learning_agent = {}\n",
    "        \n",
    "        for agent_id_turn in env.agent_iter():\n",
    "            current_obs_raw, reward_for_current_agent_turn, termination, truncation, info = env.last()\n",
    "            \n",
    "            if agent_id_turn in current_episode_rewards_accumulator:\n",
    "                 current_episode_rewards_accumulator[agent_id_turn] += reward_for_current_agent_turn\n",
    "\n",
    "            done = termination or truncation\n",
    "            action_to_take = None\n",
    "\n",
    "            if done:\n",
    "                if agent_id_turn == my_agent_id and my_agent_id in last_processed_exp_learning_agent:\n",
    "                    prev_s_vc, prev_s_other, prev_a = last_processed_exp_learning_agent.pop(my_agent_id)\n",
    "                    # Process observation even if terminal for next_state in PER\n",
    "                    obs_dict_terminal = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in current_obs_raw.items()} if current_obs_raw else {}\n",
    "                    terminal_s_vc, terminal_s_other = agent.process_observation(obs_dict_terminal) if current_obs_raw else (np.zeros_like(prev_s_vc), np.zeros_like(prev_s_other))\n",
    "                    agent.step(prev_s_vc, prev_s_other, prev_a, reward_for_current_agent_turn, terminal_s_vc, terminal_s_other, True)\n",
    "            else:\n",
    "                if current_obs_raw is None: # Should not happen if not done\n",
    "                    action_to_take = env.action_space(agent_id_turn).sample() if env.action_space(agent_id_turn) else None\n",
    "                else:\n",
    "                    obs_dict_current = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in current_obs_raw.items()}\n",
    "                    current_s_vc, current_s_other = agent.process_observation(obs_dict_current)\n",
    "\n",
    "                    if agent_id_turn == my_agent_id:\n",
    "                        if my_agent_id in last_processed_exp_learning_agent:\n",
    "                            prev_s_vc, prev_s_other, prev_a = last_processed_exp_learning_agent.pop(my_agent_id)\n",
    "                            agent.step(prev_s_vc, prev_s_other, prev_a, reward_for_current_agent_turn, current_s_vc, current_s_other, False)\n",
    "                        \n",
    "                        action_to_take = agent.select_action(current_s_vc, current_s_other, epsilon)\n",
    "                        last_processed_exp_learning_agent[my_agent_id] = (current_s_vc, current_s_other, action_to_take)\n",
    "                        \n",
    "                        global_total_steps += 1\n",
    "                        if global_total_steps > MIN_EPSILON_FRAMES and epsilon > EPSILON_END : \n",
    "                            epsilon *= EPSILON_DECAY_RATE\n",
    "                            epsilon = max(EPSILON_END, epsilon)\n",
    "                        if global_total_steps % TARGET_UPDATE_EVERY == 0 and global_total_steps > 0: \n",
    "                            agent.update_target_net()\n",
    "                    else: \n",
    "                        action_to_take = agent.select_action(current_s_vc, current_s_other, epsilon=0.2) # Non-learning agents\n",
    "            \n",
    "            env.step(action_to_take) \n",
    "        \n",
    "        # LR Scheduler Step (e.g., per episode)\n",
    "        agent.lr_scheduler.step() # Call scheduler step\n",
    "\n",
    "        episode_score_my_agent = current_episode_rewards_accumulator.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score_my_agent)\n",
    "\n",
    "        if i_episode % 100 == 0: \n",
    "            avg_score_str = f\"{np.mean(scores_deque):.2f}\" if scores_deque else \"N/A\"\n",
    "            current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "            print(f'\\rEp {i_episode}\\tAvgScore({my_agent_id}): {avg_score_str}\\tEps: {epsilon:.4f}\\tLR: {current_lr:.2e}\\tLoss: {agent.current_loss:.4f}\\tBeta: {agent.beta:.3f}\\tSteps: {global_total_steps}')\n",
    "            if save_model_to: agent.save_model() \n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model() \n",
    "    print(f\"\\nCNN DQN Training finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    print(f\"Initiating CNN DQN training at {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(training_start_time))} UTC\")\n",
    "    try:\n",
    "        from til_environment import gridworld # Ensure this is your environment module\n",
    "        \n",
    "        NUM_RESUME_EPISODES = 100000 # Number of additional episodes to train\n",
    "        \n",
    "        # --- PATHS FOR RESUMING TRAINING ---\n",
    "        # LOAD from your previously trained model\n",
    "        LOAD_MODEL_PATH = \"my_wargame_cnn_agent_best.pth\" \n",
    "        # SAVE to a new file to indicate resumed training\n",
    "        SAVE_MODEL_PATH = \"my_wargame_cnn_agent_best_100000.pth\" # Example: old_episodes + new_episodes\n",
    "        # --- END PATHS ---\n",
    "        \n",
    "        NOVICE_MODE = False # Set to True if you are using the novice track\n",
    "\n",
    "        train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=NUM_RESUME_EPISODES, # Train for additional episodes\n",
    "            novice_track=NOVICE_MODE,\n",
    "            load_model_from=LOAD_MODEL_PATH, \n",
    "            save_model_to=SAVE_MODEL_PATH\n",
    "        )\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible and contains your PettingZoo environment.\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"Error: Model file not found. {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during CNN DQN training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    total_time_seconds = time.time() - training_start_time\n",
    "    print(f\"Total CNN DQN training time for this session: {total_time_seconds:.2f} seconds ({total_time_seconds/3600:.2f} hours).\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
