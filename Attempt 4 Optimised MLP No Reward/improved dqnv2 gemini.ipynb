{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3a9ee-e7f4-45f7-b836-8c70059b8d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time # For total training time\n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "INPUT_FEATURES = 288\n",
    "HIDDEN_LAYER_1_SIZE = 256\n",
    "HIDDEN_LAYER_2_SIZE = 256\n",
    "OUTPUT_ACTIONS = 5\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE_EVERY = 100 # Updated based on agent's own steps\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995 # Per episode\n",
    "\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_FRAMES = int(1e5)\n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- SumTree for Prioritized Replay Buffer ---\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "# --- Prioritized Replay Buffer ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta) # Added epsilon for stability\n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights /= (weights.max() if weights.max() > 0 else 1.0) # Normalize\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*[e for e in batch_data])\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority)\n",
    "        self.max_priority = max(self.max_priority, priorities.max() if priorities.size > 0 else self.max_priority)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- Deep Q-Network (DQN) Model ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        self.target_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "                # print(f\"Loaded pre-trained policy_net from {model_load_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            # print(f\"No model path provided or path invalid. Initializing policy_net with random weights.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.learn_step_counter = 0 # For UPDATE_EVERY\n",
    "        self.total_agent_steps = 0 # For TARGET_UPDATE_EVERY\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(8)] # Simplified\n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        processed_features = []\n",
    "        viewcone = observation_dict.get(\"viewcone\", [])\n",
    "        for r in range(7): # Rows in viewcone\n",
    "            for c in range(5): # Columns in viewcone\n",
    "                tile_value = viewcone[r][c] if r < len(viewcone) and c < len(viewcone[r]) else 0\n",
    "                processed_features.extend(self._unpack_viewcone_tile(tile_value))\n",
    "        \n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4\n",
    "        if 0 <= direction < 4: direction_one_hot[direction] = 1.0\n",
    "        processed_features.extend(direction_one_hot)\n",
    "\n",
    "        location = observation_dict.get(\"location\", [0, 0])\n",
    "        norm_x = location[0] / MAP_SIZE_X if MAP_SIZE_X > 0 else 0.0\n",
    "        norm_y = location[1] / MAP_SIZE_Y if MAP_SIZE_Y > 0 else 0.0\n",
    "        processed_features.extend([norm_x, norm_y])\n",
    "\n",
    "        processed_features.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        norm_step = observation_dict.get(\"step\", 0) / MAX_STEPS_PER_EPISODE if MAX_STEPS_PER_EPISODE > 0 else 0.0\n",
    "        processed_features.append(norm_step)\n",
    "        \n",
    "        if len(processed_features) != INPUT_FEATURES:\n",
    "            raise ValueError(f\"Feature length mismatch. Expected {INPUT_FEATURES}, got {len(processed_features)}\")\n",
    "        return np.array(processed_features, dtype=np.float32)\n",
    "\n",
    "    def select_action(self, state_np, epsilon=0.0):\n",
    "        if random.random() > epsilon:\n",
    "            state_tensor = torch.from_numpy(state_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): action_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.total_agent_steps += 1\n",
    "        \n",
    "        self.learn_step_counter = (self.learn_step_counter + 1) % UPDATE_EVERY\n",
    "        if self.learn_step_counter == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "\n",
    "        if self.total_agent_steps % TARGET_UPDATE_EVERY == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        q_next_policy_actions = self.policy_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        q_targets_next = self.target_net(next_states).detach().gather(1, q_next_policy_actions)\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "        print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_state(self): pass\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_agent(env_module, num_episodes=2000, novice_track=False, load_model_from=None, save_model_to=\"trained_dqn_agent.pth\"):\n",
    "    print(f\"Starting training: {num_episodes} episodes, Novice: {novice_track}, Load: {load_model_from}, Save: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "    \n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\" # Fallback if empty\n",
    "    # print(f\"Training agent ID: {my_agent_id}\")\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    epsilon = EPSILON_START\n",
    "    \n",
    "    # Store (state_np, action_int) for my_agent_id, to be completed with (reward, next_state_np, done)\n",
    "    pending_experience_my_agent = {} \n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset()\n",
    "        agent.reset_state()\n",
    "        current_episode_rewards = {agent_id: 0.0 for agent_id in env.possible_agents}\n",
    "        \n",
    "        for pet_agent_id in env.agent_iter():\n",
    "            current_observation_raw, current_reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            # Update reward for the agent whose turn it was *before* this env.last() call\n",
    "            # env.rewards contains rewards for all agents from the *previous* env.step()\n",
    "            for r_agent_id, r_value in env.rewards.items():\n",
    "                 if r_agent_id in current_episode_rewards: # Ensure agent is still tracked\n",
    "                    current_episode_rewards[r_agent_id] += r_value\n",
    "\n",
    "            action_to_take = None\n",
    "            if pet_agent_id == my_agent_id:\n",
    "                obs_dict_current_turn = {k: v if isinstance(v, (int, float)) else np.array(v).tolist() for k, v in current_observation_raw.items()}\n",
    "                \n",
    "                # If there's a pending experience (S, A) for my_agent_id, complete it now.\n",
    "                # S' is obs_dict_current_turn, R is current_reward (for S,A leading to S'), D is done.\n",
    "                if my_agent_id in pending_experience_my_agent:\n",
    "                    prev_state_np, prev_action = pending_experience_my_agent.pop(my_agent_id)\n",
    "                    next_state_np = agent.process_observation(obs_dict_current_turn)\n",
    "                    agent.step(prev_state_np, prev_action, current_reward, next_state_np, done)\n",
    "                \n",
    "                if done:\n",
    "                    action_to_take = None \n",
    "                else:\n",
    "                    current_state_np_for_action = agent.process_observation(obs_dict_current_turn)\n",
    "                    action_to_take = agent.select_action(current_state_np_for_action, epsilon)\n",
    "                    pending_experience_my_agent[my_agent_id] = (current_state_np_for_action, action_to_take)\n",
    "            \n",
    "            elif not done : # Other agents take random actions if not done\n",
    "                if env.action_space(pet_agent_id) is not None:\n",
    "                    action_to_take = env.action_space(pet_agent_id).sample()\n",
    "            \n",
    "            env.step(action_to_take)\n",
    "        \n",
    "        # After episode loop, handle any final pending experience if episode ended mid-turn for my_agent_id\n",
    "        # This case should be covered if 'done' is true for my_agent_id in its last turn handling.\n",
    "\n",
    "        episode_score = current_episode_rewards.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        epsilon = max(EPSILON_END, EPSILON_DECAY * epsilon)\n",
    "\n",
    "        if i_episode % 20 == 0: # Print less frequently\n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score (100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tSteps: {agent.total_agent_steps}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score (100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tSteps: {agent.total_agent_steps}')\n",
    "            if save_model_to: agent.save_model()\n",
    "        \n",
    "        # Example early stopping condition\n",
    "        # if len(scores_deque) == 100 and np.mean(scores_deque) >= 200.0:\n",
    "        #     print(f'\\nEnvironment solved in {i_episode} episodes!\\tAvg Score: {np.mean(scores_deque):.2f}')\n",
    "        #     if save_model_to: agent.save_model()\n",
    "        #     break\n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model() # Save one last time\n",
    "    print(f\"\\nTraining finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "    return scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    try:\n",
    "        from til_environment import gridworld\n",
    "        # print(\"Successfully imported til_environment.gridworld\")\n",
    "        \n",
    "        trained_scores = train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=10000, # Shorter for quick testing, increase for real training\n",
    "            novice_track=False,  # Set to False for varied maps if your env supports it\n",
    "            load_model_from=\"trained_dqn_agent_1k.pth\", # \"agent_166k_eps.pth\", # Set to a .pth file to resume\n",
    "            save_model_to=\"trained_dqn_agent_11k.pth\"\n",
    "        )\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    total_time = training_end_time - training_start_time\n",
    "    print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6c2b5-dcb7-4085-866e-274e089c26a9",
   "metadata": {},
   "source": [
    "## Gemini 2.5 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9b195-7621-4339-9215-15d6089c11e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time # For total training time\n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "INPUT_FEATURES = 288\n",
    "HIDDEN_LAYER_1_SIZE = 256\n",
    "HIDDEN_LAYER_2_SIZE = 256\n",
    "OUTPUT_ACTIONS = 5\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE_EVERY = 100 # Updated based on agent's own steps\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995 # Per episode\n",
    "\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_FRAMES = int(1e5)\n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- SumTree for Prioritized Replay Buffer ---\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "# --- Prioritized Replay Buffer ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta) # Added epsilon for stability\n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights /= (weights.max() if weights.max() > 0 else 1.0) # Normalize\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*[e for e in batch_data])\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority)\n",
    "        self.max_priority = max(self.max_priority, priorities.max() if priorities.size > 0 else self.max_priority)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- Deep Q-Network (DQN) Model ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        self.target_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        \n",
    "        self.is_pretrained = False # Flag to indicate if a model was successfully loaded\n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "                print(f\"Successfully loaded pre-trained policy_net from {model_load_path}\")\n",
    "                self.is_pretrained = True\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model with random weights.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        elif model_load_path: # Path provided but does not exist\n",
    "             print(f\"Model path {model_load_path} not found. Initializing policy_net with random weights.\")\n",
    "             self.policy_net.apply(self._initialize_weights)\n",
    "        else: # No path provided\n",
    "            print(f\"No model load path provided. Initializing policy_net with random weights.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.learn_step_counter = 0 # For UPDATE_EVERY\n",
    "        self.total_agent_steps = 0 # For TARGET_UPDATE_EVERY\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(8)] # Simplified\n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        processed_features = []\n",
    "        viewcone = observation_dict.get(\"viewcone\", [])\n",
    "        for r in range(7): # Rows in viewcone\n",
    "            for c in range(5): # Columns in viewcone\n",
    "                tile_value = viewcone[r][c] if r < len(viewcone) and c < len(viewcone[r]) else 0\n",
    "                processed_features.extend(self._unpack_viewcone_tile(tile_value))\n",
    "        \n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4\n",
    "        if 0 <= direction < 4: direction_one_hot[direction] = 1.0\n",
    "        processed_features.extend(direction_one_hot)\n",
    "\n",
    "        location = observation_dict.get(\"location\", [0, 0])\n",
    "        norm_x = location[0] / MAP_SIZE_X if MAP_SIZE_X > 0 else 0.0\n",
    "        norm_y = location[1] / MAP_SIZE_Y if MAP_SIZE_Y > 0 else 0.0\n",
    "        processed_features.extend([norm_x, norm_y])\n",
    "\n",
    "        processed_features.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        norm_step = observation_dict.get(\"step\", 0) / MAX_STEPS_PER_EPISODE if MAX_STEPS_PER_EPISODE > 0 else 0.0\n",
    "        processed_features.append(norm_step)\n",
    "        \n",
    "        if len(processed_features) != INPUT_FEATURES:\n",
    "            raise ValueError(f\"Feature length mismatch. Expected {INPUT_FEATURES}, got {len(processed_features)}\")\n",
    "        return np.array(processed_features, dtype=np.float32)\n",
    "\n",
    "    def select_action(self, state_np, epsilon=0.0):\n",
    "        if random.random() > epsilon:\n",
    "            state_tensor = torch.from_numpy(state_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): action_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.total_agent_steps += 1\n",
    "        \n",
    "        self.learn_step_counter = (self.learn_step_counter + 1) % UPDATE_EVERY\n",
    "        if self.learn_step_counter == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "\n",
    "        if self.total_agent_steps % TARGET_UPDATE_EVERY == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        q_next_policy_actions = self.policy_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        q_targets_next = self.target_net(next_states).detach().gather(1, q_next_policy_actions)\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "        print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_state(self): pass\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_agent(env_module, num_episodes=2000, novice_track=False, load_model_from=None, save_model_to=\"trained_dqn_agent.pth\"):\n",
    "    print(f\"Starting training: {num_episodes} episodes, Novice: {novice_track}, Load: {load_model_from}, Save: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "    \n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\" # Fallback if empty\n",
    "    # print(f\"Training agent ID: {my_agent_id}\")\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    if agent.is_pretrained:\n",
    "        print(f\"Resuming with loaded model. Setting epsilon to EPSILON_END: {EPSILON_END}\")\n",
    "        epsilon = EPSILON_END\n",
    "    else:\n",
    "        print(f\"Starting with new or re-initialized model. Setting epsilon to EPSILON_START: {EPSILON_START}\")\n",
    "        epsilon = EPSILON_START\n",
    "    \n",
    "    pending_experience_my_agent = {} \n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset()\n",
    "        agent.reset_state()\n",
    "        current_episode_rewards = {agent_id: 0.0 for agent_id in env.possible_agents}\n",
    "        \n",
    "        for pet_agent_id in env.agent_iter():\n",
    "            current_observation_raw, current_reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            for r_agent_id, r_value in env.rewards.items():\n",
    "                 if r_agent_id in current_episode_rewards: \n",
    "                    current_episode_rewards[r_agent_id] += r_value\n",
    "\n",
    "            action_to_take = None\n",
    "            if pet_agent_id == my_agent_id:\n",
    "                obs_dict_current_turn = {k: v if isinstance(v, (int, float)) else np.array(v).tolist() for k, v in current_observation_raw.items()}\n",
    "                # Process current observation once for use as next_state and current_state_for_action\n",
    "                processed_current_state_np = agent.process_observation(obs_dict_current_turn)\n",
    "                \n",
    "                if my_agent_id in pending_experience_my_agent:\n",
    "                    prev_state_np, prev_action = pending_experience_my_agent.pop(my_agent_id)\n",
    "                    # Use the processed_current_state_np as the next_state for the completed experience\n",
    "                    agent.step(prev_state_np, prev_action, current_reward, processed_current_state_np, done)\n",
    "                \n",
    "                if done:\n",
    "                    action_to_take = None \n",
    "                else:\n",
    "                    # Use the same processed_current_state_np for selecting the current action\n",
    "                    action_to_take = agent.select_action(processed_current_state_np, epsilon)\n",
    "                    # Store the processed state (used for action selection) as the 'state' for the next step's experience\n",
    "                    pending_experience_my_agent[my_agent_id] = (processed_current_state_np, action_to_take)\n",
    "            \n",
    "            elif not done : \n",
    "                if env.action_space(pet_agent_id) is not None:\n",
    "                    action_to_take = env.action_space(pet_agent_id).sample()\n",
    "            \n",
    "            env.step(action_to_take)\n",
    "\n",
    "        episode_score = current_episode_rewards.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        epsilon = max(EPSILON_END, EPSILON_DECAY * epsilon)\n",
    "\n",
    "        if i_episode % 20 == 0: \n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score (100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tSteps: {agent.total_agent_steps}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score (100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tSteps: {agent.total_agent_steps}')\n",
    "            if save_model_to: agent.save_model()\n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model() \n",
    "    print(f\"\\nTraining finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "    return scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    try:\n",
    "        from til_environment import gridworld\n",
    "        import cProfile\n",
    "        import pstats\n",
    "\n",
    "        # To profile your main training function\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.enable()\n",
    "        trained_scores = train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=100, \n",
    "            novice_track=False, \n",
    "            load_model_from=\"trained_dqn_agent_1300.pth\", \n",
    "            save_model_to=\"trained_dqn_agent_11k.pth\"\n",
    "        )\n",
    "        profiler.disable()\n",
    "        stats = pstats.Stats(profiler).sort_stats('cumtime') # or 'tottime'\n",
    "        stats.print_stats(20) # Print top 20 time-consuming functions\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    total_time = training_end_time - training_start_time\n",
    "    print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac120d-bc38-4985-9e39-2990a5a5d481",
   "metadata": {},
   "source": [
    "## Gemini 2.5 V3\n",
    "This profiling output is very insightful! It clearly shows where the time is being spent.\n",
    "\n",
    "Key Takeaways from the Profiler:\n",
    "\n",
    "Environment (gridworld.py) is the Major Bottleneck:\n",
    "\n",
    "gridworld.py:401(observe): 121.812 seconds (cumulative time). This is the single most time-consuming function.\n",
    "gridworld.py:760(step): 63.757 seconds.\n",
    "Helper functions within your environment like _is_visible (54.278s) and supercover_line (24.421s) are also very significant due to high call counts and their contribution to observe.\n",
    "Conclusion: Optimizing the environment's observe and step methods, and their internal helpers, would yield the largest speedups. Since I can't modify gridworld.py, my suggestions will focus on your agent and training loop code (dqnv2.py).\n",
    "Data Transfer to/from Device (.to() and .cpu()):\n",
    "\n",
    "{method 'to' of 'torch._C.TensorBase' objects}: 79.240 seconds (total time). This is primarily moving data (observations, batch data) to your DEVICE (e.g., GPU).\n",
    "{method 'cpu' of 'torch._C.TensorBase' objects}: 45.789 seconds (total time). This is moving data back to the CPU, for example, before converting to NumPy in select_action.\n",
    "Replay Buffer Sampling (PrioritizedReplayBuffer.sample):\n",
    "\n",
    "The sample method itself has a cumulative time of 62.627 seconds. A significant portion of this is likely the data conversion and transfer (.to()) mentioned above, as batches of experiences are prepared.\n",
    "Action Selection (TrainableRLAgent.select_action):\n",
    "\n",
    "Cumulative time of 65.778 seconds. This involves a forward pass of the policy network and the data transfers to and from the device.\n",
    "Optimizations for dqnv2.py Based on Profiler:\n",
    "\n",
    "Here are changes to your dqnv2.py script focusing on the identified bottlenecks within your agent's code:\n",
    "\n",
    "1. Optimize Data Transfers in PrioritizedReplayBuffer.sample()\n",
    "\n",
    "We can make the transfer of batched experiences to the device more efficient using pinned memory and non-blocking transfers. This allows the CPU to continue working while the GPU is busy with the data transfer.\n",
    "\n",
    "2. Optimize select_action() by using torch.argmax\n",
    "\n",
    "Instead of moving action values to the CPU to use np.argmax, we can use torch.argmax directly on the GPU tensor and then get the single action item.\n",
    "\n",
    "Here's the modified code:\n",
    "\n",
    "\n",
    "dqnv2.py\n",
    "v2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "Further Recommendations (Beyond these code changes):\n",
    "\n",
    "Optimize gridworld.py: This is crucial. Profile gridworld.py itself to pinpoint slow parts within observe and step. Vectorize operations with NumPy if there are Python loops handling grid data, optimize visibility calculations, etc.\n",
    "Parallel Environment Stepping: As discussed before, running multiple environment instances in parallel to collect data is one of the most effective ways to speed up RL training when the environment is slow. This involves more significant architectural changes (e.g., using multiprocessing or libraries like Ray).\n",
    "torch.compile() (PyTorch 2.0+): If you are using PyTorch 2.0 or newer, uncomment the torch.compile lines for your DQN models. This can JIT-compile your models and potentially speed up their execution significantly with minimal code change.\n",
    "Mixed Precision Training: If you have a compatible NVIDIA GPU, using torch.cuda.amp for mixed-precision training can speed up GPU computations and reduce memory usage. This involves using GradScaler and autocast.\n",
    "Re-profile: After these changes, re-profile to see how the bottlenecks have shifted and identify the next areas for optimization.\n",
    "The changes I've made should help reduce the overhead from data transfers and action selection in your agent's code. However, the environment's performance remains the dominant factor according to your profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917908cf-2f7c-493f-b74f-d19557d5ee16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to enable profiler...\n",
      "Profiler enabled.\n",
      "Starting training: 100 episodes, Novice: False, Load: trained_dqn_agent_1500.pth, Save: trained_dqn_agent_1600.pth\n",
      "Using device: cuda\n",
      "Successfully loaded pre-trained policy_net from trained_dqn_agent_1500.pth\n",
      "Resuming with loaded model. Setting epsilon to EPSILON_END: 0.01\n",
      "Episode 100\tAvg Score (100): 14.21\tEpsilon: 0.0100\tSteps: 9941\n",
      "Model saved to trained_dqn_agent_1600.pth\n",
      "Model saved to trained_dqn_agent_1600.pth\n",
      "\n",
      "Training finished. Final model saved to trained_dqn_agent_1600.pth\n",
      "Disabling profiler...\n",
      "Profiler disabled.\n",
      "Generating profiler stats...\n",
      "         89317443 function calls (87886765 primitive calls) in 234.716 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 734 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    80328   20.784    0.000  124.252    0.002 /home/jupyter/til-25-data-chefs/til-25-environment/til_environment/gridworld.py:401(observe)\n",
      "    40164    0.125    0.000   66.910    0.002 /opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:62(step)\n",
      "80328/40164    0.106    0.000   66.531    0.002 /opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/base.py:46(step)\n",
      "    40164    0.173    0.000   66.482    0.002 /opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py:16(step)\n",
      "    40164    0.553    0.000   65.501    0.002 /home/jupyter/til-25-data-chefs/til-25-environment/til_environment/gridworld.py:760(step)\n",
      "     9941    0.489    0.000   65.148    0.007 /var/tmp/ipykernel_125804/2027340217.py:242(select_action)\n",
      "    40164    0.210    0.000   64.144    0.002 /opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/env.py:180(last)\n",
      "    40164    0.064    0.000   62.882    0.002 /opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:72(observe)\n",
      "80328/40164    0.112    0.000   62.816    0.002 /opt/conda/envs/env/lib/python3.12/site-packages/pettingzoo/utils/wrappers/base.py:40(observe)\n",
      "  2217032   15.060    0.000   55.487    0.000 /home/jupyter/til-25-data-chefs/til-25-environment/til_environment/gridworld.py:369(_is_visible)\n",
      "    39547   36.135    0.001   36.135    0.001 {method 'item' of 'torch._C.TensorBase' objects}\n",
      "     9941    0.189    0.000   32.465    0.003 /var/tmp/ipykernel_125804/2027340217.py:255(step)\n",
      "     2477    0.621    0.000   27.055    0.011 /var/tmp/ipykernel_125804/2027340217.py:268(learn)\n",
      "  2136704   16.130    0.000   24.975    0.000 /home/jupyter/til-25-data-chefs/til-25-environment/til_environment/helpers.py:93(supercover_line)\n",
      "    42036   21.524    0.001   21.524    0.001 {method 'to' of 'torch._C.TensorBase' objects}\n",
      "  2811480    8.478    0.000   14.046    0.000 /home/jupyter/til-25-data-chefs/til-25-environment/til_environment/helpers.py:30(view_to_world)\n",
      "     2477   13.673    0.006   13.673    0.006 {method 'cpu' of 'torch._C.TensorBase' objects}\n",
      "  2811480    7.681    0.000   12.508    0.000 /home/jupyter/til-25-data-chefs/til-25-environment/til_environment/helpers.py:68(idx_to_view)\n",
      "  2250122    2.141    0.000   11.840    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      " 10150047   10.956    0.000   10.956    0.000 {built-in method numpy.array}\n",
      "\n",
      "\n",
      "Total training time: 234.80 seconds (3.91 minutes)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time # For total training time\n",
    "import cProfile \n",
    "import pstats   \n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "INPUT_FEATURES = 288\n",
    "HIDDEN_LAYER_1_SIZE = 256\n",
    "HIDDEN_LAYER_2_SIZE = 256\n",
    "OUTPUT_ACTIONS = 5\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE_EVERY = 100 # Updated based on agent's own steps\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995 # Per episode\n",
    "\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_FRAMES = int(1e5)\n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- SumTree for Prioritized Replay Buffer ---\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "# --- Prioritized Replay Buffer ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights_np = np.empty(batch_size, dtype=np.float32) \n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights_np[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta) \n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights_np /= (weights_np.max() if weights_np.max() > 0 else 1.0) \n",
    "\n",
    "        states_list, actions_list, rewards_list, next_states_list, dones_list = zip(*[e for e in batch_data])\n",
    "        \n",
    "        states_np = np.ascontiguousarray(np.vstack(states_list), dtype=np.float32)\n",
    "        actions_np = np.ascontiguousarray(np.vstack(actions_list), dtype=np.int64) \n",
    "        rewards_np = np.ascontiguousarray(np.vstack(rewards_list), dtype=np.float32)\n",
    "        next_states_np = np.ascontiguousarray(np.vstack(next_states_list), dtype=np.float32)\n",
    "        dones_np = np.ascontiguousarray(np.vstack(dones_list).astype(np.uint8), dtype=np.float32)\n",
    "\n",
    "        use_pinned_memory = DEVICE.type == 'cuda'\n",
    "\n",
    "        states = torch.from_numpy(states_np).float()\n",
    "        actions = torch.from_numpy(actions_np).long()\n",
    "        rewards = torch.from_numpy(rewards_np).float()\n",
    "        next_states = torch.from_numpy(next_states_np).float()\n",
    "        dones = torch.from_numpy(dones_np).float() \n",
    "        weights = torch.from_numpy(weights_np).float()\n",
    "\n",
    "        if use_pinned_memory:\n",
    "            states = states.pin_memory()\n",
    "            actions = actions.pin_memory()\n",
    "            rewards = rewards.pin_memory()\n",
    "            next_states = next_states.pin_memory()\n",
    "            dones = dones.pin_memory()\n",
    "            weights = weights.pin_memory()\n",
    "\n",
    "        states = states.to(DEVICE, non_blocking=use_pinned_memory)\n",
    "        actions = actions.to(DEVICE, non_blocking=use_pinned_memory)\n",
    "        rewards = rewards.to(DEVICE, non_blocking=use_pinned_memory)\n",
    "        next_states = next_states.to(DEVICE, non_blocking=use_pinned_memory)\n",
    "        dones = dones.to(DEVICE, non_blocking=use_pinned_memory)\n",
    "        weights = weights.to(DEVICE, non_blocking=use_pinned_memory)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), batch_idx, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority_val in zip(batch_indices, priorities): \n",
    "            self.tree.update(idx, priority_val)\n",
    "        self.max_priority = max(self.max_priority, priorities.max() if priorities.size > 0 else self.max_priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- Deep Q-Network (DQN) Model ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        self.target_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        \n",
    "        self.is_pretrained = False \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "                print(f\"Successfully loaded pre-trained policy_net from {model_load_path}\")\n",
    "                self.is_pretrained = True\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model with random weights.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        elif model_load_path: \n",
    "             print(f\"Model path {model_load_path} not found. Initializing policy_net with random weights.\")\n",
    "             self.policy_net.apply(self._initialize_weights)\n",
    "        else: \n",
    "            print(f\"No model load path provided. Initializing policy_net with random weights.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.learn_step_counter = 0 \n",
    "        self.total_agent_steps = 0 \n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(8)] \n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        processed_features = []\n",
    "        viewcone = observation_dict.get(\"viewcone\", [])\n",
    "        for r in range(7): \n",
    "            for c in range(5): \n",
    "                tile_value = viewcone[r][c] if r < len(viewcone) and c < len(viewcone[r]) else 0\n",
    "                processed_features.extend(self._unpack_viewcone_tile(tile_value))\n",
    "        \n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4\n",
    "        if 0 <= direction < 4: direction_one_hot[direction] = 1.0\n",
    "        processed_features.extend(direction_one_hot)\n",
    "\n",
    "        location = observation_dict.get(\"location\", [0, 0])\n",
    "        norm_x = location[0] / MAP_SIZE_X if MAP_SIZE_X > 0 else 0.0\n",
    "        norm_y = location[1] / MAP_SIZE_Y if MAP_SIZE_Y > 0 else 0.0\n",
    "        processed_features.extend([norm_x, norm_y])\n",
    "\n",
    "        processed_features.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        norm_step = observation_dict.get(\"step\", 0) / MAX_STEPS_PER_EPISODE if MAX_STEPS_PER_EPISODE > 0 else 0.0\n",
    "        processed_features.append(norm_step)\n",
    "        \n",
    "        if len(processed_features) != INPUT_FEATURES:\n",
    "            raise ValueError(f\"Feature length mismatch. Expected {INPUT_FEATURES}, got {len(processed_features)}\")\n",
    "        return np.array(processed_features, dtype=np.float32)\n",
    "\n",
    "    def select_action(self, state_np, epsilon=0.0):\n",
    "        # If exploring (random.random() <= epsilon), choose a random action first\n",
    "        if random.random() <= epsilon:\n",
    "            return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "        # Else (not exploring), use the policy network\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(np.ascontiguousarray(state_np)).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): \n",
    "                action_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train()\n",
    "            return torch.argmax(action_values, dim=1).item()\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.total_agent_steps += 1\n",
    "        \n",
    "        self.learn_step_counter = (self.learn_step_counter + 1) % UPDATE_EVERY\n",
    "        if self.learn_step_counter == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "\n",
    "        if self.total_agent_steps % TARGET_UPDATE_EVERY == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            q_next_policy_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            q_targets_next = self.target_net(next_states).gather(1, q_next_policy_actions)\n",
    "            q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        q_expected = self.policy_net(states).gather(1, actions)\n",
    "        td_errors_tensor = (q_targets - q_expected).abs() \n",
    "        self.memory.update_priorities(indices, td_errors_tensor.cpu().detach().numpy().flatten())\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "        print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_state(self): pass\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_agent(env_module, num_episodes=2000, novice_track=False, load_model_from=None, save_model_to=\"trained_dqn_agent.pth\"):\n",
    "    print(f\"Starting training: {num_episodes} episodes, Novice: {novice_track}, Load: {load_model_from}, Save: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "    \n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\" \n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    if agent.is_pretrained:\n",
    "        print(f\"Resuming with loaded model. Setting epsilon to EPSILON_END: {EPSILON_END}\")\n",
    "        epsilon = EPSILON_END\n",
    "    else:\n",
    "        print(f\"Starting with new or re-initialized model. Setting epsilon to EPSILON_START: {EPSILON_START}\")\n",
    "        epsilon = EPSILON_START\n",
    "    \n",
    "    pending_experience_my_agent = {} \n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset()\n",
    "        agent.reset_state()\n",
    "        current_episode_rewards = {agent_id: 0.0 for agent_id in env.possible_agents}\n",
    "        \n",
    "        for pet_agent_id in env.agent_iter():\n",
    "            current_observation_raw, current_reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            for r_agent_id, r_value in env.rewards.items():\n",
    "                 if r_agent_id in current_episode_rewards: \n",
    "                    current_episode_rewards[r_agent_id] += r_value\n",
    "\n",
    "            action_to_take = None\n",
    "            if pet_agent_id == my_agent_id:\n",
    "                obs_dict_current_turn = {k: v if isinstance(v, (int, float)) else np.array(v).tolist() for k, v in current_observation_raw.items()}\n",
    "                processed_current_state_np = agent.process_observation(obs_dict_current_turn)\n",
    "                \n",
    "                if my_agent_id in pending_experience_my_agent:\n",
    "                    prev_state_np, prev_action = pending_experience_my_agent.pop(my_agent_id)\n",
    "                    agent.step(prev_state_np, prev_action, current_reward, processed_current_state_np, done)\n",
    "                \n",
    "                if done:\n",
    "                    action_to_take = None \n",
    "                else:\n",
    "                    action_to_take = agent.select_action(processed_current_state_np, epsilon)\n",
    "                    pending_experience_my_agent[my_agent_id] = (processed_current_state_np, action_to_take)\n",
    "            \n",
    "            elif not done : \n",
    "                if env.action_space(pet_agent_id) is not None:\n",
    "                    action_to_take = env.action_space(pet_agent_id).sample()\n",
    "            \n",
    "            env.step(action_to_take)\n",
    "\n",
    "        episode_score = current_episode_rewards.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        if not agent.is_pretrained or i_episode > 100: \n",
    "             epsilon = max(EPSILON_END, EPSILON_DECAY * epsilon)\n",
    "\n",
    "        if i_episode % 20 == 0: \n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score (100): {(np.mean(scores_deque) if scores_deque else 0.0):.2f}\\tEpsilon: {epsilon:.4f}\\tSteps: {agent.total_agent_steps}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score (100): {(np.mean(scores_deque) if scores_deque else 0.0):.2f}\\tEpsilon: {epsilon:.4f}\\tSteps: {agent.total_agent_steps}')\n",
    "            if save_model_to: agent.save_model()\n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model() \n",
    "    print(f\"\\nTraining finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "    return scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    profiler_instance = cProfile.Profile()\n",
    "    profiler_enabled_this_run = False\n",
    "    \n",
    "    try:\n",
    "        from til_environment import gridworld \n",
    "        \n",
    "        print(\"Attempting to enable profiler...\")\n",
    "        profiler_instance.enable()\n",
    "        profiler_enabled_this_run = True\n",
    "        print(\"Profiler enabled.\")\n",
    "        \n",
    "        trained_scores = train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=100, \n",
    "            novice_track=False, \n",
    "            load_model_from=\"trained_dqn_agent_1500.pth\", \n",
    "            save_model_to=\"trained_dqn_agent_1600.pth\"\n",
    "        )\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\") \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if profiler_enabled_this_run:\n",
    "            print(\"Disabling profiler...\")\n",
    "            profiler_instance.disable()\n",
    "            print(\"Profiler disabled.\")\n",
    "            print(\"Generating profiler stats...\")\n",
    "            stats = pstats.Stats(profiler_instance).sort_stats('cumtime') \n",
    "            stats.print_stats(20)\n",
    "        elif 'profiler_instance' in locals() and hasattr(profiler_instance, '_active_count') and profiler_instance._active_count > 0:\n",
    "            print(\"Profiler was active unexpectedly, attempting to disable...\")\n",
    "            profiler_instance.disable()\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    total_time = training_end_time - training_start_time\n",
    "    print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
