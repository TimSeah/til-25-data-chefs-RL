{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ac65c-31cb-491c-b048-0524f5605cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported til_environment.gridworld\n",
      "Training agent: player_0\n",
      "Action space for player_0: Discrete(5)\n",
      "Observation space for player_0: Dict('direction': Discrete(4), 'location': Box(0, 16, (2,), uint8), 'scout': Discrete(2), 'step': Discrete(100), 'viewcone': Box(0, 255, (7, 5), uint8))\n",
      "Using device: cuda\n",
      "No model path provided or path agent_66k_eps.pth does not exist. Initializing policy_net with random weights.\n",
      "Episode 100\tAverage Score (last 100): 6.83\tEpsilon: 0.6058\tTotal Steps: 9975\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 200\tAverage Score (last 100): 5.95\tEpsilon: 0.3670\tTotal Steps: 19930\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 300\tAverage Score (last 100): 7.39\tEpsilon: 0.2223\tTotal Steps: 29894\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 400\tAverage Score (last 100): 7.64\tEpsilon: 0.1347\tTotal Steps: 39720\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 500\tAverage Score (last 100): 8.59\tEpsilon: 0.0816\tTotal Steps: 49691\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 600\tAverage Score (last 100): 9.63\tEpsilon: 0.0494\tTotal Steps: 59475\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 700\tAverage Score (last 100): 7.73\tEpsilon: 0.0299\tTotal Steps: 69436\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 800\tAverage Score (last 100): 9.63\tEpsilon: 0.0181\tTotal Steps: 793931\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 900\tAverage Score (last 100): 13.73\tEpsilon: 0.0110\tTotal Steps: 89311\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1000\tAverage Score (last 100): 10.57\tEpsilon: 0.0100\tTotal Steps: 99253\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1200\tAverage Score (last 100): 10.58\tEpsilon: 0.0100\tTotal Steps: 119168\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1300\tAverage Score (last 100): 11.51\tEpsilon: 0.0100\tTotal Steps: 129151\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1400\tAverage Score (last 100): 12.06\tEpsilon: 0.0100\tTotal Steps: 139123\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1500\tAverage Score (last 100): 14.51\tEpsilon: 0.0100\tTotal Steps: 149039\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1600\tAverage Score (last 100): 16.78\tEpsilon: 0.0100\tTotal Steps: 159033\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1700\tAverage Score (last 100): 14.45\tEpsilon: 0.0100\tTotal Steps: 169016\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1800\tAverage Score (last 100): 16.11\tEpsilon: 0.0100\tTotal Steps: 179016\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 1900\tAverage Score (last 100): 19.11\tEpsilon: 0.0100\tTotal Steps: 189005\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2000\tAverage Score (last 100): 20.48\tEpsilon: 0.0100\tTotal Steps: 198891\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2100\tAverage Score (last 100): 17.30\tEpsilon: 0.0100\tTotal Steps: 208717\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2200\tAverage Score (last 100): 20.32\tEpsilon: 0.0100\tTotal Steps: 218594\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2300\tAverage Score (last 100): 22.12\tEpsilon: 0.0100\tTotal Steps: 228483\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2400\tAverage Score (last 100): 19.10\tEpsilon: 0.0100\tTotal Steps: 238421\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2500\tAverage Score (last 100): 22.49\tEpsilon: 0.0100\tTotal Steps: 248332\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2600\tAverage Score (last 100): 26.62\tEpsilon: 0.0100\tTotal Steps: 257840\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2700\tAverage Score (last 100): 23.83\tEpsilon: 0.0100\tTotal Steps: 267745\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2800\tAverage Score (last 100): 24.41\tEpsilon: 0.0100\tTotal Steps: 277617\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 2900\tAverage Score (last 100): 21.87\tEpsilon: 0.0100\tTotal Steps: 287461\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3000\tAverage Score (last 100): 25.84\tEpsilon: 0.0100\tTotal Steps: 297176\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3100\tAverage Score (last 100): 20.89\tEpsilon: 0.0100\tTotal Steps: 307129\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3200\tAverage Score (last 100): 23.73\tEpsilon: 0.0100\tTotal Steps: 316825\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3300\tAverage Score (last 100): 21.73\tEpsilon: 0.0100\tTotal Steps: 326598\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3400\tAverage Score (last 100): 24.03\tEpsilon: 0.0100\tTotal Steps: 335995\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3500\tAverage Score (last 100): 20.15\tEpsilon: 0.0100\tTotal Steps: 345720\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3600\tAverage Score (last 100): 22.17\tEpsilon: 0.0100\tTotal Steps: 355507\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3700\tAverage Score (last 100): 32.67\tEpsilon: 0.0100\tTotal Steps: 364312\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3800\tAverage Score (last 100): 25.14\tEpsilon: 0.0100\tTotal Steps: 374083\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 3900\tAverage Score (last 100): 25.82\tEpsilon: 0.0100\tTotal Steps: 383695\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4000\tAverage Score (last 100): 23.06\tEpsilon: 0.0100\tTotal Steps: 393598\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4100\tAverage Score (last 100): 22.54\tEpsilon: 0.0100\tTotal Steps: 403436\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4200\tAverage Score (last 100): 23.28\tEpsilon: 0.0100\tTotal Steps: 413324\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4300\tAverage Score (last 100): 22.17\tEpsilon: 0.0100\tTotal Steps: 423025\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4400\tAverage Score (last 100): 27.37\tEpsilon: 0.0100\tTotal Steps: 432320\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4500\tAverage Score (last 100): 22.58\tEpsilon: 0.0100\tTotal Steps: 441939\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4600\tAverage Score (last 100): 25.83\tEpsilon: 0.0100\tTotal Steps: 451562\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4700\tAverage Score (last 100): 30.40\tEpsilon: 0.0100\tTotal Steps: 461170\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4800\tAverage Score (last 100): 27.92\tEpsilon: 0.0100\tTotal Steps: 470729\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 4900\tAverage Score (last 100): 27.04\tEpsilon: 0.0100\tTotal Steps: 480335\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5000\tAverage Score (last 100): 28.78\tEpsilon: 0.0100\tTotal Steps: 489807\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5100\tAverage Score (last 100): 27.35\tEpsilon: 0.0100\tTotal Steps: 499353\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5200\tAverage Score (last 100): 27.37\tEpsilon: 0.0100\tTotal Steps: 508868\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5300\tAverage Score (last 100): 32.82\tEpsilon: 0.0100\tTotal Steps: 517547\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5400\tAverage Score (last 100): 30.02\tEpsilon: 0.0100\tTotal Steps: 526685\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5500\tAverage Score (last 100): 25.40\tEpsilon: 0.0100\tTotal Steps: 535940\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5600\tAverage Score (last 100): 27.10\tEpsilon: 0.0100\tTotal Steps: 544880\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5700\tAverage Score (last 100): 27.10\tEpsilon: 0.0100\tTotal Steps: 554533\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5800\tAverage Score (last 100): 29.49\tEpsilon: 0.0100\tTotal Steps: 563878\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 5900\tAverage Score (last 100): 30.61\tEpsilon: 0.0100\tTotal Steps: 572990\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6000\tAverage Score (last 100): 25.08\tEpsilon: 0.0100\tTotal Steps: 582427\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6100\tAverage Score (last 100): 24.51\tEpsilon: 0.0100\tTotal Steps: 592067\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6200\tAverage Score (last 100): 24.66\tEpsilon: 0.0100\tTotal Steps: 601690\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6300\tAverage Score (last 100): 24.76\tEpsilon: 0.0100\tTotal Steps: 611262\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6400\tAverage Score (last 100): 30.87\tEpsilon: 0.0100\tTotal Steps: 620493\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6500\tAverage Score (last 100): 35.86\tEpsilon: 0.0100\tTotal Steps: 628726\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6600\tAverage Score (last 100): 33.41\tEpsilon: 0.0100\tTotal Steps: 637753\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6700\tAverage Score (last 100): 33.72\tEpsilon: 0.0100\tTotal Steps: 646189\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6800\tAverage Score (last 100): 34.61\tEpsilon: 0.0100\tTotal Steps: 655094\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 6900\tAverage Score (last 100): 37.87\tEpsilon: 0.0100\tTotal Steps: 663179\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7000\tAverage Score (last 100): 30.44\tEpsilon: 0.0100\tTotal Steps: 672265\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7100\tAverage Score (last 100): 39.39\tEpsilon: 0.0100\tTotal Steps: 680313\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7200\tAverage Score (last 100): 31.13\tEpsilon: 0.0100\tTotal Steps: 689467\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7300\tAverage Score (last 100): 37.86\tEpsilon: 0.0100\tTotal Steps: 697263\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7400\tAverage Score (last 100): 32.20\tEpsilon: 0.0100\tTotal Steps: 705862\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7500\tAverage Score (last 100): 25.56\tEpsilon: 0.0100\tTotal Steps: 715155\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7600\tAverage Score (last 100): 40.99\tEpsilon: 0.0100\tTotal Steps: 723309\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7700\tAverage Score (last 100): 30.05\tEpsilon: 0.0100\tTotal Steps: 732229\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7800\tAverage Score (last 100): 36.10\tEpsilon: 0.0100\tTotal Steps: 740617\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 7900\tAverage Score (last 100): 43.10\tEpsilon: 0.0100\tTotal Steps: 748449\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8000\tAverage Score (last 100): 32.88\tEpsilon: 0.0100\tTotal Steps: 756687\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8100\tAverage Score (last 100): 38.03\tEpsilon: 0.0100\tTotal Steps: 764815\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8200\tAverage Score (last 100): 40.39\tEpsilon: 0.0100\tTotal Steps: 772514\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8300\tAverage Score (last 100): 40.41\tEpsilon: 0.0100\tTotal Steps: 780815\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8400\tAverage Score (last 100): 41.32\tEpsilon: 0.0100\tTotal Steps: 788934\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8500\tAverage Score (last 100): 36.61\tEpsilon: 0.0100\tTotal Steps: 797128\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8600\tAverage Score (last 100): 41.10\tEpsilon: 0.0100\tTotal Steps: 805419\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8700\tAverage Score (last 100): 46.61\tEpsilon: 0.0100\tTotal Steps: 812783\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8800\tAverage Score (last 100): 41.71\tEpsilon: 0.0100\tTotal Steps: 820658\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 8900\tAverage Score (last 100): 35.70\tEpsilon: 0.0100\tTotal Steps: 828981\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9000\tAverage Score (last 100): 43.50\tEpsilon: 0.0100\tTotal Steps: 836813\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9100\tAverage Score (last 100): 43.65\tEpsilon: 0.0100\tTotal Steps: 844800\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9200\tAverage Score (last 100): 39.17\tEpsilon: 0.0100\tTotal Steps: 853266\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9300\tAverage Score (last 100): 35.99\tEpsilon: 0.0100\tTotal Steps: 861662\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9400\tAverage Score (last 100): 49.47\tEpsilon: 0.0100\tTotal Steps: 868975\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9500\tAverage Score (last 100): 43.48\tEpsilon: 0.0100\tTotal Steps: 876861\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9600\tAverage Score (last 100): 42.14\tEpsilon: 0.0100\tTotal Steps: 884862\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9700\tAverage Score (last 100): 42.81\tEpsilon: 0.0100\tTotal Steps: 892984\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9800\tAverage Score (last 100): 43.37\tEpsilon: 0.0100\tTotal Steps: 900722\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 9900\tAverage Score (last 100): 39.96\tEpsilon: 0.0100\tTotal Steps: 908668\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10000\tAverage Score (last 100): 38.20\tEpsilon: 0.0100\tTotal Steps: 916850\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10100\tAverage Score (last 100): 37.29\tEpsilon: 0.0100\tTotal Steps: 924881\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10200\tAverage Score (last 100): 45.99\tEpsilon: 0.0100\tTotal Steps: 932410\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10300\tAverage Score (last 100): 40.14\tEpsilon: 0.0100\tTotal Steps: 940122\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10400\tAverage Score (last 100): 39.77\tEpsilon: 0.0100\tTotal Steps: 947925\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10500\tAverage Score (last 100): 42.98\tEpsilon: 0.0100\tTotal Steps: 955785\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10600\tAverage Score (last 100): 41.51\tEpsilon: 0.0100\tTotal Steps: 963018\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10700\tAverage Score (last 100): 50.94\tEpsilon: 0.0100\tTotal Steps: 970121\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10800\tAverage Score (last 100): 48.38\tEpsilon: 0.0100\tTotal Steps: 976839\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 10900\tAverage Score (last 100): 42.19\tEpsilon: 0.0100\tTotal Steps: 984832\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11000\tAverage Score (last 100): 45.47\tEpsilon: 0.0100\tTotal Steps: 991907\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11100\tAverage Score (last 100): 39.26\tEpsilon: 0.0100\tTotal Steps: 1000139\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11200\tAverage Score (last 100): 45.00\tEpsilon: 0.0100\tTotal Steps: 1007469\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11300\tAverage Score (last 100): 48.77\tEpsilon: 0.0100\tTotal Steps: 1014784\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11400\tAverage Score (last 100): 51.83\tEpsilon: 0.0100\tTotal Steps: 1020955\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11500\tAverage Score (last 100): 46.87\tEpsilon: 0.0100\tTotal Steps: 1027935\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11600\tAverage Score (last 100): 40.73\tEpsilon: 0.0100\tTotal Steps: 1035698\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11700\tAverage Score (last 100): 44.49\tEpsilon: 0.0100\tTotal Steps: 1043022\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11800\tAverage Score (last 100): 39.39\tEpsilon: 0.0100\tTotal Steps: 1050508\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 11900\tAverage Score (last 100): 47.41\tEpsilon: 0.0100\tTotal Steps: 1057637\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12000\tAverage Score (last 100): 53.50\tEpsilon: 0.0100\tTotal Steps: 1064424\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12100\tAverage Score (last 100): 44.66\tEpsilon: 0.0100\tTotal Steps: 1071719\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12200\tAverage Score (last 100): 47.76\tEpsilon: 0.0100\tTotal Steps: 1078921\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12300\tAverage Score (last 100): 52.76\tEpsilon: 0.0100\tTotal Steps: 1085424\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12400\tAverage Score (last 100): 43.13\tEpsilon: 0.0100\tTotal Steps: 1093119\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12500\tAverage Score (last 100): 50.20\tEpsilon: 0.0100\tTotal Steps: 1099754\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12600\tAverage Score (last 100): 53.41\tEpsilon: 0.0100\tTotal Steps: 1106017\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12700\tAverage Score (last 100): 39.53\tEpsilon: 0.0100\tTotal Steps: 1114124\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12800\tAverage Score (last 100): 48.41\tEpsilon: 0.0100\tTotal Steps: 1120894\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 12900\tAverage Score (last 100): 46.58\tEpsilon: 0.0100\tTotal Steps: 1128612\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13000\tAverage Score (last 100): 49.96\tEpsilon: 0.0100\tTotal Steps: 1135353\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13100\tAverage Score (last 100): 38.93\tEpsilon: 0.0100\tTotal Steps: 1143682\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13200\tAverage Score (last 100): 49.63\tEpsilon: 0.0100\tTotal Steps: 1150621\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13300\tAverage Score (last 100): 35.81\tEpsilon: 0.0100\tTotal Steps: 1159156\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13400\tAverage Score (last 100): 46.24\tEpsilon: 0.0100\tTotal Steps: 1166302\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13500\tAverage Score (last 100): 46.20\tEpsilon: 0.0100\tTotal Steps: 1173629\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13600\tAverage Score (last 100): 57.98\tEpsilon: 0.0100\tTotal Steps: 1179835\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13700\tAverage Score (last 100): 44.64\tEpsilon: 0.0100\tTotal Steps: 1187240\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13800\tAverage Score (last 100): 46.03\tEpsilon: 0.0100\tTotal Steps: 1194971\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 13900\tAverage Score (last 100): 47.15\tEpsilon: 0.0100\tTotal Steps: 1201883\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14000\tAverage Score (last 100): 45.87\tEpsilon: 0.0100\tTotal Steps: 1209311\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14100\tAverage Score (last 100): 50.44\tEpsilon: 0.0100\tTotal Steps: 1216296\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14200\tAverage Score (last 100): 42.53\tEpsilon: 0.0100\tTotal Steps: 1224190\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14300\tAverage Score (last 100): 53.74\tEpsilon: 0.0100\tTotal Steps: 1231043\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14400\tAverage Score (last 100): 36.68\tEpsilon: 0.0100\tTotal Steps: 1239568\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14500\tAverage Score (last 100): 51.40\tEpsilon: 0.0100\tTotal Steps: 1246823\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14600\tAverage Score (last 100): 45.18\tEpsilon: 0.0100\tTotal Steps: 1254181\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14700\tAverage Score (last 100): 47.79\tEpsilon: 0.0100\tTotal Steps: 1261118\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14800\tAverage Score (last 100): 48.48\tEpsilon: 0.0100\tTotal Steps: 1268256\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 14900\tAverage Score (last 100): 43.18\tEpsilon: 0.0100\tTotal Steps: 1275995\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15000\tAverage Score (last 100): 43.55\tEpsilon: 0.0100\tTotal Steps: 1282956\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15100\tAverage Score (last 100): 52.37\tEpsilon: 0.0100\tTotal Steps: 1289381\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15200\tAverage Score (last 100): 47.72\tEpsilon: 0.0100\tTotal Steps: 1296568\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15300\tAverage Score (last 100): 48.49\tEpsilon: 0.0100\tTotal Steps: 1303499\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15400\tAverage Score (last 100): 51.55\tEpsilon: 0.0100\tTotal Steps: 1310317\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15500\tAverage Score (last 100): 50.72\tEpsilon: 0.0100\tTotal Steps: 1317027\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15600\tAverage Score (last 100): 46.90\tEpsilon: 0.0100\tTotal Steps: 1324172\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15700\tAverage Score (last 100): 49.99\tEpsilon: 0.0100\tTotal Steps: 1330722\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15800\tAverage Score (last 100): 50.31\tEpsilon: 0.0100\tTotal Steps: 1337519\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 15900\tAverage Score (last 100): 46.00\tEpsilon: 0.0100\tTotal Steps: 1344826\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16000\tAverage Score (last 100): 50.22\tEpsilon: 0.0100\tTotal Steps: 1351682\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16100\tAverage Score (last 100): 42.20\tEpsilon: 0.0100\tTotal Steps: 1359445\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16200\tAverage Score (last 100): 43.50\tEpsilon: 0.0100\tTotal Steps: 1367079\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16300\tAverage Score (last 100): 55.04\tEpsilon: 0.0100\tTotal Steps: 1373514\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16400\tAverage Score (last 100): 46.74\tEpsilon: 0.0100\tTotal Steps: 1380727\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16500\tAverage Score (last 100): 50.28\tEpsilon: 0.0100\tTotal Steps: 1387874\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16600\tAverage Score (last 100): 46.96\tEpsilon: 0.0100\tTotal Steps: 1395103\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16700\tAverage Score (last 100): 50.35\tEpsilon: 0.0100\tTotal Steps: 1401591\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16800\tAverage Score (last 100): 46.12\tEpsilon: 0.0100\tTotal Steps: 1408002\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 16900\tAverage Score (last 100): 48.23\tEpsilon: 0.0100\tTotal Steps: 1414800\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17000\tAverage Score (last 100): 42.01\tEpsilon: 0.0100\tTotal Steps: 1422132\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17100\tAverage Score (last 100): 47.09\tEpsilon: 0.0100\tTotal Steps: 1428766\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17200\tAverage Score (last 100): 49.05\tEpsilon: 0.0100\tTotal Steps: 1436110\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17300\tAverage Score (last 100): 45.63\tEpsilon: 0.0100\tTotal Steps: 1443172\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17400\tAverage Score (last 100): 48.97\tEpsilon: 0.0100\tTotal Steps: 1450138\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17500\tAverage Score (last 100): 52.33\tEpsilon: 0.0100\tTotal Steps: 1456590\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17600\tAverage Score (last 100): 51.07\tEpsilon: 0.0100\tTotal Steps: 1462927\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17700\tAverage Score (last 100): 51.95\tEpsilon: 0.0100\tTotal Steps: 1469611\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17800\tAverage Score (last 100): 43.13\tEpsilon: 0.0100\tTotal Steps: 1477217\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 17900\tAverage Score (last 100): 52.08\tEpsilon: 0.0100\tTotal Steps: 1483952\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18100\tAverage Score (last 100): 50.32\tEpsilon: 0.0100\tTotal Steps: 1497596\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18200\tAverage Score (last 100): 48.30\tEpsilon: 0.0100\tTotal Steps: 1504745\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18300\tAverage Score (last 100): 52.50\tEpsilon: 0.0100\tTotal Steps: 1511259\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18400\tAverage Score (last 100): 47.89\tEpsilon: 0.0100\tTotal Steps: 1518537\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18500\tAverage Score (last 100): 45.07\tEpsilon: 0.0100\tTotal Steps: 1526202\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18600\tAverage Score (last 100): 44.94\tEpsilon: 0.0100\tTotal Steps: 1533696\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18700\tAverage Score (last 100): 52.22\tEpsilon: 0.0100\tTotal Steps: 1540138\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18800\tAverage Score (last 100): 43.60\tEpsilon: 0.0100\tTotal Steps: 1547087\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 18900\tAverage Score (last 100): 46.77\tEpsilon: 0.0100\tTotal Steps: 1554521\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19000\tAverage Score (last 100): 44.86\tEpsilon: 0.0100\tTotal Steps: 1561898\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19100\tAverage Score (last 100): 46.78\tEpsilon: 0.0100\tTotal Steps: 1569782\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19200\tAverage Score (last 100): 46.64\tEpsilon: 0.0100\tTotal Steps: 1577452\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19300\tAverage Score (last 100): 45.57\tEpsilon: 0.0100\tTotal Steps: 1585060\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19400\tAverage Score (last 100): 47.80\tEpsilon: 0.0100\tTotal Steps: 1592505\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19500\tAverage Score (last 100): 50.84\tEpsilon: 0.0100\tTotal Steps: 1598957\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19600\tAverage Score (last 100): 46.72\tEpsilon: 0.0100\tTotal Steps: 1606371\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19700\tAverage Score (last 100): 50.97\tEpsilon: 0.0100\tTotal Steps: 1613312\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19800\tAverage Score (last 100): 50.18\tEpsilon: 0.0100\tTotal Steps: 1619923\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 19900\tAverage Score (last 100): 49.61\tEpsilon: 0.0100\tTotal Steps: 1627126\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20000\tAverage Score (last 100): 49.83\tEpsilon: 0.0100\tTotal Steps: 1633872\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20100\tAverage Score (last 100): 52.41\tEpsilon: 0.0100\tTotal Steps: 1640666\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20200\tAverage Score (last 100): 41.81\tEpsilon: 0.0100\tTotal Steps: 1648483\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20300\tAverage Score (last 100): 50.46\tEpsilon: 0.0100\tTotal Steps: 1655036\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20400\tAverage Score (last 100): 50.21\tEpsilon: 0.0100\tTotal Steps: 1661823\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20500\tAverage Score (last 100): 46.74\tEpsilon: 0.0100\tTotal Steps: 1669439\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20600\tAverage Score (last 100): 52.70\tEpsilon: 0.0100\tTotal Steps: 1675768\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20700\tAverage Score (last 100): 52.36\tEpsilon: 0.0100\tTotal Steps: 1682356\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20800\tAverage Score (last 100): 46.60\tEpsilon: 0.0100\tTotal Steps: 1690090\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 20900\tAverage Score (last 100): 52.98\tEpsilon: 0.0100\tTotal Steps: 1696665\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 21000\tAverage Score (last 100): 47.02\tEpsilon: 0.0100\tTotal Steps: 1704095\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 21100\tAverage Score (last 100): 44.52\tEpsilon: 0.0100\tTotal Steps: 1711500\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 21200\tAverage Score (last 100): 51.25\tEpsilon: 0.0100\tTotal Steps: 1718163\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 21300\tAverage Score (last 100): 49.35\tEpsilon: 0.0100\tTotal Steps: 1725276\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 21400\tAverage Score (last 100): 47.13\tEpsilon: 0.0100\tTotal Steps: 1732798\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 21500\tAverage Score (last 100): 45.77\tEpsilon: 0.0100\tTotal Steps: 1740076\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22200\tAverage Score (last 100): 44.83\tEpsilon: 0.0100\tTotal Steps: 1789865\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22300\tAverage Score (last 100): 47.29\tEpsilon: 0.0100\tTotal Steps: 1797562\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22400\tAverage Score (last 100): 47.11\tEpsilon: 0.0100\tTotal Steps: 1804921\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22500\tAverage Score (last 100): 46.59\tEpsilon: 0.0100\tTotal Steps: 1812298\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22600\tAverage Score (last 100): 41.63\tEpsilon: 0.0100\tTotal Steps: 1819905\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22700\tAverage Score (last 100): 53.92\tEpsilon: 0.0100\tTotal Steps: 1826024\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22800\tAverage Score (last 100): 40.58\tEpsilon: 0.0100\tTotal Steps: 1834292\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 22900\tAverage Score (last 100): 54.50\tEpsilon: 0.0100\tTotal Steps: 1840772\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23000\tAverage Score (last 100): 45.33\tEpsilon: 0.0100\tTotal Steps: 1848559\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23100\tAverage Score (last 100): 52.54\tEpsilon: 0.0100\tTotal Steps: 1855032\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23200\tAverage Score (last 100): 42.73\tEpsilon: 0.0100\tTotal Steps: 1862904\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23300\tAverage Score (last 100): 44.28\tEpsilon: 0.0100\tTotal Steps: 1870598\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23400\tAverage Score (last 100): 48.17\tEpsilon: 0.0100\tTotal Steps: 1877901\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23500\tAverage Score (last 100): 47.52\tEpsilon: 0.0100\tTotal Steps: 1885057\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23600\tAverage Score (last 100): 46.58\tEpsilon: 0.0100\tTotal Steps: 1892393\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23700\tAverage Score (last 100): 50.31\tEpsilon: 0.0100\tTotal Steps: 1899613\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23800\tAverage Score (last 100): 48.08\tEpsilon: 0.0100\tTotal Steps: 1907031\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 23900\tAverage Score (last 100): 52.52\tEpsilon: 0.0100\tTotal Steps: 1913683\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 24000\tAverage Score (last 100): 53.46\tEpsilon: 0.0100\tTotal Steps: 1920291\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 24100\tAverage Score (last 100): 54.73\tEpsilon: 0.0100\tTotal Steps: 1926803\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 27600\tAverage Score (last 100): 41.05\tEpsilon: 0.0100\tTotal Steps: 2180253\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 27700\tAverage Score (last 100): 41.80\tEpsilon: 0.0100\tTotal Steps: 2187991\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 27800\tAverage Score (last 100): 46.46\tEpsilon: 0.0100\tTotal Steps: 2195673\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 27900\tAverage Score (last 100): 42.39\tEpsilon: 0.0100\tTotal Steps: 2203733\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28000\tAverage Score (last 100): 44.55\tEpsilon: 0.0100\tTotal Steps: 2211377\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28100\tAverage Score (last 100): 45.37\tEpsilon: 0.0100\tTotal Steps: 2218899\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28200\tAverage Score (last 100): 46.23\tEpsilon: 0.0100\tTotal Steps: 2226471\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28300\tAverage Score (last 100): 47.93\tEpsilon: 0.0100\tTotal Steps: 2233716\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28400\tAverage Score (last 100): 48.05\tEpsilon: 0.0100\tTotal Steps: 2240411\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28500\tAverage Score (last 100): 46.17\tEpsilon: 0.0100\tTotal Steps: 2248031\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28600\tAverage Score (last 100): 47.01\tEpsilon: 0.0100\tTotal Steps: 2255170\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28700\tAverage Score (last 100): 43.46\tEpsilon: 0.0100\tTotal Steps: 2262876\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28800\tAverage Score (last 100): 46.64\tEpsilon: 0.0100\tTotal Steps: 2270275\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 28900\tAverage Score (last 100): 44.66\tEpsilon: 0.0100\tTotal Steps: 2277191\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29000\tAverage Score (last 100): 50.96\tEpsilon: 0.0100\tTotal Steps: 2283907\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29100\tAverage Score (last 100): 50.10\tEpsilon: 0.0100\tTotal Steps: 2290715\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29200\tAverage Score (last 100): 48.81\tEpsilon: 0.0100\tTotal Steps: 2297862\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29300\tAverage Score (last 100): 39.10\tEpsilon: 0.0100\tTotal Steps: 2306037\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29400\tAverage Score (last 100): 46.97\tEpsilon: 0.0100\tTotal Steps: 2313510\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29500\tAverage Score (last 100): 47.75\tEpsilon: 0.0100\tTotal Steps: 2320718\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29600\tAverage Score (last 100): 40.23\tEpsilon: 0.0100\tTotal Steps: 2328704\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29700\tAverage Score (last 100): 47.85\tEpsilon: 0.0100\tTotal Steps: 2336389\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29800\tAverage Score (last 100): 45.40\tEpsilon: 0.0100\tTotal Steps: 2343570\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 29900\tAverage Score (last 100): 40.57\tEpsilon: 0.0100\tTotal Steps: 2351841\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30000\tAverage Score (last 100): 49.78\tEpsilon: 0.0100\tTotal Steps: 2358920\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30100\tAverage Score (last 100): 48.39\tEpsilon: 0.0100\tTotal Steps: 2366359\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30200\tAverage Score (last 100): 47.79\tEpsilon: 0.0100\tTotal Steps: 2373746\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30300\tAverage Score (last 100): 46.17\tEpsilon: 0.0100\tTotal Steps: 2381386\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30400\tAverage Score (last 100): 35.09\tEpsilon: 0.0100\tTotal Steps: 2390000\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30500\tAverage Score (last 100): 45.59\tEpsilon: 0.0100\tTotal Steps: 2398495\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30600\tAverage Score (last 100): 47.04\tEpsilon: 0.0100\tTotal Steps: 2405829\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30700\tAverage Score (last 100): 45.47\tEpsilon: 0.0100\tTotal Steps: 2413308\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30800\tAverage Score (last 100): 45.50\tEpsilon: 0.0100\tTotal Steps: 2420942\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 30900\tAverage Score (last 100): 38.73\tEpsilon: 0.0100\tTotal Steps: 2428700\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31000\tAverage Score (last 100): 51.41\tEpsilon: 0.0100\tTotal Steps: 2436017\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31100\tAverage Score (last 100): 54.84\tEpsilon: 0.0100\tTotal Steps: 2442619\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31200\tAverage Score (last 100): 45.90\tEpsilon: 0.0100\tTotal Steps: 2450341\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31300\tAverage Score (last 100): 54.62\tEpsilon: 0.0100\tTotal Steps: 2456798\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31400\tAverage Score (last 100): 47.67\tEpsilon: 0.0100\tTotal Steps: 2463806\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31500\tAverage Score (last 100): 42.76\tEpsilon: 0.0100\tTotal Steps: 2471400\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31600\tAverage Score (last 100): 49.16\tEpsilon: 0.0100\tTotal Steps: 2478841\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31700\tAverage Score (last 100): 47.95\tEpsilon: 0.0100\tTotal Steps: 2486144\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31800\tAverage Score (last 100): 40.59\tEpsilon: 0.0100\tTotal Steps: 2493874\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 31900\tAverage Score (last 100): 43.60\tEpsilon: 0.0100\tTotal Steps: 2501398\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32000\tAverage Score (last 100): 40.43\tEpsilon: 0.0100\tTotal Steps: 2509913\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32100\tAverage Score (last 100): 46.68\tEpsilon: 0.0100\tTotal Steps: 2517509\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32200\tAverage Score (last 100): 49.04\tEpsilon: 0.0100\tTotal Steps: 2524994\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32300\tAverage Score (last 100): 45.28\tEpsilon: 0.0100\tTotal Steps: 2533095\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32400\tAverage Score (last 100): 49.87\tEpsilon: 0.0100\tTotal Steps: 2539598\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32500\tAverage Score (last 100): 46.06\tEpsilon: 0.0100\tTotal Steps: 2546783\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32600\tAverage Score (last 100): 42.26\tEpsilon: 0.0100\tTotal Steps: 2554716\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32700\tAverage Score (last 100): 47.62\tEpsilon: 0.0100\tTotal Steps: 2562447\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32800\tAverage Score (last 100): 52.33\tEpsilon: 0.0100\tTotal Steps: 2569419\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 32900\tAverage Score (last 100): 46.38\tEpsilon: 0.0100\tTotal Steps: 2576864\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33000\tAverage Score (last 100): 46.99\tEpsilon: 0.0100\tTotal Steps: 2584069\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33100\tAverage Score (last 100): 46.95\tEpsilon: 0.0100\tTotal Steps: 2591282\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33200\tAverage Score (last 100): 47.41\tEpsilon: 0.0100\tTotal Steps: 2598851\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33300\tAverage Score (last 100): 47.44\tEpsilon: 0.0100\tTotal Steps: 2606516\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33400\tAverage Score (last 100): 52.12\tEpsilon: 0.0100\tTotal Steps: 2613512\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33500\tAverage Score (last 100): 50.93\tEpsilon: 0.0100\tTotal Steps: 2620311\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33600\tAverage Score (last 100): 42.43\tEpsilon: 0.0100\tTotal Steps: 2628112\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33700\tAverage Score (last 100): 43.66\tEpsilon: 0.0100\tTotal Steps: 2635773\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33800\tAverage Score (last 100): 49.37\tEpsilon: 0.0100\tTotal Steps: 2643288\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 33900\tAverage Score (last 100): 44.58\tEpsilon: 0.0100\tTotal Steps: 2650748\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34000\tAverage Score (last 100): 42.53\tEpsilon: 0.0100\tTotal Steps: 2658618\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34100\tAverage Score (last 100): 47.50\tEpsilon: 0.0100\tTotal Steps: 2666141\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34200\tAverage Score (last 100): 45.09\tEpsilon: 0.0100\tTotal Steps: 2673392\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34300\tAverage Score (last 100): 42.09\tEpsilon: 0.0100\tTotal Steps: 2681319\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34400\tAverage Score (last 100): 39.86\tEpsilon: 0.0100\tTotal Steps: 2689412\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34500\tAverage Score (last 100): 46.54\tEpsilon: 0.0100\tTotal Steps: 2696986\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34600\tAverage Score (last 100): 48.51\tEpsilon: 0.0100\tTotal Steps: 2704904\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34700\tAverage Score (last 100): 46.03\tEpsilon: 0.0100\tTotal Steps: 2712066\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34800\tAverage Score (last 100): 44.90\tEpsilon: 0.0100\tTotal Steps: 2719948\n",
      "Model saved to agent_166k_eps.pth\n",
      "Episode 34852\tAverage Score (last 100): 41.36\tEpsilon: 0.0100\tTotal Steps: 2723759"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# --- Configuration (Adjust these as needed) ---\n",
    "# Environment specific (match your game)\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100 # Max steps in one round of the game\n",
    "\n",
    "# Neural Network Hyperparameters (from rl_agent_python_v1)\n",
    "INPUT_FEATURES = 288  # 7*5*8 (viewcone) + 4 (direction) + 2 (location) + 1 (scout) + 1 (step)\n",
    "HIDDEN_LAYER_1_SIZE = 256\n",
    "HIDDEN_LAYER_2_SIZE = 256\n",
    "OUTPUT_ACTIONS = 5  # 0:Forward, 1:Backward, 2:TurnL, 3:TurnR, 4:Stay\n",
    "\n",
    "# Training Hyperparameters\n",
    "BUFFER_SIZE = int(1e5)  # Replay buffer size\n",
    "BATCH_SIZE = 32         # Minibatch size for training\n",
    "GAMMA = 0.99            # Discount factor\n",
    "LEARNING_RATE = 1e-4    # Learning rate for the optimizer\n",
    "TARGET_UPDATE_EVERY = 100 # How often to update the target network (in steps)\n",
    "UPDATE_EVERY = 4        # How often to run a learning step (in steps)\n",
    "\n",
    "# Epsilon-greedy exploration parameters (for training)\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995 # Multiplicative decay factor per episode/fixed number of steps\n",
    "\n",
    "# PER Parameters\n",
    "PER_ALPHA = 0.6  # Prioritization exponent (0 for uniform, 1 for full prioritization)\n",
    "PER_BETA_START = 0.4 # Initial importance sampling exponent\n",
    "PER_BETA_FRAMES = int(1e5) # Number of frames over which beta is annealed to 1.0\n",
    "PER_EPSILON = 1e-6 # Small constant to ensure non-zero priority\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- SumTree for Prioritized Replay Buffer ---\n",
    "class SumTree:\n",
    "    \"\"\"\n",
    "    A SumTree is a binary tree data structure where the value of a parent node\n",
    "    is the sum of its children. It is used for efficient sampling from a\n",
    "    distribution. Leaf nodes store priorities, and internal nodes store sums.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1) # Tree storage\n",
    "        self.data = np.zeros(capacity, dtype=object) # Data storage (transitions)\n",
    "        self.data_pointer = 0 # Current position to write new data\n",
    "        self.n_entries = 0 # Current number of entries in the buffer\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        \"\"\"Add priority score and data to the tree.\"\"\"\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0 # Cycle back to the beginning\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        \"\"\"Update priority of a node and propagate changes upwards.\"\"\"\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        # Propagate the change up the tree\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        \"\"\"Find sample on leaf node based on a cumulative sum value.\"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree): # Reached leaf\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        \n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "# --- Prioritized Replay Buffer ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha # Controls how much prioritization is used (0=uniform, 1=full)\n",
    "        self.max_priority = 1.0 # Max priority for new experiences\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adds a new experience to the buffer with max priority.\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences from the buffer.\n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample.\n",
    "            beta (float): Importance-sampling exponent.\n",
    "        Returns:\n",
    "            tuple: (experiences, indices, weights)\n",
    "        \"\"\"\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "\n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = priority_segment * i\n",
    "            b = priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities, -beta) # (N * P(i))^-beta\n",
    "            batch_idx[i] = index\n",
    "            batch_data[i] = data\n",
    "        \n",
    "        weights /= weights.max() # Normalize for stability\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*[e for e in batch_data])\n",
    "\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        \"\"\"Updates the priorities of sampled experiences.\"\"\"\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON # Add epsilon to ensure non-zero priority\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        \n",
    "        for idx, priority in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority) # Update max_priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- Deep Q-Network (DQN) Model (same as in rl_agent_python_v1) ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.policy_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        self.target_net = DQN(INPUT_FEATURES, HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "                print(f\"Loaded pre-trained policy_net from {model_load_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model from {model_load_path}: {e}. Initializing with random weights.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            print(f\"No model path provided or path {model_load_path} does not exist. Initializing policy_net with random weights.\")\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict()) # Initialize target_net with policy_net weights\n",
    "        self.target_net.eval() # Target network is only for inference\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step = 0 # Counter for triggering learning and target network updates\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES\n",
    "\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value): # Same as in rl_agent_python_v1\n",
    "        tile_features = []\n",
    "        tile_features.append(float(tile_value & 0b01)) \n",
    "        tile_features.append(float((tile_value & 0b10) >> 1))\n",
    "        for i in range(2, 8):\n",
    "            tile_features.append(float((tile_value >> i) & 1))\n",
    "        return tile_features\n",
    "\n",
    "    def process_observation(self, observation_dict): # Same as in rl_agent_python_v1\n",
    "        processed_features = []\n",
    "        viewcone = observation_dict.get(\"viewcone\", [])\n",
    "        for r in range(7):\n",
    "            for c in range(5):\n",
    "                tile_value = viewcone[r][c] if r < len(viewcone) and c < len(viewcone[r]) else 0\n",
    "                processed_features.extend(self._unpack_viewcone_tile(tile_value))\n",
    "        \n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4\n",
    "        if 0 <= direction < 4: direction_one_hot[direction] = 1.0\n",
    "        processed_features.extend(direction_one_hot)\n",
    "\n",
    "        location = observation_dict.get(\"location\", [0, 0])\n",
    "        norm_x = location[0] / MAP_SIZE_X if MAP_SIZE_X > 0 else 0.0\n",
    "        norm_y = location[1] / MAP_SIZE_Y if MAP_SIZE_Y > 0 else 0.0\n",
    "        processed_features.extend([norm_x, norm_y])\n",
    "\n",
    "        scout_role = float(observation_dict.get(\"scout\", 0))\n",
    "        processed_features.append(scout_role)\n",
    "\n",
    "        step = observation_dict.get(\"step\", 0)\n",
    "        norm_step = step / MAX_STEPS_PER_EPISODE if MAX_STEPS_PER_EPISODE > 0 else 0.0\n",
    "        processed_features.append(norm_step)\n",
    "        \n",
    "        # Ensure correct feature length (should be INPUT_FEATURES)\n",
    "        if len(processed_features) != INPUT_FEATURES:\n",
    "            # This indicates an issue with feature processing or constants\n",
    "            raise ValueError(f\"Feature length mismatch. Expected {INPUT_FEATURES}, got {len(processed_features)}\")\n",
    "\n",
    "        return np.array(processed_features, dtype=np.float32) # Return as numpy array for buffer\n",
    "\n",
    "    def select_action(self, state_np, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        Args:\n",
    "            state_np (np.ndarray): Processed state as a numpy array.\n",
    "            epsilon (float): Exploration rate.\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state_tensor = torch.from_numpy(state_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval() # Set to evaluation mode for action selection\n",
    "            with torch.no_grad():\n",
    "                action_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train() # Set back to training mode\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "                self.learn(experiences, indices, weights, GAMMA)\n",
    "                # Anneal beta\n",
    "                self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "\n",
    "\n",
    "        # Update target network\n",
    "        if self.t_step % TARGET_UPDATE_EVERY == 0 : # A bit confusing, t_step is modulo UPDATE_EVERY\n",
    "                                                    # A global step counter might be better here.\n",
    "                                                    # Let's assume a global step counter is incremented elsewhere\n",
    "                                                    # and this check is done based on that.\n",
    "                                                    # For now, this will update target less frequently.\n",
    "            self.update_target_net()\n",
    "\n",
    "\n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * Q_target(s', argmax_a Q_policy(s', a))\n",
    "        Args:\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            indices (np.ndarray): indices of these experiences in the SumTree\n",
    "            importance_sampling_weights (torch.Tensor): weights for IS\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from policy network\n",
    "        # This is for Double DQN: action selection from policy_net, evaluation from target_net\n",
    "        q_next_policy = self.policy_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        # Get Q values for next_states from target_net using actions selected by policy_net\n",
    "        q_targets_next = self.target_net(next_states).detach().gather(1, q_next_policy)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from policy_net\n",
    "        q_expected = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        # Compute TD errors for PER\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        # Compute loss (element-wise multiplication with IS weights)\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0) # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def update_target_net(self):\n",
    "        \"\"\"Soft update model parameters: θ_target = τ*θ_local + (1 - τ)*θ_target\"\"\"\n",
    "        # For hard update:\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # print(\"Updated target network.\")\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "        print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_state(self): # For compatibility with potential stateful components, not used in this DQN\n",
    "        pass\n",
    "\n",
    "\n",
    "# --- Main Training Loop (Example) ---\n",
    "def train_agent(env_module, num_episodes=2000, novice_track=False, load_model_from=None, save_model_to=\"trained_dqn_agent.pth\"):\n",
    "    \"\"\"\n",
    "    Main training loop for the RL agent.\n",
    "    Args:\n",
    "        env_module: The imported environment module (e.g., til_environment.gridworld).\n",
    "        num_episodes (int): Number of episodes to train for.\n",
    "        novice_track (bool): If true, uses the novice environment settings.\n",
    "        load_model_from (str, optional): Path to load a pre-trained model.\n",
    "        save_model_to (str): Path to save the trained model.\n",
    "    \"\"\"\n",
    "    # Initialize your game environment\n",
    "    # This assumes your 'til_environment.gridworld' has an 'env' function\n",
    "    # that returns a PettingZoo-like environment.\n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "    \n",
    "    # Assuming your agent is always the first one in possible_agents\n",
    "    # Adjust if your setup is different or if you want to train a specific agent\n",
    "    my_agent_id = env.possible_agents[0] \n",
    "    print(f\"Training agent: {my_agent_id}\")\n",
    "    print(f\"Action space for {my_agent_id}: {env.action_space(my_agent_id)}\")\n",
    "    print(f\"Observation space for {my_agent_id}: {env.observation_space(my_agent_id)}\")\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    \n",
    "    scores_deque = deque(maxlen=100) # For tracking recent scores\n",
    "    scores = [] # List of scores from all episodes\n",
    "    epsilon = EPSILON_START\n",
    "    total_steps_taken = 0\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset() # Reset environment at the start of each episode\n",
    "        agent.reset_state() # Reset agent's internal state if any (not for this DQN)\n",
    "        \n",
    "        # The environment interaction loop from your test script\n",
    "        current_rewards_this_episode = {agent_id: 0 for agent_id in env.possible_agents}\n",
    "        \n",
    "        # Get initial observation for our agent\n",
    "        # This part needs careful handling with PettingZoo's agent_iter\n",
    "        # We need to get the first observation for our agent\n",
    "        \n",
    "        # The loop below processes all agents. We only train `my_agent_id`.\n",
    "        # We need to store the state for `my_agent_id` to pass to `agent.step`\n",
    "        \n",
    "        last_observation_for_my_agent = None\n",
    "        \n",
    "        for pet_agent_id in env.agent_iter(): # PettingZoo's iterator\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            # Accumulate rewards for all agents for this step\n",
    "            for ag_id in env.agents: # env.agents are live agents in current step\n",
    "                 current_rewards_this_episode[ag_id] += env.rewards.get(ag_id, 0)\n",
    "\n",
    "            done = termination or truncation\n",
    "\n",
    "            if done: # If an agent is done, it might not take an action\n",
    "                action = None # PettingZoo expects None if agent is done\n",
    "            elif pet_agent_id == my_agent_id:\n",
    "                # It's our agent's turn\n",
    "                # 1. Process observation\n",
    "                obs_dict = {k: v if isinstance(v, (int, float)) else v.tolist() for k, v in observation.items()}\n",
    "                current_state_np = agent.process_observation(obs_dict)\n",
    "                \n",
    "                # 2. Store previous transition if available\n",
    "                if last_observation_for_my_agent is not None:\n",
    "                    # last_observation_for_my_agent = (prev_state, prev_action, prev_reward_for_my_agent)\n",
    "                    prev_state_np, prev_action, prev_reward = last_observation_for_my_agent\n",
    "                    # The reward for the (s,a) pair is what we received *after* taking action 'a' in state 's'\n",
    "                    # which is the 'reward' variable from env.last() *now*\n",
    "                    agent.step(prev_state_np, prev_action, reward, current_state_np, done)\n",
    "                    total_steps_taken +=1\n",
    "\n",
    "                # 3. Select action\n",
    "                action = agent.select_action(current_state_np, epsilon)\n",
    "                \n",
    "                # 4. Store current state, action, and this step's reward for the *next* transition\n",
    "                last_observation_for_my_agent = (current_state_np, action, reward) # reward here is for the current (s,a)\n",
    "\n",
    "            else:\n",
    "                # Other agents take random actions (or use their own policies if implemented)\n",
    "                if env.action_space(pet_agent_id) is not None:\n",
    "                     action = env.action_space(pet_agent_id).sample()\n",
    "                else:\n",
    "                    action = None # Should not happen if agent is not done\n",
    "\n",
    "            env.step(action) # Step the environment with the chosen action (or None)\n",
    "            \n",
    "            if done and pet_agent_id == my_agent_id and last_observation_for_my_agent is not None:\n",
    "                # If our agent is done, we need to record the final transition\n",
    "                prev_state_np, prev_action, _ = last_observation_for_my_agent \n",
    "                # The final reward is `reward` from env.last() when done is true\n",
    "                # The next_state is not critical as it's a terminal state, can be zeros or current_state_np\n",
    "                final_next_state_np = np.zeros_like(prev_state_np) # Or current_state_np\n",
    "                agent.step(prev_state_np, prev_action, reward, final_next_state_np, True)\n",
    "                total_steps_taken +=1\n",
    "                last_observation_for_my_agent = None # Reset for next episode start\n",
    "\n",
    "        # End of episode\n",
    "        episode_score = current_rewards_this_episode[my_agent_id]\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        \n",
    "        epsilon = max(EPSILON_END, EPSILON_DECAY * epsilon) # Decay epsilon\n",
    "\n",
    "        print(f'\\rEpisode {i_episode}\\tAverage Score (last 100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tTotal Steps: {total_steps_taken}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEpisode {i_episode}\\tAverage Score (last 100): {np.mean(scores_deque):.2f}\\tEpsilon: {epsilon:.4f}\\tTotal Steps: {total_steps_taken}')\n",
    "            agent.save_model()\n",
    "        \n",
    "        if np.mean(scores_deque) >= 200.0: # Example condition to stop training\n",
    "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes!\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "            agent.save_model()\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    return scores\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- HOW TO USE ---\n",
    "    # 1. Make sure you have 'til_environment' and its 'gridworld' module accessible.\n",
    "    #    (e.g., it's in your PYTHONPATH or the same directory)\n",
    "    # 2. Install PyTorch: pip install torch\n",
    "    # 3. Run this script: python your_training_script_name.py\n",
    "    import time\n",
    "    start = time.time()\n",
    "    # Example:\n",
    "    try:\n",
    "        from til_environment import gridworld # Assuming this is your environment module\n",
    "        print(\"Successfully imported til_environment.gridworld\")\n",
    "        \n",
    "        # Start training\n",
    "        # Set load_model_from to a .pth file to continue training or fine-tune\n",
    "        # Set save_model_to to where you want the final model to be saved\n",
    "        trained_scores = train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=100000, # Adjust as needed\n",
    "            novice_track=False, # Or True for the Novice track map\n",
    "            load_model_from=\"agent_66k_eps.pth\", # \"trained_dqn_agent.pth\" to resume\n",
    "            save_model_to=\"agent_166k_eps.pth\"\n",
    "        )\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # You can add plotting for scores if you like:\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # plt.plot(np.arange(len(trained_scores)), trained_scores)\n",
    "        # plt.ylabel('Score')\n",
    "        # plt.xlabel('Episode #')\n",
    "        # plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'.\")\n",
    "        print(\"Please ensure the environment module is correctly set up and accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time taken = \", end-start)\n",
    "    print(f\"Time taken = {end-start:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
