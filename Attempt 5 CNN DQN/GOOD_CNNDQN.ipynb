{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57893f10-b574-49f5-940a-2f6c8f2e4a9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time # For timing training\n",
    "import imageio # For video export\n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "VIEWCONE_CHANNELS = 8\n",
    "VIEWCONE_HEIGHT = 7\n",
    "VIEWCONE_WIDTH = 5\n",
    "OTHER_FEATURES_SIZE = 4 + 2 + 1 + 1\n",
    "\n",
    "CNN_OUTPUT_CHANNELS_1 = 16\n",
    "CNN_OUTPUT_CHANNELS_2 = 32\n",
    "KERNEL_SIZE_1 = (3, 3)\n",
    "STRIDE_1 = 1\n",
    "KERNEL_SIZE_2 = (3, 3)\n",
    "STRIDE_2 = 1\n",
    "MLP_HIDDEN_LAYER_1_SIZE = 128\n",
    "MLP_HIDDEN_LAYER_2_SIZE = 128\n",
    "OUTPUT_ACTIONS = 5\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "TARGET_UPDATE_EVERY = 1000 # Global steps\n",
    "UPDATE_EVERY = 4 # Agent steps within an episode\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY_RATE = 0.999 # Applied per global step after min frames\n",
    "MIN_EPSILON_FRAMES = int(5e4)\n",
    "\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_FRAMES = int(1e5) # Global steps for beta annealing\n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VIDEO_OUTPUT_DIR = \"training_videos_cnn_dqn\" # Directory for videos\n",
    "\n",
    "# --- SumTree and PrioritizedReplayBuffer (largely unchanged) ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state_viewcone\", \"state_other\", \"action\", \"reward\", \"next_state_viewcone\", \"next_state_other\", \"done\"])\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        experience = Experience(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / batch_size if batch_size > 0 and self.tree.total_priority > 0 else 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta)\n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights /= (weights.max() if weights.max() > 0 else 1.0)\n",
    "\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = zip(*[e for e in batch_data])\n",
    "        states_viewcone = torch.from_numpy(np.array(states_viewcone)).float().to(DEVICE)\n",
    "        states_other = torch.from_numpy(np.array(states_other)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states_viewcone = torch.from_numpy(np.array(next_states_viewcone)).float().to(DEVICE)\n",
    "        next_states_other = torch.from_numpy(np.array(next_states_other)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority_val in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority_val)\n",
    "        if priorities.size > 0:\n",
    "            self.max_priority = max(self.max_priority, priorities.max())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- CNN-DQN Model ---\n",
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, viewcone_channels, viewcone_height, viewcone_width, other_features_size, mlp_hidden1, mlp_hidden2, num_actions, dropout_rate):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(viewcone_channels, CNN_OUTPUT_CHANNELS_1, kernel_size=KERNEL_SIZE_1, stride=STRIDE_1, padding=1)\n",
    "        self.relu_conv1 = nn.ReLU()\n",
    "        h_out1 = (viewcone_height + 2 * 1 - KERNEL_SIZE_1[0]) // STRIDE_1 + 1\n",
    "        w_out1 = (viewcone_width + 2 * 1 - KERNEL_SIZE_1[1]) // STRIDE_1 + 1\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(CNN_OUTPUT_CHANNELS_1, CNN_OUTPUT_CHANNELS_2, kernel_size=KERNEL_SIZE_2, stride=STRIDE_2, padding=1)\n",
    "        self.relu_conv2 = nn.ReLU()\n",
    "        h_out2 = (h_out1 + 2 * 1 - KERNEL_SIZE_2[0]) // STRIDE_2 + 1\n",
    "        w_out2 = (w_out1 + 2 * 1 - KERNEL_SIZE_2[1]) // STRIDE_2 + 1\n",
    "\n",
    "        self.cnn_output_flat_size = CNN_OUTPUT_CHANNELS_2 * h_out2 * w_out2\n",
    "        # print(f\"CNN output HxW: {h_out2}x{w_out2}, Flattened CNN output size: {self.cnn_output_flat_size}\")\n",
    "\n",
    "        self.fc1_mlp = nn.Linear(self.cnn_output_flat_size + other_features_size, mlp_hidden1)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2_mlp = nn.Linear(mlp_hidden1, mlp_hidden2)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc_output = nn.Linear(mlp_hidden2, num_actions)\n",
    "\n",
    "    def forward(self, viewcone_input, other_features_input):\n",
    "        x_cnn = self.relu_conv1(self.conv1(viewcone_input))\n",
    "        x_cnn = self.relu_conv2(self.conv2(x_cnn))\n",
    "        x_cnn_flat = x_cnn.view(-1, self.cnn_output_flat_size)\n",
    "        combined_features = torch.cat((x_cnn_flat, other_features_input), dim=1)\n",
    "        x = self.relu_fc1(self.fc1_mlp(combined_features))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu_fc2(self.fc2_mlp(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc_output(x)\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_cnn_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        self.target_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step_episode = 0 # For UPDATE_EVERY\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(VIEWCONE_CHANNELS)] # Assumes 8 channels\n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        raw_viewcone = observation_dict.get(\"viewcone\", np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8))\n",
    "        if not isinstance(raw_viewcone, np.ndarray): raw_viewcone = np.array(raw_viewcone)\n",
    "        if raw_viewcone.shape != (VIEWCONE_HEIGHT, VIEWCONE_WIDTH):\n",
    "            # print(f\"Warning: Viewcone shape mismatch. Expected ({VIEWCONE_HEIGHT},{VIEWCONE_WIDTH}), got {raw_viewcone.shape}. Using zeros.\")\n",
    "            padded_viewcone = np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8)\n",
    "            h, w = raw_viewcone.shape\n",
    "            h_min, w_min = min(h, VIEWCONE_HEIGHT), min(w, VIEWCONE_WIDTH)\n",
    "            padded_viewcone[:h_min, :w_min] = raw_viewcone[:h_min, :w_min]\n",
    "            raw_viewcone = padded_viewcone\n",
    "\n",
    "        processed_viewcone_channels_data = np.zeros((VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.float32)\n",
    "        for r in range(VIEWCONE_HEIGHT):\n",
    "            for c in range(VIEWCONE_WIDTH):\n",
    "                tile_value = raw_viewcone[r, c]\n",
    "                unpacked_features = self._unpack_viewcone_tile(tile_value)\n",
    "                for channel_idx in range(VIEWCONE_CHANNELS):\n",
    "                    processed_viewcone_channels_data[channel_idx, r, c] = unpacked_features[channel_idx]\n",
    "        \n",
    "        other_features_list = []\n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4; direction_one_hot[direction % 4] = 1.0\n",
    "        other_features_list.extend(direction_one_hot)\n",
    "        location = observation_dict.get(\"location\", [0,0]); norm_x = location[0]/MAP_SIZE_X; norm_y = location[1]/MAP_SIZE_Y\n",
    "        other_features_list.extend([norm_x, norm_y])\n",
    "        other_features_list.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        other_features_list.append(observation_dict.get(\"step\", 0)/MAX_STEPS_PER_EPISODE)\n",
    "        state_other_np = np.array(other_features_list, dtype=np.float32)\n",
    "        \n",
    "        return processed_viewcone_channels_data, state_other_np\n",
    "\n",
    "    def select_action(self, state_viewcone_np, state_other_np, epsilon=0.0):\n",
    "        if random.random() > epsilon:\n",
    "            state_viewcone_tensor = torch.from_numpy(state_viewcone_np).float().unsqueeze(0).to(self.device)\n",
    "            state_other_tensor = torch.from_numpy(state_other_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): action_values = self.policy_net(state_viewcone_tensor, state_other_tensor)\n",
    "            self.policy_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        self.memory.add(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.t_step_episode = (self.t_step_episode + 1) % UPDATE_EVERY\n",
    "        if self.t_step_episode == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "    \n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = experiences\n",
    "        q_next_actions_policy = self.policy_net(next_states_viewcone, next_states_other).detach().max(1)[1].unsqueeze(1)\n",
    "        q_targets_next = self.target_net(next_states_viewcone, next_states_other).detach().gather(1, q_next_actions_policy)\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.policy_net(states_viewcone, states_other).gather(1, actions)\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        self.optimizer.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_save_path:\n",
    "            torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "            print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_episode_counters(self): self.t_step_episode = 0\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_agent(env_module, num_episodes=10000, novice_track=False, load_model_from=None, save_model_to=\"trained_cnn_dqn_agent.pth\", video_export_every_n_episodes=100):\n",
    "    print(f\"Starting CNN DQN training: {num_episodes} episodes, Novice: {novice_track}, Video every: {video_export_every_n_episodes} episodes\")\n",
    "    print(f\"Load: {load_model_from}, Save: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    if video_export_every_n_episodes > 0 and not os.path.exists(VIDEO_OUTPUT_DIR):\n",
    "        os.makedirs(VIDEO_OUTPUT_DIR)\n",
    "        print(f\"Created video output directory: {VIDEO_OUTPUT_DIR}\")\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    epsilon = EPSILON_START\n",
    "    global_total_steps = 0\n",
    "    \n",
    "    # Initialize env first time without render mode for performance\n",
    "    current_render_mode = None\n",
    "    env = env_module.env(env_wrappers=[], render_mode=current_render_mode, novice=novice_track)\n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\"\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        episode_frames = []\n",
    "        record_video_this_episode = (video_export_every_n_episodes > 0 and i_episode % video_export_every_n_episodes == 0)\n",
    "\n",
    "        if record_video_this_episode and current_render_mode != \"rgb_array\":\n",
    "            env.close() # Close existing env\n",
    "            env = env_module.env(env_wrappers=[], render_mode=\"rgb_array\", novice=novice_track)\n",
    "            current_render_mode = \"rgb_array\"\n",
    "            # print(f\"Episode {i_episode}: Switched to rgb_array for video.\")\n",
    "        elif not record_video_this_episode and current_render_mode == \"rgb_array\":\n",
    "            env.close()\n",
    "            env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "            current_render_mode = None\n",
    "            # print(f\"Episode {i_episode}: Switched to render_mode=None.\")\n",
    "\n",
    "        env.reset() \n",
    "        agent.reset_episode_counters()\n",
    "        current_episode_rewards = {id: 0.0 for id in env.possible_agents}\n",
    "        last_processed_exp_my_agent = {} # Stores (prev_s_vc, prev_s_other, prev_a)\n",
    "        \n",
    "        for pet_agent_id_turn in env.agent_iter():\n",
    "            obs_raw, _, termination, truncation, info = env.last()\n",
    "            reward_for_last_action = env.rewards.get(my_agent_id, 0.0) # Reward for my_agent's last action\n",
    "            \n",
    "            for r_ag_id, r_val in env.rewards.items(): # Accumulate all rewards for episode score\n",
    "                if r_ag_id in current_episode_rewards: current_episode_rewards[r_ag_id] += r_val\n",
    "            \n",
    "            done = termination or truncation\n",
    "            action_to_take = None\n",
    "\n",
    "            if pet_agent_id_turn == my_agent_id:\n",
    "                obs_dict_current = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in obs_raw.items()}\n",
    "                current_s_vc, current_s_other = agent.process_observation(obs_dict_current)\n",
    "\n",
    "                if my_agent_id in last_processed_exp_my_agent:\n",
    "                    prev_s_vc, prev_s_other, prev_a = last_processed_exp_my_agent.pop(my_agent_id)\n",
    "                    agent.step(prev_s_vc, prev_s_other, prev_a, reward_for_last_action, current_s_vc, current_s_other, done)\n",
    "                \n",
    "                if done:\n",
    "                    action_to_take = None\n",
    "                else:\n",
    "                    action_to_take = agent.select_action(current_s_vc, current_s_other, epsilon)\n",
    "                    last_processed_exp_my_agent[my_agent_id] = (current_s_vc, current_s_other, action_to_take)\n",
    "                \n",
    "                global_total_steps += 1\n",
    "                if global_total_steps > MIN_EPSILON_FRAMES: epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY_RATE)\n",
    "                if global_total_steps % TARGET_UPDATE_EVERY == 0 and global_total_steps > 0: agent.update_target_net()\n",
    "\n",
    "            elif not done and env.action_space(pet_agent_id_turn) is not None:\n",
    "                action_to_take = env.action_space(pet_agent_id_turn).sample()\n",
    "            \n",
    "            env.step(action_to_take)\n",
    "            if record_video_this_episode and current_render_mode == \"rgb_array\": # Add frame after step\n",
    "                frame = env.render()\n",
    "                if frame is not None: episode_frames.append(frame)\n",
    "        \n",
    "        # End of episode\n",
    "        if record_video_this_episode and episode_frames:\n",
    "            video_path = os.path.join(VIDEO_OUTPUT_DIR, f\"episode_{i_episode:05d}.mp4\")\n",
    "            try:\n",
    "                imageio.mimsave(video_path, episode_frames, fps=10)\n",
    "                # print(f\"Saved video: {video_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving video for episode {i_episode}: {e}\")\n",
    "\n",
    "        episode_score = current_episode_rewards.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score); scores.append(episode_score)\n",
    "\n",
    "        if i_episode % 20 == 0:\n",
    "            print(f'\\rEp {i_episode}\\tAvgScore(100): {np.mean(scores_deque):.2f}\\tEps: {epsilon:.4f}\\tGlobalSteps: {global_total_steps}', end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'\\rEp {i_episode}\\tAvgScore(100): {np.mean(scores_deque):.2f}\\tEps: {epsilon:.4f}\\tGlobalSteps: {global_total_steps}')\n",
    "            if save_model_to: agent.save_model()\n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model()\n",
    "    print(f\"\\nCNN DQN Training finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "    return scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    print(f\"Initiating CNN DQN training at {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(training_start_time))} UTC\")\n",
    "    try:\n",
    "        from til_environment import gridworld \n",
    "        \n",
    "        NUM_TRAIN_EPISODES = 10000 # Matched to MLP script's typical setting\n",
    "        VIDEO_EVERY = 200 # Export video every 200 episodes (can be adjusted)\n",
    "\n",
    "        trained_scores = train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=NUM_TRAIN_EPISODES,\n",
    "            novice_track=False, # Or False for varied maps\n",
    "            load_model_from=None, # \"my_wargame_cnn_agent.pth\" to resume\n",
    "            save_model_to=\"my_wargame_cnn_agent_final.pth\",\n",
    "            video_export_every_n_episodes=VIDEO_EVERY\n",
    "        )\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during CNN DQN training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    total_time_seconds = time.time() - training_start_time\n",
    "    print(f\"Total CNN DQN training time: {total_time_seconds:.2f} seconds ({total_time_seconds/60:.2f} minutes or {total_time_seconds/3600:.2f} hours).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b846f-1c77-42ab-9a98-868b93080710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Overnight Optimized CNN DQN training at 2025-05-24 17:47:14 UTC\n",
      "Starting CNN DQN training: 100000 episodes, Novice: False\n",
      "Attempting to load model from: my_wargame_cnn_agent_best_100k.pth\n",
      "Models will be saved to: my_wargame_cnn_agent_cont_100k.pth\n",
      "Using device: cuda\n",
      "Epsilon will start at 0.5000 and decay towards 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_352124/2245522371.py:202: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 100\tAvgScore(100): 30.98\tEps: 0.5000\tGlobalSteps: 8712\tBeta: 0.4127\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 200\tAvgScore(100): 37.93\tEps: 0.2514\tGlobalSteps: 16876\tBeta: 0.4248\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 300\tAvgScore(100): 48.14\tEps: 0.1201\tGlobalSteps: 24264\tBeta: 0.4356\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 400\tAvgScore(100): 39.77\tEps: 0.0573\tGlobalSteps: 31655\tBeta: 0.4464\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 500\tAvgScore(100): 52.78\tEps: 0.0295\tGlobalSteps: 38284\tBeta: 0.4561\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 600\tAvgScore(100): 46.33\tEps: 0.0145\tGlobalSteps: 45429\tBeta: 0.4666\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 700\tAvgScore(100): 44.66\tEps: 0.0100\tGlobalSteps: 52710\tBeta: 0.4772\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 800\tAvgScore(100): 46.29\tEps: 0.0100\tGlobalSteps: 60085\tBeta: 0.4881\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 900\tAvgScore(100): 45.99\tEps: 0.0100\tGlobalSteps: 67597\tBeta: 0.4991\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1000\tAvgScore(100): 47.01\tEps: 0.0100\tGlobalSteps: 74359\tBeta: 0.5090\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1100\tAvgScore(100): 48.95\tEps: 0.0100\tGlobalSteps: 81001\tBeta: 0.5187\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1200\tAvgScore(100): 42.84\tEps: 0.0100\tGlobalSteps: 88550\tBeta: 0.5298\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1300\tAvgScore(100): 50.55\tEps: 0.0100\tGlobalSteps: 95756\tBeta: 0.5404\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1400\tAvgScore(100): 53.26\tEps: 0.0100\tGlobalSteps: 101633\tBeta: 0.5489\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1500\tAvgScore(100): 49.29\tEps: 0.0100\tGlobalSteps: 108997\tBeta: 0.5597\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1600\tAvgScore(100): 45.88\tEps: 0.0100\tGlobalSteps: 116271\tBeta: 0.5704\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1700\tAvgScore(100): 53.81\tEps: 0.0100\tGlobalSteps: 122843\tBeta: 0.5800\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1800\tAvgScore(100): 54.94\tEps: 0.0100\tGlobalSteps: 129214\tBeta: 0.5893\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 1900\tAvgScore(100): 50.83\tEps: 0.0100\tGlobalSteps: 136194\tBeta: 0.5995\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2000\tAvgScore(100): 51.09\tEps: 0.0100\tGlobalSteps: 142736\tBeta: 0.6091\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2100\tAvgScore(100): 50.67\tEps: 0.0100\tGlobalSteps: 149278\tBeta: 0.6186\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2200\tAvgScore(100): 53.03\tEps: 0.0100\tGlobalSteps: 156104\tBeta: 0.6286\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2300\tAvgScore(100): 54.74\tEps: 0.0100\tGlobalSteps: 162798\tBeta: 0.6383\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2400\tAvgScore(100): 49.53\tEps: 0.0100\tGlobalSteps: 169979\tBeta: 0.6489\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2500\tAvgScore(100): 54.14\tEps: 0.0100\tGlobalSteps: 176605\tBeta: 0.6585\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2600\tAvgScore(100): 46.19\tEps: 0.0100\tGlobalSteps: 184513\tBeta: 0.6702\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2700\tAvgScore(100): 51.50\tEps: 0.0100\tGlobalSteps: 191896\tBeta: 0.6810\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2800\tAvgScore(100): 60.56\tEps: 0.0100\tGlobalSteps: 197116\tBeta: 0.6885\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 2900\tAvgScore(100): 49.02\tEps: 0.0100\tGlobalSteps: 204209\tBeta: 0.6989\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3000\tAvgScore(100): 49.53\tEps: 0.0100\tGlobalSteps: 211097\tBeta: 0.7090\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3100\tAvgScore(100): 55.36\tEps: 0.0100\tGlobalSteps: 217333\tBeta: 0.7181\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3200\tAvgScore(100): 56.19\tEps: 0.0100\tGlobalSteps: 223869\tBeta: 0.7276\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3300\tAvgScore(100): 54.22\tEps: 0.0100\tGlobalSteps: 230421\tBeta: 0.7371\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3400\tAvgScore(100): 58.53\tEps: 0.0100\tGlobalSteps: 236273\tBeta: 0.7456\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3500\tAvgScore(100): 52.79\tEps: 0.0100\tGlobalSteps: 243075\tBeta: 0.7555\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3600\tAvgScore(100): 52.09\tEps: 0.0100\tGlobalSteps: 250095\tBeta: 0.7658\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3700\tAvgScore(100): 54.60\tEps: 0.0100\tGlobalSteps: 256430\tBeta: 0.7750\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3800\tAvgScore(100): 53.23\tEps: 0.0100\tGlobalSteps: 263260\tBeta: 0.7850\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 3900\tAvgScore(100): 51.83\tEps: 0.0100\tGlobalSteps: 270389\tBeta: 0.7954\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4000\tAvgScore(100): 58.10\tEps: 0.0100\tGlobalSteps: 276554\tBeta: 0.8044\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4100\tAvgScore(100): 49.78\tEps: 0.0100\tGlobalSteps: 283935\tBeta: 0.8152\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4200\tAvgScore(100): 53.33\tEps: 0.0100\tGlobalSteps: 290323\tBeta: 0.8245\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4300\tAvgScore(100): 50.29\tEps: 0.0100\tGlobalSteps: 297531\tBeta: 0.8350\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4400\tAvgScore(100): 49.10\tEps: 0.0100\tGlobalSteps: 305238\tBeta: 0.8463\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4500\tAvgScore(100): 57.03\tEps: 0.0100\tGlobalSteps: 311286\tBeta: 0.8551\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4600\tAvgScore(100): 52.33\tEps: 0.0100\tGlobalSteps: 318632\tBeta: 0.8659\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4700\tAvgScore(100): 53.19\tEps: 0.0100\tGlobalSteps: 326137\tBeta: 0.8769\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4800\tAvgScore(100): 58.56\tEps: 0.0100\tGlobalSteps: 332425\tBeta: 0.8860\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 4900\tAvgScore(100): 51.98\tEps: 0.0100\tGlobalSteps: 339883\tBeta: 0.8970\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5000\tAvgScore(100): 55.04\tEps: 0.0100\tGlobalSteps: 346293\tBeta: 0.9063\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5100\tAvgScore(100): 58.08\tEps: 0.0100\tGlobalSteps: 352626\tBeta: 0.9155\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5200\tAvgScore(100): 60.66\tEps: 0.0100\tGlobalSteps: 358760\tBeta: 0.9244\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5300\tAvgScore(100): 56.45\tEps: 0.0100\tGlobalSteps: 365414\tBeta: 0.9341\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5400\tAvgScore(100): 54.98\tEps: 0.0100\tGlobalSteps: 371939\tBeta: 0.9437\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5500\tAvgScore(100): 50.71\tEps: 0.0100\tGlobalSteps: 379115\tBeta: 0.9542\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5600\tAvgScore(100): 53.85\tEps: 0.0100\tGlobalSteps: 385633\tBeta: 0.9637\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5700\tAvgScore(100): 54.39\tEps: 0.0100\tGlobalSteps: 392257\tBeta: 0.9733\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5800\tAvgScore(100): 51.91\tEps: 0.0100\tGlobalSteps: 399399\tBeta: 0.9838\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 5900\tAvgScore(100): 55.46\tEps: 0.0100\tGlobalSteps: 405525\tBeta: 0.9927\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6000\tAvgScore(100): 56.27\tEps: 0.0100\tGlobalSteps: 412394\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6100\tAvgScore(100): 46.92\tEps: 0.0100\tGlobalSteps: 420072\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6200\tAvgScore(100): 56.59\tEps: 0.0100\tGlobalSteps: 426522\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6300\tAvgScore(100): 54.04\tEps: 0.0100\tGlobalSteps: 433314\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6400\tAvgScore(100): 57.33\tEps: 0.0100\tGlobalSteps: 440258\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6500\tAvgScore(100): 54.85\tEps: 0.0100\tGlobalSteps: 446677\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6600\tAvgScore(100): 54.19\tEps: 0.0100\tGlobalSteps: 453729\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6700\tAvgScore(100): 56.24\tEps: 0.0100\tGlobalSteps: 460525\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6800\tAvgScore(100): 50.94\tEps: 0.0100\tGlobalSteps: 467593\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 6900\tAvgScore(100): 60.55\tEps: 0.0100\tGlobalSteps: 473750\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7000\tAvgScore(100): 56.56\tEps: 0.0100\tGlobalSteps: 479718\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7100\tAvgScore(100): 48.60\tEps: 0.0100\tGlobalSteps: 486823\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7200\tAvgScore(100): 52.08\tEps: 0.0100\tGlobalSteps: 493888\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7300\tAvgScore(100): 57.62\tEps: 0.0100\tGlobalSteps: 500318\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7400\tAvgScore(100): 54.88\tEps: 0.0100\tGlobalSteps: 507011\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7500\tAvgScore(100): 60.14\tEps: 0.0100\tGlobalSteps: 513210\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7600\tAvgScore(100): 50.90\tEps: 0.0100\tGlobalSteps: 520403\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7700\tAvgScore(100): 53.37\tEps: 0.0100\tGlobalSteps: 527516\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7800\tAvgScore(100): 53.06\tEps: 0.0100\tGlobalSteps: 534381\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 7900\tAvgScore(100): 54.93\tEps: 0.0100\tGlobalSteps: 541196\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8000\tAvgScore(100): 58.60\tEps: 0.0100\tGlobalSteps: 547127\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8100\tAvgScore(100): 57.41\tEps: 0.0100\tGlobalSteps: 553381\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8200\tAvgScore(100): 56.60\tEps: 0.0100\tGlobalSteps: 559742\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8300\tAvgScore(100): 52.11\tEps: 0.0100\tGlobalSteps: 566117\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8400\tAvgScore(100): 59.11\tEps: 0.0100\tGlobalSteps: 572733\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8500\tAvgScore(100): 51.29\tEps: 0.0100\tGlobalSteps: 579794\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8600\tAvgScore(100): 58.90\tEps: 0.0100\tGlobalSteps: 585795\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8700\tAvgScore(100): 50.92\tEps: 0.0100\tGlobalSteps: 592927\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8800\tAvgScore(100): 50.38\tEps: 0.0100\tGlobalSteps: 600086\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 8900\tAvgScore(100): 56.71\tEps: 0.0100\tGlobalSteps: 606703\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9000\tAvgScore(100): 53.19\tEps: 0.0100\tGlobalSteps: 613389\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9100\tAvgScore(100): 56.76\tEps: 0.0100\tGlobalSteps: 619714\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9200\tAvgScore(100): 58.22\tEps: 0.0100\tGlobalSteps: 625744\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9300\tAvgScore(100): 53.91\tEps: 0.0100\tGlobalSteps: 632351\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9400\tAvgScore(100): 54.52\tEps: 0.0100\tGlobalSteps: 638835\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9500\tAvgScore(100): 61.20\tEps: 0.0100\tGlobalSteps: 644573\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9600\tAvgScore(100): 62.71\tEps: 0.0100\tGlobalSteps: 651058\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9700\tAvgScore(100): 57.40\tEps: 0.0100\tGlobalSteps: 657486\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9800\tAvgScore(100): 49.65\tEps: 0.0100\tGlobalSteps: 665022\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 9900\tAvgScore(100): 53.15\tEps: 0.0100\tGlobalSteps: 671777\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10000\tAvgScore(100): 54.36\tEps: 0.0100\tGlobalSteps: 678167\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10100\tAvgScore(100): 55.30\tEps: 0.0100\tGlobalSteps: 684500\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10200\tAvgScore(100): 59.77\tEps: 0.0100\tGlobalSteps: 690110\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10300\tAvgScore(100): 49.13\tEps: 0.0100\tGlobalSteps: 697074\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10400\tAvgScore(100): 58.85\tEps: 0.0100\tGlobalSteps: 703064\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10500\tAvgScore(100): 49.68\tEps: 0.0100\tGlobalSteps: 710252\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10600\tAvgScore(100): 52.22\tEps: 0.0100\tGlobalSteps: 717454\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10700\tAvgScore(100): 49.55\tEps: 0.0100\tGlobalSteps: 724312\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10800\tAvgScore(100): 53.64\tEps: 0.0100\tGlobalSteps: 730650\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 10900\tAvgScore(100): 57.40\tEps: 0.0100\tGlobalSteps: 737436\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11000\tAvgScore(100): 56.92\tEps: 0.0100\tGlobalSteps: 743434\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11100\tAvgScore(100): 53.04\tEps: 0.0100\tGlobalSteps: 750251\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11200\tAvgScore(100): 56.34\tEps: 0.0100\tGlobalSteps: 756375\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11300\tAvgScore(100): 54.78\tEps: 0.0100\tGlobalSteps: 762779\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11400\tAvgScore(100): 59.15\tEps: 0.0100\tGlobalSteps: 769032\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11500\tAvgScore(100): 52.48\tEps: 0.0100\tGlobalSteps: 776083\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11600\tAvgScore(100): 56.02\tEps: 0.0100\tGlobalSteps: 782480\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11700\tAvgScore(100): 46.91\tEps: 0.0100\tGlobalSteps: 789512\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11800\tAvgScore(100): 48.87\tEps: 0.0100\tGlobalSteps: 797318\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 11900\tAvgScore(100): 49.06\tEps: 0.0100\tGlobalSteps: 804253\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 12000\tAvgScore(100): 51.95\tEps: 0.0100\tGlobalSteps: 811317\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 12100\tAvgScore(100): 58.21\tEps: 0.0100\tGlobalSteps: 817647\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 12200\tAvgScore(100): 52.64\tEps: 0.0100\tGlobalSteps: 824769\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 12300\tAvgScore(100): 54.46\tEps: 0.0100\tGlobalSteps: 831073\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n",
      "Ep 12400\tAvgScore(100): 52.34\tEps: 0.0100\tGlobalSteps: 838215\tBeta: 1.0000\n",
      "Model saved to my_wargame_cnn_agent_cont_100k.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "MAP_SIZE_X = 16\n",
    "MAP_SIZE_Y = 16\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "VIEWCONE_CHANNELS = 8\n",
    "VIEWCONE_HEIGHT = 7\n",
    "VIEWCONE_WIDTH = 5\n",
    "OTHER_FEATURES_SIZE = 4 + 2 + 1 + 1\n",
    "\n",
    "CNN_OUTPUT_CHANNELS_1 = 16\n",
    "CNN_OUTPUT_CHANNELS_2 = 32\n",
    "KERNEL_SIZE_1 = (3, 3)\n",
    "STRIDE_1 = 1\n",
    "KERNEL_SIZE_2 = (3, 3)\n",
    "STRIDE_2 = 1\n",
    "MLP_HIDDEN_LAYER_1_SIZE = 128\n",
    "MLP_HIDDEN_LAYER_2_SIZE = 128\n",
    "OUTPUT_ACTIONS = 5\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BUFFER_SIZE = int(1e5) # Consider increasing if memory allows for very long runs\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4 # Could consider a slightly smaller LR for very long fine-tuning, e.g., 5e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "TARGET_UPDATE_EVERY = 1000 # Global steps\n",
    "UPDATE_EVERY = 4 # Agent steps within an episode\n",
    "\n",
    "# Epsilon settings for long overnight training, resuming from 0.05\n",
    "EPSILON_START = 0.5      # Start at the previous end value\n",
    "EPSILON_END = 0.01        # Target lower epsilon for more exploitation\n",
    "EPSILON_DECAY_RATE = 0.9999 # Slower decay for a very long run to reach 0.01 gradually\n",
    "MIN_EPSILON_FRAMES = int(1e4) # Number of global steps before decay significantly takes hold or starts.\n",
    "                              # Adjust based on how quickly you want it to drop from 0.05.\n",
    "                              # With 100k episodes (potentially millions of steps), decay will be gradual.\n",
    "\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4 # Could also consider loading saved beta and global_step from a checkpoint if you had one\n",
    "PER_BETA_FRAMES = int(1e5) # Global steps for beta annealing\n",
    "PER_EPSILON = 1e-6\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ... (SumTree, PrioritizedReplayBuffer, CNNDQN, TrainableRLAgent class definitions remain the same as in train_cnn_dqn_optimized_resumed_advanced.py) ...\n",
    "# Make sure to copy the full class definitions from the previous complete script.\n",
    "# For brevity, I'm omitting them here, but they are essential.\n",
    "\n",
    "# --- SumTree and PrioritizedReplayBuffer (largely unchanged) ---\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state_viewcone\", \"state_other\", \"action\", \"reward\", \"next_state_viewcone\", \"next_state_other\", \"done\"])\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if value <= self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    value -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=PER_ALPHA):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        experience = Experience(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.tree.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=PER_BETA_START):\n",
    "        batch_idx = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_data = np.empty(batch_size, dtype=object)\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "        priority_segment = self.tree.total_priority / batch_size if batch_size > 0 and self.tree.total_priority > 0 else 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            sampling_probabilities = priority / self.tree.total_priority if self.tree.total_priority > 0 else 0\n",
    "            weights[i] = np.power(self.tree.n_entries * sampling_probabilities + 1e-8, -beta)\n",
    "            batch_idx[i], batch_data[i] = index, data\n",
    "        \n",
    "        weights /= (weights.max() if weights.max() > 0 else 1.0)\n",
    "\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = zip(*[e for e in batch_data])\n",
    "        states_viewcone = torch.from_numpy(np.array(states_viewcone)).float().to(DEVICE)\n",
    "        states_other = torch.from_numpy(np.array(states_other)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(DEVICE)\n",
    "        next_states_viewcone = torch.from_numpy(np.array(next_states_viewcone)).float().to(DEVICE)\n",
    "        next_states_other = torch.from_numpy(np.array(next_states_other)).float().to(DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(DEVICE)\n",
    "        \n",
    "        return (states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones), batch_idx, torch.from_numpy(weights).float().to(DEVICE)\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        priorities = np.abs(td_errors) + PER_EPSILON\n",
    "        priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority_val in zip(batch_indices, priorities):\n",
    "            self.tree.update(idx, priority_val)\n",
    "        if priorities.size > 0:\n",
    "            self.max_priority = max(self.max_priority, priorities.max())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "# --- CNN-DQN Model ---\n",
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, viewcone_channels, viewcone_height, viewcone_width, other_features_size, mlp_hidden1, mlp_hidden2, num_actions, dropout_rate):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(viewcone_channels, CNN_OUTPUT_CHANNELS_1, kernel_size=KERNEL_SIZE_1, stride=STRIDE_1, padding=1)\n",
    "        self.relu_conv1 = nn.ReLU()\n",
    "        h_out1 = (viewcone_height + 2 * 1 - KERNEL_SIZE_1[0]) // STRIDE_1 + 1\n",
    "        w_out1 = (viewcone_width + 2 * 1 - KERNEL_SIZE_1[1]) // STRIDE_1 + 1\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(CNN_OUTPUT_CHANNELS_1, CNN_OUTPUT_CHANNELS_2, kernel_size=KERNEL_SIZE_2, stride=STRIDE_2, padding=1)\n",
    "        self.relu_conv2 = nn.ReLU()\n",
    "        h_out2 = (h_out1 + 2 * 1 - KERNEL_SIZE_2[0]) // STRIDE_2 + 1\n",
    "        w_out2 = (w_out1 + 2 * 1 - KERNEL_SIZE_2[1]) // STRIDE_2 + 1\n",
    "\n",
    "        self.cnn_output_flat_size = CNN_OUTPUT_CHANNELS_2 * h_out2 * w_out2\n",
    "        # print(f\"CNN output HxW: {h_out2}x{w_out2}, Flattened CNN output size: {self.cnn_output_flat_size}\")\n",
    "\n",
    "        self.fc1_mlp = nn.Linear(self.cnn_output_flat_size + other_features_size, mlp_hidden1)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2_mlp = nn.Linear(mlp_hidden1, mlp_hidden2)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc_output = nn.Linear(mlp_hidden2, num_actions)\n",
    "\n",
    "    def forward(self, viewcone_input, other_features_input):\n",
    "        x_cnn = self.relu_conv1(self.conv1(viewcone_input))\n",
    "        x_cnn = self.relu_conv2(self.conv2(x_cnn))\n",
    "        x_cnn_flat = x_cnn.view(-1, self.cnn_output_flat_size)\n",
    "        combined_features = torch.cat((x_cnn_flat, other_features_input), dim=1)\n",
    "        x = self.relu_fc1(self.fc1_mlp(combined_features))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu_fc2(self.fc2_mlp(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc_output(x)\n",
    "\n",
    "# --- Trainable RL Agent ---\n",
    "class TrainableRLAgent:\n",
    "    def __init__(self, model_load_path=None, model_save_path=\"trained_cnn_dqn_model.pth\"):\n",
    "        self.device = DEVICE\n",
    "        self.policy_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        self.target_net = CNNDQN(VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH, \n",
    "                                 OTHER_FEATURES_SIZE, MLP_HIDDEN_LAYER_1_SIZE, \n",
    "                                 MLP_HIDDEN_LAYER_2_SIZE, OUTPUT_ACTIONS, DROPOUT_RATE).to(self.device)\n",
    "        \n",
    "        if model_load_path and os.path.exists(model_load_path):\n",
    "            try:\n",
    "                self.policy_net.load_state_dict(torch.load(model_load_path, map_location=self.device))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading model from {model_load_path}: {e}. Initializing new model.\")\n",
    "                self.policy_net.apply(self._initialize_weights)\n",
    "        else:\n",
    "            self.policy_net.apply(self._initialize_weights)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, alpha=PER_ALPHA)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.t_step_episode = 0 # For UPDATE_EVERY\n",
    "        self.beta = PER_BETA_START\n",
    "        self.beta_increment_per_sampling = (1.0 - PER_BETA_START) / PER_BETA_FRAMES if PER_BETA_FRAMES > 0 else 0\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _unpack_viewcone_tile(self, tile_value):\n",
    "        return [float((tile_value >> i) & 1) for i in range(VIEWCONE_CHANNELS)] # Assumes 8 channels\n",
    "\n",
    "    def process_observation(self, observation_dict):\n",
    "        raw_viewcone = observation_dict.get(\"viewcone\", np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8))\n",
    "        if not isinstance(raw_viewcone, np.ndarray): raw_viewcone = np.array(raw_viewcone)\n",
    "        if raw_viewcone.shape != (VIEWCONE_HEIGHT, VIEWCONE_WIDTH):\n",
    "            # print(f\"Warning: Viewcone shape mismatch. Expected ({VIEWCONE_HEIGHT},{VIEWCONE_WIDTH}), got {raw_viewcone.shape}. Using zeros.\")\n",
    "            padded_viewcone = np.zeros((VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.uint8)\n",
    "            h, w = raw_viewcone.shape\n",
    "            h_min, w_min = min(h, VIEWCONE_HEIGHT), min(w, VIEWCONE_WIDTH)\n",
    "            padded_viewcone[:h_min, :w_min] = raw_viewcone[:h_min, :w_min]\n",
    "            raw_viewcone = padded_viewcone\n",
    "\n",
    "        processed_viewcone_channels_data = np.zeros((VIEWCONE_CHANNELS, VIEWCONE_HEIGHT, VIEWCONE_WIDTH), dtype=np.float32)\n",
    "        for r in range(VIEWCONE_HEIGHT):\n",
    "            for c in range(VIEWCONE_WIDTH):\n",
    "                tile_value = raw_viewcone[r, c]\n",
    "                unpacked_features = self._unpack_viewcone_tile(tile_value)\n",
    "                for channel_idx in range(VIEWCONE_CHANNELS):\n",
    "                    processed_viewcone_channels_data[channel_idx, r, c] = unpacked_features[channel_idx]\n",
    "        \n",
    "        other_features_list = []\n",
    "        direction = observation_dict.get(\"direction\", 0)\n",
    "        direction_one_hot = [0.0] * 4; direction_one_hot[direction % 4] = 1.0\n",
    "        other_features_list.extend(direction_one_hot)\n",
    "        location = observation_dict.get(\"location\", [0,0]); norm_x = location[0]/MAP_SIZE_X; norm_y = location[1]/MAP_SIZE_Y\n",
    "        other_features_list.extend([norm_x, norm_y])\n",
    "        other_features_list.append(float(observation_dict.get(\"scout\", 0)))\n",
    "        other_features_list.append(observation_dict.get(\"step\", 0)/MAX_STEPS_PER_EPISODE)\n",
    "        state_other_np = np.array(other_features_list, dtype=np.float32)\n",
    "        \n",
    "        return processed_viewcone_channels_data, state_other_np\n",
    "\n",
    "    def select_action(self, state_viewcone_np, state_other_np, epsilon=0.0):\n",
    "        if random.random() > epsilon:\n",
    "            state_viewcone_tensor = torch.from_numpy(state_viewcone_np).float().unsqueeze(0).to(self.device)\n",
    "            state_other_tensor = torch.from_numpy(state_other_np).float().unsqueeze(0).to(self.device)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad(): action_values = self.policy_net(state_viewcone_tensor, state_other_tensor)\n",
    "            self.policy_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        return random.choice(np.arange(OUTPUT_ACTIONS))\n",
    "\n",
    "    def step(self, state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done):\n",
    "        self.memory.add(state_viewcone, state_other, action, reward, next_state_viewcone, next_state_other, done)\n",
    "        self.t_step_episode = (self.t_step_episode + 1) % UPDATE_EVERY\n",
    "        if self.t_step_episode == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences, indices, weights = self.memory.sample(BATCH_SIZE, beta=self.beta)\n",
    "            self.learn(experiences, indices, weights, GAMMA)\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment_per_sampling)\n",
    "    \n",
    "    def learn(self, experiences, indices, importance_sampling_weights, gamma):\n",
    "        states_viewcone, states_other, actions, rewards, next_states_viewcone, next_states_other, dones = experiences\n",
    "        q_next_actions_policy = self.policy_net(next_states_viewcone, next_states_other).detach().max(1)[1].unsqueeze(1)\n",
    "        q_targets_next = self.target_net(next_states_viewcone, next_states_other).detach().gather(1, q_next_actions_policy)\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.policy_net(states_viewcone, states_other).gather(1, actions)\n",
    "        td_errors = (q_targets - q_expected).abs().cpu().detach().numpy().flatten()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        loss = (importance_sampling_weights * nn.MSELoss(reduction='none')(q_expected, q_targets)).mean()\n",
    "        self.optimizer.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_save_path:\n",
    "            torch.save(self.policy_net.state_dict(), self.model_save_path)\n",
    "            print(f\"Model saved to {self.model_save_path}\")\n",
    "\n",
    "    def reset_episode_counters(self): self.t_step_episode = 0\n",
    "\n",
    "\n",
    "# --- Main Training Loop (Same structure as train_cnn_dqn_optimized_resumed_advanced.py) ---\n",
    "def train_agent(env_module, num_episodes=100000, novice_track=False, load_model_from=None, save_model_to=\"trained_cnn_dqn_agent.pth\"):\n",
    "    print(f\"Starting CNN DQN training: {num_episodes} episodes, Novice: {novice_track}\")\n",
    "    if load_model_from: print(f\"Attempting to load model from: {load_model_from}\")\n",
    "    print(f\"Models will be saved to: {save_model_to}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Epsilon will start at {EPSILON_START:.4f} and decay towards {EPSILON_END:.4f}\")\n",
    "\n",
    "    agent = TrainableRLAgent(model_load_path=load_model_from, model_save_path=save_model_to)\n",
    "    scores_deque = deque(maxlen=100) # For tracking average score\n",
    "    epsilon = EPSILON_START \n",
    "    global_total_steps = 0 # This should ideally be loaded if resuming, to continue epsilon decay correctly.\n",
    "                           # For this setup, it resets, so epsilon decay path also resets.\n",
    "    \n",
    "    # If you have the global_total_steps from the previous run, load it here:\n",
    "    # Example: loaded_checkpoint_data = torch.load(\"checkpoint.pth\")\n",
    "    # global_total_steps = loaded_checkpoint_data.get('global_total_steps', 0)\n",
    "    # agent.beta = loaded_checkpoint_data.get('beta', PER_BETA_START)\n",
    "    # epsilon = loaded_checkpoint_data.get('epsilon', EPSILON_START) # More robust way to resume epsilon\n",
    "\n",
    "    env = env_module.env(env_wrappers=[], render_mode=None, novice=novice_track)\n",
    "    my_agent_id = env.possible_agents[0] if env.possible_agents else \"agent_0\"\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        env.reset() \n",
    "        agent.reset_episode_counters()\n",
    "        current_episode_rewards = {id: 0.0 for id in env.possible_agents}\n",
    "        last_processed_exp_my_agent = {} # Stores (prev_s_vc, prev_s_other, prev_a)\n",
    "        \n",
    "        for pet_agent_id_turn in env.agent_iter():\n",
    "            obs_raw, _, termination, truncation, info = env.last()\n",
    "            # Reward for my_agent's action that led to obs_raw\n",
    "            reward_for_last_action = env.rewards.get(my_agent_id, 0.0) \n",
    "            \n",
    "            # Accumulate rewards for episode score tracking\n",
    "            for r_ag_id, r_val in env.rewards.items():\n",
    "                if r_ag_id in current_episode_rewards: current_episode_rewards[r_ag_id] += r_val\n",
    "            \n",
    "            done = termination or truncation\n",
    "            action_to_take = None\n",
    "\n",
    "            if pet_agent_id_turn == my_agent_id:\n",
    "                obs_dict_current = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in obs_raw.items()}\n",
    "                current_s_vc, current_s_other = agent.process_observation(obs_dict_current)\n",
    "\n",
    "                # If there was a previous state and action for my_agent, complete the experience\n",
    "                if my_agent_id in last_processed_exp_my_agent:\n",
    "                    prev_s_vc, prev_s_other, prev_a = last_processed_exp_my_agent.pop(my_agent_id)\n",
    "                    agent.step(prev_s_vc, prev_s_other, prev_a, reward_for_last_action, current_s_vc, current_s_other, done)\n",
    "                \n",
    "                if done:\n",
    "                    action_to_take = None\n",
    "                else:\n",
    "                    # Agent selects an action based on the current processed state\n",
    "                    action_to_take = agent.select_action(current_s_vc, current_s_other, epsilon)\n",
    "                    # Store this state and action to be completed in the next turn for my_agent_id\n",
    "                    last_processed_exp_my_agent[my_agent_id] = (current_s_vc, current_s_other, action_to_take)\n",
    "                \n",
    "                global_total_steps += 1\n",
    "                # Epsilon decay logic\n",
    "                if global_total_steps > MIN_EPSILON_FRAMES and epsilon > EPSILON_END : \n",
    "                    epsilon *= EPSILON_DECAY_RATE\n",
    "                    epsilon = max(EPSILON_END, epsilon) # Ensure it doesn't go below EPSILON_END\n",
    "                \n",
    "                # Target network update\n",
    "                if global_total_steps % TARGET_UPDATE_EVERY == 0 and global_total_steps > 0: \n",
    "                    agent.update_target_net()\n",
    "\n",
    "            elif not done and env.action_space(pet_agent_id_turn) is not None: # Other agents' turns\n",
    "                action_to_take = env.action_space(pet_agent_id_turn).sample() # Example: random action\n",
    "            \n",
    "            env.step(action_to_take) # Step the environment\n",
    "        \n",
    "        # End of episode\n",
    "        episode_score = current_episode_rewards.get(my_agent_id, 0.0)\n",
    "        scores_deque.append(episode_score)\n",
    "\n",
    "        if i_episode % 100 == 0: # Print summary every 100 episodes\n",
    "            print(f'\\rEp {i_episode}\\tAvgScore(100): {np.mean(scores_deque):.2f}\\tEps: {epsilon:.4f}\\tGlobalSteps: {global_total_steps}\\tBeta: {agent.beta:.4f}')\n",
    "            if save_model_to: agent.save_model() # Save model periodically\n",
    "            \n",
    "    env.close()\n",
    "    if save_model_to: agent.save_model() # Save final model\n",
    "    print(f\"\\nCNN DQN Training finished. Final model saved to {save_model_to if save_model_to else 'N/A'}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_start_time = time.time()\n",
    "    print(f\"Initiating Overnight Optimized CNN DQN training at {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(training_start_time))} UTC\")\n",
    "    try:\n",
    "        # ENSURE YOU HAVE THE FULL CLASS DEFINITIONS FOR SumTree, PrioritizedReplayBuffer, CNNDQN, TrainableRLAgent HERE\n",
    "        # I've put stubs above for brevity in this response.\n",
    "        from til_environment import gridworld # Ensure this is your environment module\n",
    "        \n",
    "        # --- Overnight Training Configuration for Advanced Track ---\n",
    "        NUM_OVERNIGHT_EPISODES = 100000 \n",
    "        LOAD_MODEL_PATH = \"my_wargame_cnn_agent_best_100k.pth\" \n",
    "        SAVE_MODEL_PATH = \"my_wargame_cnn_agent_cont_100k.pth\" # New save path\n",
    "        NOVICE_MODE = False # For Advanced Track\n",
    "\n",
    "        # Global Epsilon constants are now:\n",
    "        # EPSILON_START = 0.05\n",
    "        # EPSILON_END = 0.01\n",
    "        # EPSILON_DECAY_RATE = 0.9999 (slower decay)\n",
    "        # MIN_EPSILON_FRAMES = int(1e4) (decay starts after these many global steps)\n",
    "\n",
    "        train_agent(\n",
    "            gridworld, \n",
    "            num_episodes=NUM_OVERNIGHT_EPISODES,\n",
    "            novice_track=NOVICE_MODE,\n",
    "            load_model_from=LOAD_MODEL_PATH, \n",
    "            save_model_to=SAVE_MODEL_PATH\n",
    "        )\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Could not import 'til_environment.gridworld'. Ensure it's accessible.\")\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"Error: Model file not found. {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during CNN DQN training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    total_time_seconds = time.time() - training_start_time\n",
    "    print(f\"Total Optimized CNN DQN training time for this session: {total_time_seconds:.2f} seconds ({total_time_seconds/3600:.2f} hours).\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env_rl",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "env_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
